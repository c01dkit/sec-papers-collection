[
  {
    "id": 1514,
    "year": 2025,
    "title": "Oedipus: LLM-enchanced Reasoning CAPTCHA Solver",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744872",
    "abstract": "CAPTCHAs have become a ubiquitous tool in safeguarding applications from automated bots. Over time, the arms race between CAPTCHA development and evasion techniques has led to increasingly sophisticated and diverse designs. The latest iteration, reasoning CAPTCHAs, exploits tasks that are intuitively simple for humans but challenging for conventional AI technologies, thereby enhancing security measures.Driven by the evolving AI capabilities, particularly the advancements in Large Language Models (LLMs), we investigate the potential of multimodal LLMs to solve modern reasoning CAPTCHAs. Our empirical analysis reveals that, despite their reasoning capabilities, LLMs struggle to solve these CAPTCHAs effectively. In response, we introduce Oedipus, an innovative end-to-end framework for automated reasoning CAPTCHA solving. Central to this framework is a novel strategy that dissects the complex and human-easy-AI-hard tasks into a sequence of simpler and AI-easy steps. This is achieved through the development of a Domain Specific Language (DSL) for CAPTCHAs that guides LLMs in generating actionable sub-steps for each challenge. The DSL is customized to ensure that each unit operation is a highly solvable subtask by LLMs as revealed in our empirical study. These sub-steps are then tackled sequentially using the Chain-of-Thought methodology. Our evaluation shows that Oedipus effectively resolves the studied CAPTCHAs, achieving an average success rate of 63.5\\%. Remarkably, it also shows adaptability to the most recent CAPTCHA designs introduced in late 2023, which are not included initial study. This prompts a discussion on future strategies for designing reasoning CAPTCHAs that can effectively counter advanced AI solutions.",
    "status": "done"
  },
  {
    "id": 1515,
    "year": 2025,
    "title": "The Odyssey of robots.txt Governance: Measuring Convention Implications of Web Bots in Large Language Model Services",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765063",
    "abstract": "Web content is an essential element for large language model (LLM) services, supporting both training and inference processes. To manage the content access of web bots from LLM service vendors (i.e., LLM bots), web content publishers are increasingly incorporated content access rules into robots.txt, a long-established web content management protocol. However, the rise of proprietary LLM bots, such as OpenAI's ChatGPT-User and Google's Google-Extended, has raised concerns about the transparency of web content access and whether these bots adherence to robots.txt rules. However, there is limited understanding of these LLM bots, concerning their impact on web publishers and broader web content governance. To fill this gap, we present a systematic analysis of 18 LLM bots on 582,281 robots.txt files. Our findings reveal a significant increase in robots.txt rules associated with LLM bots, particularly in domains that fall into the finance and news category. Despite the heightened integration, web publishers face challenges in managing robots.txt configurations due to the complexity of the LLM ecosystem and the involvement of third-party brokers. Furthermore, we identified several cases of robots.txt violations, including instances where LLMs memorized web content from restricted domains, and where ChatGPT-User ignored robots.txt and accessed restricted content. These results highlight the gaps in the current web content governance and underscore the need for enforceable content management mechanisms to respect web publishers' intentions and content control.",
    "status": "done"
  },
  {
    "id": 1516,
    "year": 2025,
    "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript Deobfuscation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744871",
    "abstract": "Deobfuscating JavaScript (JS) code poses a significant challenge in web security, particularly as obfuscation techniques are frequently used to conceal malicious activities within scripts. While Large Language Models (LLMs) have recently shown promise in automating the deobfuscation process, transforming detection and mitigation strategies against these obfuscated threats, a systematic benchmark to quantify their effectiveness and limitations has been notably absent. To address this gap, we present JsDeObsBench, a dedicated benchmark designed to rigorously evaluate the effectiveness of LLMs in the context of JS deobfuscation. We detail our benchmarking methodology, which includes a wide range of obfuscation techniques ranging from basic variable renaming to sophisticated structure transformations, providing a robust framework for assessing LLM performance in real-world scenarios. Our extensive experimental analysis investigates the proficiency of cutting-edge LLMs, e.g., GPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in code simplification despite challenges in maintaining syntax accuracy and execution reliability compared to baseline methods. We further evaluate the deobfuscation of JS malware to exhibit the potential of LLMs in security scenarios. The findings highlight the utility of LLMs in deobfuscation applications and pinpoint crucial areas for further improvement.",
    "status": "done"
  },
  {
    "id": 1517,
    "year": 2025,
    "title": "Quantifying Security Training in Organizations Through the Analysis of U.S. SEC 10-K Filings",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765179",
    "abstract": "The Security Awareness and Training (SAT) market exceeds multiple billion dollars annually, yet reliable data on organizational adoption remains scarce. Conflicting, survey-based figures from cybersecurity vendors leave researchers and decision-makers reliant on questionable insights. A new U.S. Securities and Exchange Commission (SEC) regulation, effective since late 2023, requires companies to disclose cybersecurity strategies in annual Form 10-K filings, offering a more consistent data source. In this study, we crawl and analyze filings from 5,286 U.S. companies across diverse sectors and sizes, using keyword searches and thematic analysis, which offers a lower-bound estimate of prevalent topics. We find that 78\\% of companies report implementing SAT and 27\\% conduct phishing simulations, with adoption varying significantly by sector and size. Larger companies report more extensive SAT efforts, often aligned with standards like NIST CSF. While multi-factor authentication (11\\%) is the most common employee-facing security control, many filings frame employees as a risk factor. Our findings help organizations critically assess SAT strategies and vendor claims, offer actionable insights for policymakers, and equip scholars with a coded dataset and crawling tools for ongoing longitudinal analysis.",
    "status": "done"
  },
  {
    "id": 1518,
    "year": 2025,
    "title": "WhisperTest: A Voice-Control-based Library for iOS UI Automation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765183",
    "abstract": "Dynamic analysis and UI automation are essential for scalable detection of privacy leaks, vulnerabilities, and malicious code in mobile apps. While the Android ecosystem offers a variety of tools, options for iOS apps are limited and require either access to the app source code or jailbreaking the test device. To address this gap, we introduce WhisperTest, an open-source iOS UI automation library that operates without jailbreaking. WhisperTest is based on a newly designed approach that leverages Apple's Voice Control accessibility feature to interact with app or system UIs via text-to-speech. During interactions, WhisperTest monitors the device system logs in real time and scrapes the UI via screenshots and accessibility audits to recover app state changes. We demonstrate WhisperTest's capabilities through a diverse set of tasks, including a web privacy measurement and a fully-automated dynamic analysis of 200 child-directed iOS apps. To overcome the challenges of automating apps with diverse UI designs, WhisperTest optionally integrates multimodal large language models to reason about context and interact with system permission prompts, consent dialogs, subscription prompts, and age gates. Our exploratory analysis of children's apps uncovers widespread use of third-party tracking, limited recognition of user consent, and unencrypted HTTP requests. Overall, we show that WhisperTest enables scalable dynamic analysis of iOS applications across diverse tasks, contributing to a safer and more transparent mobile ecosystem.",
    "status": "done"
  },
  {
    "id": 1519,
    "year": 2025,
    "title": "The Challenges and Opportunities with Cybersecurity Regulations: A Case Study of the US Electric Power Sector",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765184",
    "abstract": "In various industries, cybersecurity regulations have been enacted in an effort to drive improvements to organizational security postures. Despite the prominent influence of these regulations, there has been limited prior investigation of how organizations engage with these regulations and the challenges that they face. Assessing these factors is vital for understanding the impact of cybersecurity regulations in practice and how to enhance them moving forward. In this paper, we take a step towards filling this gap by investigating in depth the mature cybersecurity standard regulating the US electric power industry, NERC CIP (North American Electric Reliability Corporation Critical Infrastructure Protection), mandatory across the industry for the past 15 years. Seeking to improve this existing regulation, we assess the challenges with how this regulation is developed, adopted, and audited, and provide directions for improvement. Given the human-centric nature of regulation compliance, we do so by conducting in-depth semi-structured interviews with a diverse set of industry professionals who have direct experience with the regulation. While this standard is specific to the US energy sector, the challenges and insights uncovered through this qualitative exploration have broader lessons on how regulatory frameworks shape the security of various other industries. Our study reveals varied issues that can arise with a cybersecurity regulation, such as with the standard's specificity, burdensome compliance documentation, auditing subjectivity and inconsistency, and development processes that result in outdated guidelines. These findings in turn shed light on promising directions for policymakers, industry stakeholders, and regulatory bodies to improve cybersecurity regulations and their compliance.",
    "status": "done"
  },
  {
    "id": 1520,
    "year": 2025,
    "title": "Automatically Detecting Online Deceptive Patterns",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765191",
    "abstract": "Deceptive patterns in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous on various digital platforms. While efforts to mitigate deceptive patterns have emerged from legal and technical perspectives, a significant gap remains in creating usable and scalable solutions. We introduce our AutoBot framework to address this gap and help web stakeholders navigate and mitigate online deceptive patterns. AutoBot accurately identifies and localizes deceptive patterns from a screenshot of a website without relying on the underlying HTML code. AutoBot employs a two-stage pipeline that leverages the capabilities of specialized vision models to analyze website screenshots, identify interactive elements, and extract textual features. Next, using a large language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We also use AutoBot, to create a synthetic dataset to distill knowledge from 'teacher' LLMs to smaller language models. Through extensive evaluation, we demonstrate AutoBot's effectiveness in detecting deceptive patterns on the web, achieving an F1-score of 0.93 in this task, underscoring its potential as an essential tool for mitigating online deceptive patterns.We implement AutoBot, across three downstream applications targeting different web stakeholders: (1) a local browser extension providing users with real-time feedback, (2) a Lighthouse audit to inform developers of potential deceptive patterns on their sites, and (3) as a measurement tool for researchers and regulators.",
    "status": "done"
  },
  {
    "id": 1521,
    "year": 2025,
    "title": "Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765148",
    "abstract": "Leakage contracts have been proposed as a new security abstraction at the instruction set architecture level. Leakage contracts aim to capture the information that processors may leak via microarchitectural side channels. Recently, the first tools have emerged to verify whether a processor satisfies a given contract. However, coming up with a contract that is both sound and precise for a given processor is challenging, time-consuming, and error-prone, as it requires in-depth knowledge of the timing side channels introduced by microarchitectural optimizations.In this paper, we address this challenge by proposing LeaSyn, the first tool for automatically synthesizing leakage contracts that are both sound and precise for processor designs at register-transfer level. Starting from a user-provided contract template that captures the space of possible contracts, LeaSyn automatically constructs a contract, alternating between contract synthesis, which ensures precision based on an empirical characterization of the processor's leaks, and contract verification, which ensures soundness.Using LeaSyn, we automatically synthesize contracts for six open-source RISC-V CPUs for a variety of contract templates. Our experiments indicate that LeaSyn's contracts are sound and more precise (i.e., represent the actual leaks in the target processor more faithfully) than contracts constructed by existing approaches.",
    "status": "done"
  },
  {
    "id": 1522,
    "year": 2025,
    "title": "Wanilla: Sound Noninterference Analysis for WebAssembly",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765156",
    "abstract": "WebAssembly (Wasm) is rapidly gaining popularity as a distribution format for software components embedded in various security-critical domains. Unfortunately, despite its prudent design, WebAssembly's primary use case as a compilation target for memory-unsafe languages leaves some possibilities for memory corruption. Independently of that, Wasm is an inherently interesting target for information flow analysis due to its interfacing role. Both the information flows between a Wasm module and its embedding context, as well as the memory integrity within a module, can be described by the hyperproperty noninterference. So far, no sound, fully static noninterference analysis for Wasm has been presented, but sound reachability analyses were. This work presents a novel and general approach to lift reachability analyses to noninterference by tracking taints on values and using value-sensitive, relational reasoning to remove them when appropriate. We implement this approach in Wanilla, the first automatic, sound, and fully static noninterference analysis for WebAssembly, and demonstrate its performance and precision by verifying memory integrity and other noninterference properties with several synthetic and real-world benchmarks.",
    "status": "done"
  },
  {
    "id": 1523,
    "year": 2025,
    "title": "Securing Cryptographic Software via Typed Assembly Language",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765116",
    "abstract": "Authors of cryptographic software are well aware that their code should not leak secrets through its timing behavior, and, until 2018, they believed that following industry-standard constant-time coding guidelines was sufficient. However, the revelation of the Spectre family of speculative execution attacks injected new complexities.To block speculative attacks, prior work has proposed annotating the program's source code to mark secret data, with hardware using this information to decide when to speculate (i.e., when only public values are involved) or not (when secrets are in play). While these solutions are able to track secret information stored on the heap, they suffer from limitations that prevent them from correctly tracking secrets on the stack, at a cost in performance.This paper introduces SecSep, a transformation framework that rewrites assembly programs so that they partition secret and public data on the stack. By moving from the source-code level to assembly rewriting, SecSep is able to address limitations of prior work. The key challenge in performing this assembly rewriting stems from the loss of semantic information through the lengthy compilation process. The key innovation of our methodology is a new variant of typed assembly language (TAL), Octal, which allows us to address this challenge. Assembly rewriting is driven by compile-time inference within Octal. We apply our technique to cryptographic programs and demonstrate that it enables secure speculation efficiently, incurring a low average overhead of 1.2\\%.",
    "status": "done"
  },
  {
    "id": 1524,
    "year": 2025,
    "title": "Formally Verified Correctness Bounds for Lattice-Based Cryptography",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765218",
    "abstract": "Decryption errors play a crucial role in the security of KEMs based on Fujisaki-Okamoto because the concrete security guarantees provided by this transformation directly depend on the probability of such an event being bounded by a small real number. In this paper we present an approach to formally verify the claims of statistical probabilistic bounds for incorrect decryption in lattice-based KEM constructions. Our main motivating example is the PKE encryption scheme underlying ML-KEM. We formalize the statistical event that is used in the literature to heuristically approximate ML-KEM decryption errors and confirm that the upper bounds given in the literature for this event are correct. We consider FrodoKEM as an additional example, to demonstrate the wider applicability of the approach and the verification of a correctness bound without heuristic approximations. We also discuss other (non-approximate) approaches to bounding the probability of ML-KEM decryption.",
    "status": "done"
  },
  {
    "id": 1525,
    "year": 2025,
    "title": "Enabling Secure and Efficient Data Loss Prevention with a Retention-aware Versioning SSD",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765135",
    "abstract": "Cyberattacks resulting in data loss remain a critical concern in modern data protection. To mitigate such threats, data versioning has been introduced to recover compromised data by reverting the storage to a prior uncompromised state. However, most current versioning solutions are implemented at the host level (e.g., within the operating system), making them vulnerable to adversaries with escalated privileges who can compromise OS-level protections. Thus, device-level methods have been proposed to shift the versioning logic to hardware-isolated storage devices outside the untrusted OS. Unfortunately, these solutions suffer from limited retention times for historical data, narrowing the protection window and leaving systems exposed to persistent attacks. In this paper, we propose LAST, an invaLidation-Aware VerSioning sysTem for flash-based SSDs, that enables data versioning with enhanced awareness of data retention time, ensuring long-term availability of historical data with small performance impact. LAST modifies the SSD's flash translation layer (FTL) to retain the data invalidation order for tracking data retention time. Then, it leverages an ordered garbage collection (GC) that always reclaims versioned data with the longest retention time, as determined by the invalidation sequence. Therefore, this approach prevents the premature deletion of data with shorter retention, significantly extending the protection window and reducing the risk of data loss. Evaluated under various real-world workloads, LAST achieves a small latency overhead of 1.5\\% over a regular SSD while maintaining data history for up to 126.4 days with an average of 52.6 days. This significantly outperforms the average retention of current versioning methods by 61.4\\% at least and 165.9\\% at most, enhancing the protection window against data loss from cyberattacks.",
    "status": "done"
  },
  {
    "id": 1526,
    "year": 2025,
    "title": "Don't Panic! Finding Bugs Hidden Behind Rust Runtime Safety Checks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765142",
    "abstract": "Rust has been extensively used in software and system development due to its guarantees for memory and concurrency safety. Fuzzing is a popular bug detection technique for examining the correctness and robustness of programs. However, we identify that current state-of-the-art Rust fuzzers are significantly impeded by the ubiquitous presence of Rust runtime safety checks, resulting in poor effectiveness and efficiency. These checks, which are inserted either implicitly by the compiler or explicitly by the compiler or developers, could cause a high number of panic crashes and early program termination in fuzzing. Consequently, current fuzzers are unable to effectively explore deep code behind the runtime safety checks, leaving potential vulnerabilities undetected.To address these limitations, we propose PanicKiller, a new Rust fuzzing technique to detect bugs hidden in deep and unsafe code. It performs a cross-IR analysis to precisely identify runtime safety checks and unsafe code in Rust programs, and employs a novel dynamic taint analysis to track the critical input bytes associated with the conditions enforced by these checks. PanicKiller further performs novel input prioritization and mutation strategies to achieve effective and efficient fuzzing. Our evaluation shows that PanicKiller significantly outperformed current state-of-the-art Rust fuzzers by achieving average improvements of 22.0\\texttimes{} in bug exposure speed, 1.68\\texttimes{} in code coverage, and 18.2\\texttimes{} in false-positive crash reduction, and up to 129.0\\texttimes{}, 2.10\\texttimes{}, and 64.8\\texttimes{} improvements, respectively. PanicKiller further helped detect 14 and 53 previously unknown vulnerabilities in the benchmark dataset and in the real world, with 11 RustSec IDs assigned.",
    "status": "done"
  },
  {
    "id": 1527,
    "year": 2025,
    "title": "Hardening Deep Neural Network Binaries against Reverse Engineering Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765144",
    "abstract": "Deep Neural Networks (DNNs) are proprietary assets due to the expertise, confidential data, and high development costs involved in model training. Well-trained DNN models are compiled into DNN binaries to be efficiently executed on various platforms, such as edge devices and cloud infrastructures. Recent research on DNN binary decompilation shows the potential of stealing DNN models via binary reverse engineering techniques. While obfuscation is a well-studied technique to hamper binary reverse engineering, general obfuscation schemes are not designed for this new type of binary and have limitations in concealing information within DNN binaries due to the unique characteristics of DNN binaries. In this paper, we show that existing reverse engineering attacks on DNN binaries can recover 98.5\\% of DNN operators from DNN binaries that have been obfuscated using general obfuscators. We then propose new obfuscation schemes tailored for DNN binaries, namely, (1) Flexible Operator Fusion; (2) Fake Operator Insertion; and (3) Operator Computation Reordering. We implement our dedicated obfuscation schemes as an end-to-end obfuscation toolchain called NeuroShield. Experiments show that NeuroShield is resilient to existing model reverse engineering attacks while introducing a reasonable overhead. Specifically, NeuroShield reduces the operator recovery rate to 3.03\\% for CV models and 47.18\\% for NLP models. Moreover, it has comparable binary size overhead and significantly lower execution time overhead (7.8\\% - 36.1\\%) compared to OLLVM, one of the commonly used general obfuscators.",
    "status": "done"
  },
  {
    "id": 1528,
    "year": 2025,
    "title": "CROSS-X: Generalized and Stable Cross-Cache Attack on the Linux Kernel",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765152",
    "abstract": "The cross-cache attack is a fundamental component of modern Linux kernel exploits, spanning real-world attacks and recent research. Despite its importance, it is often regarded as unreliable due to its complex setup, and existing studies lack in-depth analysis of its mechanics. In this paper, we address this gap by: (1) reviewing public strategies and their limitations, (2) proposing two optimized strategies effective in varied conditions, and (3) introducing sys, an automated system that identifies suitable target objects for cross-cache attacks. We evaluated our strategies on a synthetic vulnerability and nine real-world CVEs, achieving over 99\\% and 85\\% success rates under idle and busy workloads, respectively. They also outperformed existing methods in 6 of 8 CVEs under idle workloads and 5 of 8 under busy workloads. For object identification, we define three key properties: (1) spray capability, (2) minimal interference, and (3) useful primitives. Based on these, sys identified seven versatile target objects and their relationship with interfering allocations. We believe our work will enhance public understanding of cross-cache attacks and contribute to improving Linux kernel security.",
    "status": "done"
  },
  {
    "id": 1529,
    "year": 2025,
    "title": "Uncovering Hidden Paths in 5G: Exploiting Protocol Tunneling and Network Boundary Bridging",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765206",
    "abstract": "5G networks are designed with a clear separation between control and user planes, interfaces, and core functions—each expected to operate within isolated trust boundaries. When these boundaries are not properly enforced, malicious traffic from user equipment can traverse unintended paths and reach sensitive components. This work demonstrates how an attacker-controlled UE can exploit such weaknesses using protocol tunneling and network boundary bridging to bypass isolation and interact with internal elements of the 5G core.To assess the real-world impact of these attack vectors, we evaluated six open-source and commercial 5G core deployments, focusing on how user-plane infrastructure processes adversarial traffic from a UE. Our findings reveal inconsistent enforcement of routing, segmentation, and validation policies—rooted in architectural misconfigurations and specification ambiguities. These systemic issues allow crafted but standards-compliant traffic to cross core boundaries.This led to the discovery of seven vulnerabilities and six previously undocumented weaknesses—including flaws in UPF forwarding logic, incomplete protocol validation, and ambiguous specification behaviors. Building on these insights, we design and evaluate three novel attacks that enable direct data injection into UEs, charging fraud via traffic reflection, and full user traffic interception through a rogue gNodeB. We conclude by outlining practical mitigations, emphasizing the need for strict interface validation and stronger isolation controls in 5G core networks.",
    "status": "done"
  },
  {
    "id": 1530,
    "year": 2025,
    "title": "Finding SSH Strict Key Exchange Violations by State Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765208",
    "abstract": "SSH is an important protocol for secure remote shell access to servers on the Internet. At USENIX 2024, B\\\"{a}umer et al. presented the Terrapin attack on SSH, which relies on the attacker injecting optional messages during the key exchange. To mitigate this attack, SSH vendors adopted an extension developed by OpenSSH called strict key exchange (''strict KEX''). With strict KEX, optional messages are forbidden during the handshake, preventing the attack. In practice, this should simplify the state machine of an SSH handshake to a linear message flow similar to that of TLS. In this work, we analyze the design, implementation, and security of strict KEX in popular SSH servers, using black-box state learning, which can uncover the hidden state machine of an implementation. In practice, it is limited by the number of learned messages and the complexity of the state machine. Thus, learning the complete state machine of SSH is infeasible. Previous research on SSH, therefore, excluded optional messages, learning only a partial state machine. However, these messages are a critical part of the Terrapin attack. We propose to instead learn the complete state machine of the handshake phase of an SSH server, but with strict KEX enabled. We investigate the security of ten SSH implementations supporting strict KEX for up to five key exchange algorithms. In total, we learn 33 state machines, revealing significant differences in the implementations. We show that seven implementations violate the strict KEX specification and find two critical security vulnerabilities. One results in a rogue session attack in the proprietary Tectia SSH implementation. Another affects the official SSH implementation of the Erlang Open Telecom Platform, and enables unauthenticated remote code execution in the security context of the SSH server.",
    "status": "done"
  },
  {
    "id": 1531,
    "year": 2025,
    "title": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765219",
    "abstract": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade detection in system-level audit logs. Provenance graphs model these logs as connected entities and events, revealing relationships that are missed by linear log representations. Existing systems apply anomaly detection to these graphs but often suffer from high false positive rates and coarse-grained alerts. Their reliance on node attributes like file paths or IPs leads to spurious correlations, reducing detection robustness and reliability. To fully understand an attack's progression and impact, security analysts need systems that can generate accurate, human-like narratives of the entire attack. To address these challenges, we introduce OCR-APT, a system for APT detection and reconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks (GNNs) for subgraph anomaly detection, learning behavior patterns around nodes rather than fragile attributes such as file paths or IPs. This approach leads to a more robust anomaly detection. It then iterates over detected subgraphs using Large Language Models (LLMs) to reconstruct multi-stage attack stories. Each stage is validated before proceeding, reducing hallucinations and ensuring an interpretable final report. Our evaluations on the DARPA TC3, OpTC, and NODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both detection accuracy and alert interpretability. Moreover, OCR-APT reconstructs human-like reports that comprehensively capture the attack story.",
    "status": "done"
  },
  {
    "id": 1532,
    "year": 2025,
    "title": "CITesting: Systematic Testing of Context Integrity Violations in LTE Core Networks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765230",
    "abstract": "Cellular networks increasingly support critical infrastructure, yet their security remains an ongoing concern. While prior research has focused mainly on downlink vulnerabilities, uplink security—how user equipment (UE) affects the core network—has received limited attention. We study a class of uplink vulnerabilities, which we define as context integrity violations (CIVs), where an unauthenticated or improperly authenticated UE modifies the internal state of other subscribers. Prior work identified a few instances of CIVs, but the broader attack surface remains unexplored. We present CITesting, the first framework for systematically detecting CIVs in LTE core networks. CITesting explores diverse procedure chains, tests a broad range of Information Elements (IEs), and validates behavior across UE connection states. It introduces stateful dual-UE control testing to manage victim UE state and employs a behavioral oracle to detect context modifications in black-box networks. We evaluated CITesting on two open-source (Open5GS, srsRAN) and two commercial (Amarisoft, Nokia) LTE core network implementations, identifying 29, 22, 16, and 59 distinct CIVs after post-analysis. These findings enable remote attacks including UE detachment, IMSI exposure, and presence detection attacks. Note that traditional attack models such as fake base station and active SigOver require the active attacker to be co-located in the same cell. In contrast, our attacks require the active attacker to be in the same MME region (significantly broader than a cell) as the victim UE. All findings were responsibly disclosed, and patches were contributed to Amarisoft and Open5GS.",
    "status": "done"
  },
  {
    "id": 1533,
    "year": 2025,
    "title": "CryptGNN: Enabling Secure Inference for Graph Neural Networks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765232",
    "abstract": "We present CryptGNN, a secure and effective inference solution for third-party graph neural network (GNN) models in the cloud, which are accessed by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its secure message passing and feature transformation layers using distributed secure multi-party computation (SMPC) techniques. CryptGNN protects the client's input data and graph structure from the cloud provider and the third-party model owner, and it protects the model parameters from the cloud provider and the clients. CryptGNN works with any number of SMPC parties, does not require a trusted server, and is provably secure even if P -1 out of P parties in the cloud collude. Theoretical analysis and empirical experiments demonstrate the security and efficiency of CryptGNN.",
    "status": "done"
  },
  {
    "id": 1534,
    "year": 2025,
    "title": "PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765151",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard method for enforcing privacy in deep learning, typically using the Gaussian mechanism to perturb gradient updates. However, conventional mechanisms such as Gaussian and Laplacian noise are parameterized only by variance or scale. This single degree of freedom ties the magnitude of noise directly to both privacy loss and utility degradation, preventing independent control of these two factors. The problem becomes more pronounced when the number of composition rounds T and batch size B vary across tasks, as these variations induce task-dependent shifts in the privacy–utility trade-off, where small changes in noise parameters can disproportionately affect model accuracy. To address this limitation, we introduce PLRV-O, a framework that defines a broad search space of parameterized DP-SGD noise distributions, where privacy loss moments are tightly characterized yet can be optimized more independently with respect to utility loss. This formulation enables systematic adaptation of noise to task-specific requirements, including (i) model size, (ii) training duration, (iii) batch sampling strategies, and (iv) clipping thresholds under both training and fine-tuning settings. Empirical results demonstrate that PLRV-O substantially improves utility under strict privacy constraints. On CIFAR-10, a fine-tuned ViT achieves 94.03\\% accuracy at ∈ ≈ 0.5, compared to 83.93\\% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20\\% accuracy at ∈ ≈ 0.2, versus 50.25\\% with Gaussian. Source code is available at https://github.com/datasec-lab/plrvo.",
    "status": "done"
  },
  {
    "id": 1535,
    "year": 2025,
    "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765160",
    "abstract": "Split Learning (SL) is a distributed learning approach that enables resource-constrained clients to collaboratively train deep neural networks (DNNs) by offloading most layers to a central server while keeping in- and output layers on the client-side. This setup enables SL to leverage server computation capacities without sharing data, making it highly effective in resource-constrained environments dealing with sensitive data. However, the distributed nature enables malicious clients to manipulate the training process. By sending poisoned intermediate gradients, they can inject backdoors into the shared DNN. Existing defenses are limited by often focusing on server-side protection and introducing additional overhead for the server. A significant challenge for client-side defenses is enforcing malicious clients to correctly execute the defense algorithm. We present ZORRO, a private, verifiable, and robust SL defense scheme. Through our novel design and application of interactive zero-knowledge proofs (ZKPs), clients prove their correct execution of a client-located defense algorithm, resulting in proofs of computational integrity attesting to the benign nature of locally trained DNN portions. Leveraging the frequency representation of model partitions enables ZORRO to conduct an in-depth inspection of the locally trained models in an untrusted environment, ensuring that each client forwards a benign checkpoint to its succeeding client. In our extensive evaluation, covering different model architectures as well as various attack strategies and data scenarios, we show ZORRO's effectiveness, as it reduces the attack success rate to less than 6\\% while causing even for models storing 1000000 parameters on the client-side an overhead of less than 10 seconds.",
    "status": "done"
  },
  {
    "id": 1536,
    "year": 2025,
    "title": "ImportSnare: Directed 'Code Manual' Hijacking in Retrieval-Augmented Code Generation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765161",
    "abstract": "Code generation has emerged as a pivotal capability of Large Language Models (LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces.In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., ''matplotlib_safe'') can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1) Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2) Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50\\% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01\\%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting LLMs' inadequate security alignment for code generation tasks. The project homepage is https://importsnare.github.io/.Disclaimer. This paper contains examples of harmful content. Reader discretion is recommended.",
    "status": "done"
  },
  {
    "id": 1537,
    "year": 2025,
    "title": "Generic Anonymity Wrapper for Messaging Protocols",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765186",
    "abstract": "Modern messengers use advanced end-to-end encryption protocols to protect message content even if user secrets are ever temporarily exposed. Yet, encryption alone does not prevent user tracking, as protocols often attach metadata, such as sequence numbers, public keys, or even plain user identifiers. This metadata reveals the social network as well as communication patterns between users. Existing protocols that hide metadata in Signal (i.e., Sealed Sender), for MLS-like constructions (Hashimoto et al., CCS 2022), or in mesh networks (Bienstock et al., CCS 2023) are relatively inefficient or specially tailored for only particular settings. Moreover, all existing practical solutions reveal crucial metadata upon exposures of user secrets.In this work, we introduce a formal definition of Anonymity Wrappers (AW) that generically hide metadata of underlying two-party and group messaging protocols. Our definition captures forward and post-compromise anonymity as well as authenticity in the presence of temporary state exposures. Inspired by prior wrapper designs, the idea of our provably secure AW construction is to use shared keys of the underlying wrapped (group) messaging protocols to derive and continuously update symmetric keys for hiding metadata. Beyond hiding metadata on the wire, we also avoid and hide structural metadata in users' local states for stronger anonymity upon their exposure.We implement our construction, evaluate its performance, and provide a detailed comparison with Signal's current approach based on Sealed Sender: Our construction reduces the wire size of small 1:1 messages from 441 bytes to 114 bytes. For a group of 100 members, it reduces the wire size of outgoing group messages from 7240 bytes to 155 bytes. We see similar improvements in computation time for encryption and decryption, but these improvements come with substantial storage costs for receivers. For this reason, we develop extensions with a Bloom filter for compressing the receiver storage. Based on this, Signal considers deploying our solution.",
    "status": "done"
  },
  {
    "id": 1538,
    "year": 2025,
    "title": "Panther: Private Approximate Nearest Neighbor Search in the Single Server Setting",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765190",
    "abstract": "Approximate nearest neighbor search (ANNS), also known as vector search, is an important building block for various applications, such as recommendation systems, biometric authentication, and machine learning. In this work, we are interested in the private ANNS problem, where the client wants to learn (and can only learn) the ANNS results without revealing the query to the server. Previous private ANNS works either suffer from high communication cost (Chen et al., USENIX Security 2020) or work under a stronger security assumption of two non-colluding servers (Servan-Schreiber et al., SP 2022). We present Panther, an efficient private ANNS framework under the single server setting. Panther achieves its high performance via several novel co-designs of private information retrieval, secret-sharing, garbled circuits, and homomorphic encryption. We made extensive experiments using Panther on four public datasets, showing that Panther could answer an ANNS query on 10 million points in 18 seconds with 284 MB of communication. This is more than 7.8\\texttimes{} faster and 20\\texttimes{} more compact than Chen et al..",
    "status": "done"
  },
  {
    "id": 1539,
    "year": 2025,
    "title": "Trout: Two-Round Threshold ECDSA from Class Groups",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765192",
    "abstract": "We present Trout (Two-ROUnd Threshold), the first distributed two-round ECDSA signing protocol for arbitrary thresholds. Trout has constant upload bandwidth per-party and processing time linear in the amount of participants. Moreover, Trout achieves the Identifiable Abort (IA) property, which means that if the protocol cannot terminate due to a failure, parties can attribute the failure to a specific party. We achieve this without a trusted setup. Our protocol relies on linear-homomorphic encryptions and commitments over class groups. To obtain our result, we leverage the recent construction of an exponent-VRF (Boneh et al., Eurocrypt 2025) and a novel protocol to multiply an encrypted value with a committed value and simultaneously decrypt it, which we call ''scaled decryption''. We believe that this protocol may be of independent interest. Our protocol has a very low communication cost of just 6.5 KB sent per party. Furthermore, we implemented our protocol in Rust and provide benchmarks for various configurations, showing its practicality even for 100 parties. Our implementation includes a constant-time variant which, to the best of our knowledge, is the first of its kind for class-group-based threshold ECDSA protocols.",
    "status": "done"
  },
  {
    "id": 1540,
    "year": 2025,
    "title": "Encrypted Matrix-Vector Products from Secret Dual Codes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765194",
    "abstract": "Motivated by applications to efficient secure computation, we consider the following problem of encrypted matrix-vector product (EMVP). Let ⅇ be a finite field. In an offline phase, a client uploads an encryption of a matrix M∈ ⅇmxℓ to a server, keeping only a short secret key. The server stores the encrypted matrix M. In the online phase, the client may repeatedly send encryptions qi of query vectors qi∈ ⅇℓ, which enables the client and the server to locally compute compact shares of the matrix-vector product M qi. The server learns nothing about M or qi. The shared output can either be revealed to the client or processed by another protocol.We present efficient EMVP protocols based on variants of the learning parity with noise (LPN) assumption and the related learning subspace with noise (LSN) assumption. Our EMVP protocols are field-agnostic in the sense that the parties only perform arithmetic operations over ⅃, and are close to optimal with respect to both communication and computation. In fact, for sufficiently large ℓ (typically a few hundreds), the online computation and communication costs of our LSN-based EMVP can be less than twice the costs of computing Mqi in the clear.Combined with suitable secure post-processing protocols on the secret-shared output, our EMVP protocols are useful for a variety of secure computation tasks, including encrypted fuzzy search and secure ML.Our technical approach builds on recent techniques for private information retrieval in the secret-key setting. The core idea is to encode the matrix M and the queries qi using a pair of secret dual linear codes, while defeating algebraic attacks by adding noise.",
    "status": "done"
  },
  {
    "id": 1541,
    "year": 2025,
    "title": "KZH-Fold: Accountable Voting from Sublinear Accumulation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744796",
    "abstract": "Accumulation schemes are powerful primitives that enable distributed and incremental verifiable computation with less overhead than recursive SNARKs. However, existing schemes with constant-size accumulation verifiers, suffer from linear-sized accumulators and deciders, leading to linear-sized proofs that are unsuitable in distributed settings. Motivated by the need for bandwidth efficient accountable voting protocols, (I) We introduce KZH, a novel polynomial commitment scheme, and (II) KZH-fold, the first sublinear accumulation scheme with a constant-size verifier (3 group scalar multiplications) and O(n1/2 ) accumulator size and decider time. Our scheme generalizes to achieve accumulator and decider complexity of k • n1/k with a verifier of size k. Using the BCLMS compiler, (III) we build the first IVC/PCD scheme with sublinear proof and decider. (IV) Next, we propose a new approach to non-uniform IVC, where the cost of proving a step is proportional to the maximum size of all instruction circuits, and unlike previous approaches, the witness size is not linear in the number of instructions. (V) Leveraging these advancements, we demonstrate the power of KZH-fold by implementing an accountable voting scheme using a novel signature aggregation protocol supporting millions of nodes, significantly reducing communication overhead and verifier time compared to BLS-based aggregation. We implemented and benchmarked our protocols, and KZH-fold achieves a 2000x reduction in communication and a 50x improvement in decider time over Nova when proving 2000 Poseidon hashes, at the cost of 3x the prover time.",
    "status": "done"
  },
  {
    "id": 1542,
    "year": 2025,
    "title": "SyRA: Sybil-Resilient Anonymous Signatures with Applications to Decentralized Identity",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744806",
    "abstract": "We study Sybil-Resilient Anonymous (SyRA) signatures, a cryptographic primitive that enables credentialed users to generate, on demand, unlinkable pseudonyms tied to any given context, and issue signatures on behalf of these pseudonyms. Concretely, SyRA allows a distributed issuer to turn any legacy identity or personhood identifier, possibly of low entropy, into a unique associated cryptographic key of high pseudoentropy, for use in generating signatures for any given context. Sybil-resilient anonymous signatures achieve three main objectives: 1) Sybil resilience: every user is entitled to at most one digital identity, 2) anonymity: no information about the user's real identity is leaked, and 3) non-interactive context switching: users can create on their own at most one credential for any given context in a manner that is unlinkable across contexts.We conceptualize the SyRA primitive as an ideal functionality in the Universal Composition (UC) setting and put forth SASSI, an efficient, pairing-based construction that realizes it by utilizing two levels of verifiable random functions (VRFs), a design which may be of independent interest. The first level consists of threshold VRF issuance of a user's unique secret key tied to their real-world identifier. The second level allows a user to create signatures for each context, under a unique pseudonym per context. Compared to prior cryptographic tools capable of realizing SyRA, SASSI has the unique feature that issuers are stateless and hence do not need to retain any information about past user interactions, a relevant property for a decentralized implementation.We overview various applications of SASSI in multiparty systems, such as cryptocurrency account management and airdrops, e-voting (e.g., for decentralized governance), and privacy-preserving regulatory compliance (e.g., AML/CFT checks). In the context of creating addresses for digital assets, SyRA signatures enable users to embed their legacy identity into their address in a manner that protects their privacy for each application with which they interact. We demonstrate the practicality of SASSI by providing an implementation and performance evaluation of our construction.",
    "status": "done"
  },
  {
    "id": 1543,
    "year": 2025,
    "title": "Surpassing the Word Size Limitation of TFHE with Noise Calibration",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744808",
    "abstract": "Torus fully homomorphic encryption (TFHE) is a promising solution for secure computation, offering low computational cost and simple setup requirements. A key feature of TFHE is programmable bootstrap (PBS), which enables efficient homomorphic evaluation of arbitrary functions over small domains. However, the domain size of PBS is constrained by the word size limitation of TFHE, an unavoidable restriction to ensure data security. This limitation raises scalability challenges for extending homomorphic function evaluation to larger domains. Existing approaches attempt to overcome this limitation but suffer from high computational costs. The vertical packing technique (Chillotti et al., 2020) supports function evaluations beyond the word size limitation but depends on circuit bootstrap, a computationally expensive primitive. The tree-based method (Guimar\\~{a}es et al., 2021) avoids using circuit bootstrap but introduces significant computational overhead, requiring O(2W) PBS calls for a W-bit domain.In this paper, we propose noise calibration method that enables homomorphic function evaluation beyond the word size limitation of TFHE. Our approach supports an exponential expansion of the function domain size while achieving both low computational complexity and high accuracy. For a W-bit domain, the proposed method requires a total of only O(W2) PBS calls for homomorphic function evaluation. Experimental results show that when W is around 10, the proposed method achieves a performance improvement of about 3 times compared to the vertical packing and 10 times compared to the tree-based method, without loss of accuracy.The main advantage of the noise calibration method is its compatibility with PBS, making it highly adaptable to large-scale applications. We demonstrate its application to oblivious stable sorting for long input arrays. Experiments show that the proposed method outperforms sorting network-based methods by about 4-8 times in speed of the 4-bit array sorting and scales efficiently to array lengths beyond the word size limitation.",
    "status": "done"
  },
  {
    "id": 1544,
    "year": 2025,
    "title": "QV-net: Decentralized Self-Tallying Quadratic Voting with Maximal Ballot Secrecy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744810",
    "abstract": "Decentralized e-voting enables secure and transparent elections without relying on trusted authorities, with blockchain emerging as a popular platform. It has compelling applications in Decentralized Autonomous Organizations (DAOs), where governance relies on voting with blockchain-issued tokens. Quadratic voting (QV), a mechanism that mitigates the dominance of large token holders, has been adopted by many DAO elections to enhance fairness. However, current QV systems deployed in practice publish voters' choices in plaintext with digital signatures. The open nature of all ballots comprises voter privacy, potentially affecting voters' honest participation. Prior research proposes using cryptographic techniques to encrypt QV ballots, but they work in a centralized setting, relying on a trusted group of tallying authorities to administrate an election. However, in DAO voting, there is no trusted third party.In this paper, we propose QV Network (QV-net), the first decentralized quadratic voting scheme, in which voters do not need to trust any third party other than themselves for ballot secrecy. QV-net is self-tallying with maximal ballot secrecy. Self-tallying allows anyone to compute election results once all ballots are cast. Maximal ballot secrecy ensures that what each voter learns from QV-net is nothing more than the tally and their own ballot. We provide an open-source implementation of QV-net to demonstrate its practicality based on real-world DAO voting settings, reporting only a few milliseconds for voting and a maximum of 255 milliseconds for tallying. The exceptional efficiency of QV-net is attributed to the design of two new Zero-Knowledge Argument of Knowledge (ZKAoK) protocols for QV ballot secrecy and integrity. Previous works generally rely on pairing-friendly curves to prove the well-formedness of an encrypted QV ballot. But they incur heavy computation and large data sizes. We tackle the challenges of appropriately formalizing and proving ZKAoK relations for QV without using these curves. Specifically, we develop a succinct ZKAoK to prove a new relation: the sum of squares of a private vector's components equals a private scalar. We also introduce the first aggregated range proof to prove that values committed under different keys fall within their respective ranges. Together, these two new zero-knowledge protocols enable us to build an efficient decentralized QV scheme and are of independent interest.",
    "status": "done"
  },
  {
    "id": 1545,
    "year": 2025,
    "title": "End-to-End Encrypted Git Services",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744815",
    "abstract": "Git services such as GitHub, have been widely used to manage projects and enable collaborations among multiple entities. Just as in messaging and cloud storage, where end-to-end security has been gaining increased attention, such a level of security is also demanded for Git services. Content in the repositories (and the data/code supply-chain facilitated by Git services) could be highly valuable, whereas the threat of system breaches has become routine nowadays. However, existing studies of Git security to date (mostly open source projects) suffer in two ways: they provide only very weak security, and they have a large overhead.In this paper, we initiate the needed study of efficient end-to-end encrypted Git services. Specifically, we formally define the syntax and critical security properties, and then propose two constructions that provably meet those properties. Moreover, our constructions have the important property of platform-compatibility: They are compatible with current Git servers and reserve all basic Git operations, thus can be directly tested and deployed on top of existing platforms. Furthermore, the overhead we achieve is only proportional to the actual difference caused by each edit, instead of the whole file (or even the whole repository) as is the case with existing works. We implemented both constructions and tested them directly on several public GitHub repositories. Our evaluations show (1) the effectiveness of platform-compatibility, and (2) the significant efficiency improvement we got (while provably providing much stronger security than prior ad-hoc treatments).",
    "status": "done"
  },
  {
    "id": 1546,
    "year": 2025,
    "title": "RingSG: Optimal Secure Vertex-Centric Computation for Collaborative Graph Processing",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744824",
    "abstract": "Collaborative graph processing refers to the joint analysis of inter-connected graphs held by multiple graph owners. To honor data privacy and support various graph processing algorithms, existing approaches employ secure multi-party computation (MPC) protocols to express the vertex-centric abstraction. Yet, due to certain computation-intensive cryptography constructions, state-of-the-art (SOTA) approaches are asymptotically suboptimal, imposing significant overheads in terms of computation and communication. In this paper, we present RingSG, the first system to attain optimal communication/computation complexity within the MPC-based vertex-centric abstraction for collaborative graph processing. This optimal complexity is attributed to Ring-ScatterGather, a novel computation paradigm that can avoid exceedingly expensive cryptography operations (e.g., oblivious sort), and simultaneously ensure the overall workload can be optimally decomposed into parallelizable and mutually exclusive MPC tasks. Within Ring-ScatterGather, RingSG improves the concrete runtime efficiency by incorporating 3-party secure computation via share conversion, and optimizing the most cost-heavy part using a novel oblivious group aggregation protocol. Finally, unlike prior approaches, we instantiate RingSG into two end-to-end applications to effectively obtain application-specific results from the protocol outputs in a privacy-preserving manner. We developed a prototype of RingSG and extensively evaluated it across various graph collaboration settings, including different graph sizes, numbers of parties, and average vertex degrees. The results show RingSG reduces the system running time of SOTA approaches by up to 15.34\\texttimes{} and per-party communication by up to 10.36\\texttimes{}. Notably, RingSG excels in processing sparse global graphs collectively held by more parties, consistent with our theoretical cost analysis.",
    "status": "done"
  },
  {
    "id": 1547,
    "year": 2025,
    "title": "Nebula: Efficient, Private and Accurate Histogram Estimation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744789",
    "abstract": "We present Nebula, a system for differentially private histogram estimation on data distributed among clients. Nebula allows clients to independently decide whether to participate in the system, and locally encode their data so that an untrusted server only learns data values whose multiplicity exceeds a predefined aggregation threshold, with (ε,δ) differential privacy guarantees. Compared to existing systems, Nebula uniquely achieves: i) a strict upper bound on client privacy leakage; ii) significantly higher utility than standard local differential privacy systems; and iii) no requirement for trusted third-parties, multi-party computation, or trusted hardware. We provide a formal evaluation of Nebula 's privacy, utility and efficiency guarantees, along with an empirical assessment on three real-world datasets. On the United States Census dataset, clients can submit their data in just 0.0036 seconds and 0.0016 MB (efficient), under strong (ε=1,δ=10-8) differential privacy guarantees (private), enabling Nebula's untrusted aggregation server to estimate histograms with over 88\\% better utility than existing local differential privacy deployments (accurate). Additionally, we describe a variant that allows clients to submit multi-dimensional data, with similar privacy, utility, and performance. Finally, we provide an implementation of Nebula.",
    "status": "done"
  },
  {
    "id": 1548,
    "year": 2025,
    "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744794",
    "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns. This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models. We view data auditing through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples. MembershipTracker only requires the users to mark a small fraction of data (0.005\\%∼0.1\\% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 100\\% TPR@0\\% FPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures. 1Our code is available at https://github.com/DependableSystemsLab/MembershipTracker",
    "status": "done"
  },
  {
    "id": 1549,
    "year": 2025,
    "title": "Posterior Security: Anonymity and Message Hiding of Standard Signatures",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744797",
    "abstract": "We introduce posterior security of digital signatures, the additional security features after the original signature is generated. It is motivated by the scenario that some people store their secret keys in secure hardware and can only obtain a standard signature through a standardized interface. In this paper, we consider two different posterior security features: anonymity and message hiding.We first introduce incognito signature, a new mechanism to anonymize a standard signature. Different from other ring or group signatures, the signer generates a standard (non-anonymous) signature first. The signature is then anonymized by a converter before sending to the verifier, by hiding the signer public key with a set of decoy public keys. We then introduce concealed signature which hides the message in a commitment. The standard signature is converted such that it can be verified with the commitment. The models of posterior anonymity and posterior message hiding capture the separation of the signer and the converter. Anonymity or message hiding is provided by the converter after the creation of a standard signature by the signer. It is useful in applications like two-tier central bank digital currency, where users want to hide their addresses (public keys) and transaction amounts (messages) when the payment is settled in the interbank layer.",
    "status": "done"
  },
  {
    "id": 1550,
    "year": 2025,
    "title": "Accuracy for Differentially Private Quotients by Fractional Uncertainties",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744799",
    "abstract": "Differential Privacy (DP) is a cornerstone for ensuring privacy in data analysis by injecting carefully calibrated noise into statistical queries. While numerous DP tools focus on privacy protection, few provide accuracy information, specially for data-dependent computations like averages or quotients of DP-sums. This paper introduces a novel approach to compute confidence intervals, i.e., α-β accuracy, for these computations, leveraging principles from uncertainty propagation. Our method identifies conditions under which analytical error can be predicted, revealing two key invariants: the analytical error improves with large dataset sizes, and addition of values with higher variability require larger dataset sizes for accurate estimation. To simplify adoption, we also propose accuracy tuners to enable rapid determination of minimum dataset sizes and explore trade-offs between privacy budgets and the possibility to perform accuracy estimations. Our theoretical contributions are validated through an empirical evaluation that explores the applicability of fractional uncertainties for computing concrete α-β error across diverse scenarios.",
    "status": "done"
  },
  {
    "id": 1551,
    "year": 2025,
    "title": "Sabot: Efficient and Strongly Anonymous Bootstrapping of Communication Channels",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744803",
    "abstract": "Anonymous communication is vital for enabling individuals to participate in social discourse without fear of marginalization or persecution. An important but often overlooked part of anonymous communication is the bootstrapping of new communication channels. If Alice wants to communicate with Bob, she must first learn his in-system identifier. In synchronous designs, message exchange is only possible once both communication partners have agreed to communicate. Thus, Alice must notify Bob of her intent, Bob must learn her in-system identifier, and Bob must acknowledge her notification. This bootstrapping process is generally assumed to occur out-of-band, but if it discloses metadata, communication partners are revealed even if the channel itself is fully anonymized. We propose Sabot, the first anonymous bootstrapping protocol that achieves both strong cryptographic privacy guarantees and bandwidth-efficient communication. In Sabot, clients cooperatively generate a private relationship matrix, which encodes who wants to contact whom. Clients communicate with k ≥ 2 servers to obtain ''their'' part of the matrix and augment the received information using Private Information Retrieval (PIR) to learn about their prospective communication partners. Compared to previous solutions, Sabot achieves stronger privacy guarantees and reduces the bandwidth overhead by an order of magnitude.",
    "status": "done"
  },
  {
    "id": 1552,
    "year": 2025,
    "title": "LZKSA: Lattice-Based Special Zero-Knowledge Proofs for Secure Aggregation's Input Verification",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744812",
    "abstract": "In many fields, the need to securely collect and aggregate data from distributed systems is growing. However, designs that rely solely on encrypted data transmission make it difficult to trace malicious users. To address this challenge, we have enhanced the secure aggregation (SA) protocol proposed by Bell et al. (CCS 2020) by introducing verification features that ensure compliance with user inputs and encryption processes while preserving data privacy. We present LZKSA, a quantum-safe secure aggregation system with input verification. LZKSA employs seven zero-knowledge proof (ZKP) protocols based on the Ring Learning with Errors problem, specifically designed for secure aggregation. These protocols verify whether users have correctly used SA keys and their L∞, L2 norms and cosine similarity of data, meet specified constraints, to exclude malicious users from current and future aggregation processes. The specialized ZKPs we propose significantly enhance proof efficiency. In practical federated learning scenarios, our experimental evaluations demonstrate that the proof generation time for L∞ and L2 constraints is reduced to about 10-3 of that required by the current state-of-the-art method, RoFL (S&amp;P 2023), and ACORN (USENIX 2023). For example, the proof generation/verification time of RoFL, ACORN and LZKSA for L∞ is 94s/29.9s, 78.7s/33.9s, and 0.02s/0.0062s for CIFAR10, respectively.",
    "status": "done"
  },
  {
    "id": 1553,
    "year": 2025,
    "title": "A Sea of Cyber Threats: Maritime Cybersecurity from the Perspective of Mariners",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744816",
    "abstract": "Maritime systems, including ships and ports, are critical components of global infrastructure, essential for transporting over 80\\% of the world's goods and supporting internet connectivity. However, these systems face growing cybersecurity threats, as highlighted by recent attacks disrupting Maersk, one of the world's largest shipping companies, causing widespread impacts on international trade and shipping. The unique challenges of the maritime environment-including diverse operational conditions, extensive physical access points, fragmented regulatory frameworks, and its deeply interconnected, international structure—require maritime-specific cybersecurity research. Despite the sector's critical importance, maritime cybersecurity remains an underexplored area, leaving significant gaps in our understanding of its challenges and risks.To take an early step in addressing these gaps, we investigate how operators of maritime systems perceive and navigate cybersecurity challenges within the complex maritime landscape. We conducted a user study comprising surveys and semi-structured interviews with 21 officer-level mariners. Participants reported direct experiences with shipboard cyber-attacks, including offshore GPS spoofing and logistics-disrupting ransomware, demonstrating the real-world impact of these threats. Despite this, our findings reveal systemic and human-centric issues, such as cybersecurity training that is poorly designed to address the unique challenges of maritime operations, insufficient detection and response solutions, and severe gaps in mariners' understanding of cybersecurity. Our contributions include a detailed categorization of cyber threats identified by mariners, as well as actionable recommendations for improving maritime security, including enhancements to cybersecurity training, attack response protocols, and regulatory frameworks. These insights aim to guide future research and policy to bolster the resilience of maritime systems against evolving cyber threats.",
    "status": "done"
  },
  {
    "id": 1554,
    "year": 2025,
    "title": "Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744855",
    "abstract": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education and training. With the evolution of large language models (LLMs), there is growing interest in their ability to automate CTF challenge solving, with DARPA's AIxCC competition (since 2023) being a notable example. However,this demands a combination of multiple abilities of LLMs, from knowledge to reasoning and further to actions. In this paper, we highlight the importance of technical knowledge in solving CTF problems and deliberately construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs' performance in this core aspect. Our study offers a focused and innovative measurement of LLMs' capability in understanding CTF knowledge and applying it to solve CTF challenges. Our key findings reveal that while LLMs possess substantial technical knowledge, they struggle to apply it accurately to specific scenarios and adapt based on feedback from CTF environments.Based on insights derived from this measurement study, we propose CTFAgent, a novel LLM-driven framework for advancing CTF problem-solving. CTFAgent introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and interactive Environmental Augmentation, which enhance LLMs' technical knowledge and vulnerability exploitation on CTF, respectively. Experiments on two popular CTF datasets show that CTFAgent both achieves over 80\\% performance improvement. Moreover, in the picoCTF2024 hosted by CMU, CTFAgent ranked in the top 23.6\\% of nearly 7,000 participating teams. This reflects the benefit of our measurement study and the potential of our framework in advancing LLMs' capabilities in CTF problem-solving.",
    "status": "done"
  },
  {
    "id": 1555,
    "year": 2025,
    "title": "Exposing the Roots of DNS Abuse: A Data-Driven Analysis of Key Factors Behind Phishing Domain Registrations",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744869",
    "abstract": "Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of registrars and TLDs. However, no existing study systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this paper, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. Having compiled a list of 14.5 k malicious and 15.4 k benign domains, we collect a comprehensive set of 73 features for all the domains encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. With a GLM regression analysis, we found that each dollar reduction in registration fees corresponds to a 49\\% increase in malicious domain registrations. The availability of free bundled services, such as web hosting, drives an 88\\% surge in phishing activities. Conversely, stringent registration restrictions cut down abuse by 63\\%, while registrars providing API access for domain registration or account creation experience a staggering 401\\% rise in malicious domains. The results enable intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic interests.",
    "status": "done"
  },
  {
    "id": 1556,
    "year": 2025,
    "title": "Noise and Stress Don't Help With Learning: A Qualitative Study to Inform Design of Effective Cybersecurity Awareness in Manufacturing Environments",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744880",
    "abstract": "With Industry 4.0, cybersecurity risks in manufacturing contexts are increasing rapidly. Since mandatory cybersecurity awareness programs (CAP) are considered best practice, companies looking at adapting training for this group, and allowed us to conduct a study. We conducted semi-structured interviews with n=33 manufacturing workers in 6 locations, to determine what they knew about cybersecurity risks, to what extent they consider them relevant, and what their experiences with, and perceptions of cybersecurity measures and training were. The interviews were analyzed using qualitative content analysis. Most of our participants reported only occasional interaction with what they consider ''office'' information and communication technology (ICT) in the context of their daily work. For most, the only touchpoints were HR-related transactions (pay and vacation), conducted via shared digital shopfloor kiosk PCs, through which they also received corporate communications. Most participants did not consider cybersecurity their responsibility, associating it with ''office'' and ''management'' roles. Most ICT and cybersecurity as potential threats to ''smooth running'' of work processes and their productivity. At the same time, there was positive perception of safety measures and training, with a clear preference for face-to-face team-based training in situ, so they could ask questions and point out possible issues - very different from the company's idea of individual computer-based trained, which most would receive via shared kiosk PCs on a noisy shop floor. Our results suggest that successful CAP needs to tailor content not only according to relevant risks, but relating those to key values and work practices, and consider different ways of delivering it.",
    "status": "done"
  },
  {
    "id": 1557,
    "year": 2025,
    "title": "An Empirical Study Measuring In-The-Wild Cryptographic Microarchitectural Side-Channel Patches",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744881",
    "abstract": "Patching microarchitectural side channels in real-world cryptographic software is a challenging task that does not always result in efficient and secure patches. Despite the continuous efforts of researchers and developers, the security and performance of microarchitectural side-channel patches have not been comprehensively studied before. To systematically study this patching effort, this paper conducts the first measurement study on in-the-wild side-channel patches, yielding the SideBench dataset comprising 165 patches from three mainstream cryptographic libraries (OpenSSL, WolfSSL, and MbedTLS), and offering an automated analysis tool, SideEval, tailored to analyze side-channel patches through a combination of dynamic taint analysis and static symbolic execution. Our analysis reveals that even among patches written by experienced developers, 25 are insecure, leaving residual side-channel leakages potentially unnoticed by developers for years. Furthermore, some patches rashly issued to fix one microarchitectural side channel may inadvertently open new leakages against other side-channel models. We also observed that patches in different cryptographic libraries, even when fixing the same code pattern, can incur drastically different overheads, varying from 10\\% to 170\\%. Additionally, our measurements show that recent rule-based and large language model (LLM)-based automated patching tools are not as secure as expected. We summarize our findings and provide insights for developers to fix side channels securely and efficiently.",
    "status": "done"
  },
  {
    "id": 1558,
    "year": 2025,
    "title": "Can IOCs Impose Cost? The Effects of Publishing Threat Intelligence on Adversary Behavior",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765026",
    "abstract": "Exposing intrusion campaigns has become a geopolitical tool, with governments and commercial firms publishing threat intelligence reports about hacking attempts and modus operandi. U.S. government officials have explained this as not just a defensive practice but also as a way to 'impose cost' on attackers by forcing them to develop new infrastructure, tools, and techniques. We empirically examine this claim by analyzing attacker behavior before and after publication of indicators of compromise (IOCs). Using IOC feeds from two leading commercial providers, we matched IOCs against a large dataset of real-world network traffic metadata. This enabled us to generate sightings retroactively, capturing malicious activity up to 150 days before and after publication. Unlike prior work focused on post-publication malicious activity, our method provides a more complete view over time. Our results show that most IOCs point to resources that attackers had already abandoned by publication, limiting their utility for detecting ongoing attacks and undermining the idea of 'imposing costs'. Statistical modeling further reveals that publication status has low explanatory power for sightings, suggesting that confounding variables exist. We also observed a 30-day delay between the peak of threat actor activity and IOC publication for one provider. This study is the first empirical assessment linking threat intelligence publication to attacker behavior, bridging computer science and international relations.",
    "status": "done"
  },
  {
    "id": 1559,
    "year": 2025,
    "title": "TEMPEST-LoRa: Cross-Technology Covert Communication",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744817",
    "abstract": "Electromagnetic (EM) covert channels pose significant threats to computer and communications security in air-gapped networks. Previous works exploit EM radiation from various components (e.g., video cables, memory buses, CPUs) to secretly send sensitive information. These approaches typically require the attacker to deploy highly specialized receivers near the victim, which limits their real-world impact. This paper reports a new EM covert channel, TEMPEST-LoRa, that builds on Cross-Technology Covert Communication (CTCC), which could allow attackers to covertly transmit EM-modulated secret data from air-gapped networks to widely deployed operational LoRa receivers from afar. We reveal the potential risk and demonstrate the feasibility of CTCC by tackling practical challenges involved in manipulating video cables to precisely generate the EM leakage that could readily be received by third-party commercial LoRa nodes/gateways. Experiment results show that attackers can reliably decode secret data modulated by the EM leakage from a video cable at a maximum distance of 87.5m or a rate of 21.6 kbps. We note that the secret data transmission can be performed with monitors turned off (therefore covertly).",
    "status": "done"
  },
  {
    "id": 1560,
    "year": 2025,
    "title": "MOLE: Breaking GPU TEE with GPU-Embedded MCU",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744823",
    "abstract": "Graphics Processing Units (GPUs) are extensively used for applications such as machine learning, scientific computing, and graphics rendering. To protect sensitive data processed by GPUs, Trusted Execution Environments (TEEs) for GPUs have been proposed. GPU TEEs, built with hardware-based isolation primitives, can defend against high-privilege attackers like OS kernels. However, in this paper, we present MOLE, a novel attack that compromises the security of GPU TEEs on Arm Mali GPUs by exploiting the GPU-embedded Microcontroller Unit (MCU). By injecting malicious firmware into the MCU, an attacker can bypass GPU TEEs' security guarantees. We evaluated MOLE with state-of-the-art GPU TEE proposals under multiple real-world attack scenarios, such as in-GPU AES encryption and object detection tasks. Our evaluation shows that MOLE can successfully extract sensitive data or manipulate the computation results of GPU TEEs. We responsibly disclosed our findings to the authors of the affected GPU TEE proposals and received acknowledgments from all of them. Moreover, our findings prompted Arm to enhance the security of its GPU firmware supply chains.",
    "status": "done"
  },
  {
    "id": 1561,
    "year": 2025,
    "title": "WireTap: Breaking Server SGX via DRAM Bus Interposition",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765204",
    "abstract": "Intel's Software Guard eXtension (SGX) aims to offer strong integrity and confidentiality properties, even in the presence of root-level attackers. However, while Intel clearly indicates that SGX offers no security against attackers with physical access, many current real world SGX deployments are actually done in potentially adversarial environments, where node operators have a financial incentive to subvert computations performed inside SGX enclaves. While the two threat models clearly differ, a common conception is that physical attacks on SGX require expensive laboratory equipment, thus putting them out of reach of hobbyist-level attackers.In this work we challenge this belief, showing how simple memory bus interposition hardware can be constructed cheaply and easily in basic environments, using equipment easily purchased on the internet. We then combine our setup with SGX's recent migration from client CPUs to servers, which resulted in a weaker (and deterministic) memory encryption being used to encrypt the machine's physical memory. Applying our acquisition setup to SGX's attestation enclaves, we are able to extract an SGX attestation key from a machine in fully trusted status.Finally, we study the real world implication of such SGX breaches, by examining how SGX-backed blockchain deployments perform in the presence of these adversaries. As many of these deployments allow any SGX machine in trusted status to perform critical network functionality, we show end-to-end attacks on both confidentiality and integrity guarantees of deployments with multi-million dollar market caps, allowing attackers to disclose confidential transactions or illegitimately obtain transaction rewards.",
    "status": "done"
  },
  {
    "id": 1562,
    "year": 2025,
    "title": "One Video to Steal Them All: 3D-Printing IP Theft through Optical Side-Channels",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744837",
    "abstract": "The 3D printing industry is rapidly growing and increasingly adopted across various sectors, including manufacturing, healthcare, and defense. However, the operational setup often involves hazardous environments, necessitating remote monitoring through cameras and other sensors, which opens the door to cyber-based attacks. In this paper, we show that an adversary with access to video recordings of the 3D printing process can reverse-engineer the underlying 3D print instructions. Our model tracks the printer nozzle's movements during the printing process and maps the corresponding trajectory into G-code instructions. Further, it identifies the correct parameters, such as feed rate and extrusion rate, leading us to be able to successfully perform IP theft. To validate the success of IP theft, we design an equivalence checker that quantitatively compares two sets of 3D print instructions, evaluating their similarity in producing objects that are alike in shape, external appearance, and internal structure. Our equivalence checker, unlike other simple distance-based metrics such as normalized mean square error, is rotational as well as translational invariant. This is necessary to capture shifts in the base/start position of the reverse-engineered instructions relative to the actual 3D print instructions that can happen due to different camera positions. Our model achieves an average accuracy of 90.87\\% and generates 30.20\\% fewer instructions compared to the current state-of-the-art methods that produce instructions that either lead to faulty or incorrect (in terms of difference in shape and internal structure) 3D prints. Additionally, we use our model to reverse-engineer the 3D print instructions from a video recording and print a fully-functional counterfeit object.",
    "status": "done"
  },
  {
    "id": 1563,
    "year": 2025,
    "title": "ControlLoc: Physical-World Hijacking Attack on Camera-based Perception in Autonomous Driving",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744842",
    "abstract": "Recent research shows that adversarial patches can attack object detectors in camera-based perception for Autonomous Driving (AD). However, camera-based perception includes more than object detection; it also involves Multiple Object Tracking (MOT), which enhances robustness by requiring consistent detection across multiple frames before affecting tracking and thus, driving decisions. This makes attacks on object detection alone less effective. To attack such robust systems, a digital hijacking attack has been proposed, aiming to induce dangerous scenarios such as collisions. However, this attack has limited effectiveness, especially in the physical world.In this paper, we introduce a novel physical-world adversarial patch attack, ControlLoc, which exploits hijacking vulnerabilities in entire AD camera-based perception. ControlLoc utilizes a two-stage process: 1) identifying the optimal patch location and 2) generating the patch to modify the perceived location and shape of objects at that optimal location. Extensive experiments demonstrate the superior performance of ControlLoc, with an average attack success rate of around 98.1\\% across various AD camera-based perception and datasets, four times higher than that of the best existing method. Furthermore, the physical-world effectiveness of ControlLoc is validated in real vehicle tests under different conditions, such as outdoor lighting, angle, and background, achieving an average ASR of 79\\%. We also assess AD system-level impact with a production-grade AD simulator. ControlLoc yields a vehicle collision rate of 72.5\\% and an unnecessary emergency stop rate of 96.3\\%.",
    "status": "done"
  },
  {
    "id": 1564,
    "year": 2025,
    "title": "PipID: Light-Pupillary Response Based User Authentication for Virtual Reality",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744850",
    "abstract": "During the use of Virtual Reality (VR) applications such as gaming, education, and military training, sensitive information may be generated or collected by VR sensors, raising user concerns about potential data leakage. This highlights the critical need for effective user authentication to prevent unauthorized access. Existing authentication methods for VR are often either cumbersome (e.g., entering passwords via handheld controllers), reliant on specialized hardware (e.g., iris recognition), or vulnerable to credential replay attacks. In this study, we propose PipID, a lightweight VR authentication approach that leverages commercial off-the-shelf (COTS) eye trackers integrated into VR headsets. PipID is based on the fact that users' pupillary responses to visual stimuli vary uniquely. Thus, by displaying lights of randomly selected colors (i.e., wavelengths) on the VR screen, PipID can utilize pupil diameter responses to these wavelengths as the basis for authentication. For pupil data collected by precision-limited COTS eye trackers, PipID mitigates the impact of unrelated eye movements (e.g., blinks) and leverages pupillary response differences between the left and right eyes to further enhance the granularity of authentication features. Additionally, the randomized sequence of light colors helps prevent replay attacks. We implemented PipID on a COTS VR headset and tested it with 52 participants. Experimental results show that PipID achieves an accuracy of 98.65\\% and maintains robust performance under various conditions (e.g., keeping 98\\% and 91\\% accuracy after 7 and 14 days respectively).",
    "status": "done"
  },
  {
    "id": 1565,
    "year": 2025,
    "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744790",
    "abstract": "Modern processors are equipped with single instruction multiple data (SIMD) instructions for fine-grained data parallelism. Compiler auto-vectorization techniques that target SIMD instructions face performance limitations due to insufficient information available at compile time, requiring programmers to manually manipulate SIMD instructions. SIMD intrinsics, a type of built-in function provided by modern compilers, enable programmers to manipulate SIMD instructions within high-level programming languages. Bugs in compilers for SIMD intrinsics can introduce potential threats to software security, producing unintended calculation results, data loss, program crashes, etc.To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a randomized fuzzer that generates well-defined C programs that include various invocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design RVISmith to achieve the following objectives: (i) achieving high intrinsic coverage, (ii) improving sequence variety, and (iii) without known undefined behaviors. We implement RVISmith based on the ratified RVV intrinsic specification and evaluate our approach with three modern compilers: GCC, LLVM, and XuanTie. Experimental results show that RVISmith achieves 11.5 times higher intrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By differential testing that compares results across different compilers, optimizations, and equivalent programs, we detect and report 13 previously unknown bugs of the three compilers under test to date. Of these bugs, 10 are confirmed and another 3 are fixed by the compiler developers.",
    "status": "done"
  },
  {
    "id": 1566,
    "year": 2025,
    "title": "Fuzzing Processing Pipelines for Zero-Knowledge Circuits",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744791",
    "abstract": "Zero-knowledge (ZK) protocols have recently found numerous practical applications, such as in authentication, online-voting, and blockchain systems. These protocols are powered by highly complex pipelines that process deterministic programs, called circuits, written in one of many domain-specific programming languages, e.g., Circom, Noir, and others. Logic bugs in circuit-processing pipelines could have catastrophic consequences and cause significant financial and reputational damage. As an example, consider that a logic bug in a ZK pipeline could result in attackers stealing identities or assets. It is, therefore, critical to develop effective techniques for checking their correctness. In this paper, we present the first systematic fuzzing technique for ZK pipelines, which uses metamorphic test oracles to detect critical logic bugs. We have implemented our technique in a tool called Circuzz. We used Circuzz to test four significantly different ZK pipelines and found a total of 16 logic bugs in all pipelines. Due to their critical nature, 15 of our bugs have already been fixed by the pipeline developers.",
    "status": "done"
  },
  {
    "id": 1567,
    "year": 2025,
    "title": "Error Messages to Fuzzing: Detecting XPS Parsing Vulnerabilities in Windows Printing Components",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744807",
    "abstract": "Windows printing services remain a notable vector for attacks. Previous studies have predominantly targeted vulnerabilities within various control aspects of printing services, such as spooler services and firmware updates. Yet, we contend that an essential aspect of data processing—the document parser within printer drivers—has been overlooked in past research. We present a coverage-based fuzzing system, PrintXPSurge, specifically crafted to detect weaknesses in the XPS printer driver's parsing function. To craft semantically correct XPS files, we leverage a large language model-assisted repair approach to automate the creation of semantically correct XPS files that comply with necessary constraints. To ensure our fuzzing process effectively interacts with the XPS printer driver, we develop a progressive state reconstruction method that addresses individual dependency requirements across the entire printing service workflow. Furthermore, when a crash is detected, we employ backtracing to confirm its origin in the XPS parser, isolating it from other components in the pipeline. Our evaluation reveals that PrintXPSurge surpasses existing top Windows fuzzers in performance, successfully identifying 102 bugs in 10 drivers from major brands, including 17 zero-day vulnerabilities confirmed by Microsoft and third-party vendors.",
    "status": "done"
  },
  {
    "id": 1568,
    "year": 2025,
    "title": "SyzSpec: Specification Generation for Linux Kernel Fuzzing via Under-Constrained Symbolic Execution",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744811",
    "abstract": "Fuzzing has become one of the most effective and widely used techniques for discovering bugs and vulnerabilities, particularly in large-scale and complex programs like operating system kernels. A notable example is the kernel fuzzer syzkaller, which has identified over 6,800 bugs in the Linux kernel, with more than 5,500 already fixed. A crucial reason behind the success of the syzkaller is its collection of syscall descriptions, which are typically provided by human experts. Although some methods exist for automatically generating these syscall descriptions for device drivers, they often fall short when dealing with complex user inputs. These existing methods either lack precision or have a limited analysis scope, resulting in incomplete syscall descriptions.In this paper, we present SyzSpec, a tool designed to address these limitations by performing fully inter- procedural under- constrained symbolic execution on syscall handler functions. This approach enables SyzSpec to explore all possible user inputs and generate syscall descriptions with more precision. The primary innovation in SyzSpec is a novel method to improve symbolic pointer reasoning in under-constrained symbolic execution, working along with the under-under-constrained memory object (UCMO). We compared SyzSpec with existing automated solutions and manually written syscall descriptions from syzkaller. Our results demonstrate that SyzSpec achieves better coverage than other automated tools and offers coverage comparable to that of manually written syscall descriptions. Additionally, we evaluated SyzSpec on the latest stable version of the Linux kernel (v6.10) and identified 86 unique and previously unknown crashes across 11 different categories.",
    "status": "done"
  },
  {
    "id": 1569,
    "year": 2025,
    "title": "Validating Interior Gateway Routing Protocols via Equivalent Topology Synthesis",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744826",
    "abstract": "Routers, relying on routing protocols to determine how data packets travel across the Internet, serve as the backbone of modern networks. Vulnerable routing protocols can lead to serious consequences, including data leaks and network congestion. This work focuses on validating the implementation of a key class of routing protocols known as Interior Gateway Protocols (IGPs). Unlike communication protocols such as TCP/IP, which define structured data packets and state machines to facilitate communication, IGPs are designed to automatically manage the network topology. Thus, conventional techniques, which primarily focus on communication correctness, cannot be applied directly to IGPs. We propose ToDiff, a differential validation technique to uncover IGP bugs in three steps: (1) it uses a network generation algorithm to create random yet valid IGP networks, (2) it applies a semantics-guided program synthesizer to generate equivalent topological programs, and (3) it simulates the network via the equivalent topological programs, with any discrepancies suggesting the presence of a potential bug. We have evaluated ToDiff on the implementation of two common IGP protocols, OSPF and IS-IS. The results demonstrate that ToDiff outperforms existing approaches. To date, our tool has successfully identified 26 bugs, all confirmed or fixed by developers.",
    "status": "done"
  },
  {
    "id": 1570,
    "year": 2025,
    "title": "Disa: Accurate Learning-based Static Disassembly with Attentions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744828",
    "abstract": "For reverse engineering related security domains, such as vulnerability detection, malware analysis, and binary hardening, disassembly is crucial yet challenging. The fundamental challenge of disassembly is to identify instruction and function boundaries. Classic approaches rely on file-format assumptions and architecture-specific heuristics to guess the boundaries, resulting in incomplete and incorrect disassembly, especially when the binary is obfuscated. Recent advancements of disassembly have demonstrated that deep learning can improve both the accuracy and efficiency of disassembly. In this paper, we propose Disa, a new learning-based disassembly approach that uses the information of superset instructions over the multi-head self-attention to learn the instructions' correlations, thus being able to infer function entry-points and instruction boundaries. Disa can further identify instructions relevant to memory block boundaries to facilitate an advanced block-memory model based value-set analysis for an accurate control flow graph (CFG) generation. Our experiments show that Disa outperforms prior deep-learning disassembly approaches in function entry-point identification, especially achieving 9.1\\% and 13.2\\% F1-score improvement on binaries respectively obfuscated by the disassembly desynchronization technique and popular source-level obfuscator. By achieving an 18.5\\% improvement in the memory block precision, Disa generates more accurate CFGs with a 4.4\\% reduction in Average Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based approach.",
    "status": "done"
  },
  {
    "id": 1571,
    "year": 2025,
    "title": "Efficient Constant-Size Linkable Ring Signatures for Ad-Hoc Rings via Pairing-Based Set Membership Arguments",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744830",
    "abstract": "Linkable Ring Signatures (LRS) allow users to anonymously sign messages on behalf of ad-hoc rings, while ensuring that multiple signatures from the same user can be linked. This feature makes LRS widely used in privacy-preserving applications like e-voting and e-cash. To scale to systems with large user groups, efficient schemes with short signatures and fast verification are essential. Recent works, such as DualDory (ESORICS'22) and LLRing (ESORICS'24), improve verification efficiency through offline precomputations but rely on static rings, limiting their applicability in ad-hoc ring scenarios. Similarly, constant-size ring signature schemes based on accumulators face the same limitation.In this paper, we propose a framework for constructing constant-size LRS suitable for large ad-hoc rings. We introduce a novel pairing-based Set Membership Argument (SMA) with a proof size of only three group elements. By leveraging KZG polynomial commitments, we optimize the verification to require only constant group exponentiations and pairings, as well as linear field multiplications. Utilizing the SMA, our framework achieves constant-size signatures with verification dominated by linear field operations, outperforming existing schemes that require linear group exponentiations in ad-hoc ring settings. Moreover, it exhibits strong scalability: (i) compatibility with any PKI-based cryptosystem and (ii) scoped linkability, enabling flexible definitions of linking scope.We instantiate our framework using a discrete logarithm public key structure. On the BN254 curve, our signature size is fixed at 687 bytes, which to our best knowledge is the shortest LRS for ring sizes larger than 32. For a ring size of 1024, our verification cost is only 10.4 ms, achieving 48.6\\texttimes{}, 2.6\\texttimes{}–467\\texttimes{}, 7.9\\texttimes{}–13.2\\texttimes{}, and 2.2\\texttimes{}–102.5\\texttimes{} improvements over Omniring (CCS'19), DualDory (with and without precomputation), LLRing-DL (with and without precomputation), and LLRing-P (with and without precomputation), respectively. Moreover, this performance gap continues to grow as the ring size increases.",
    "status": "done"
  },
  {
    "id": 1572,
    "year": 2025,
    "title": "Forking the RANDAO: Manipulating Ethereum's Distributed Randomness Beacon",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744852",
    "abstract": "Proof-of-stake consensus protocols often rely on distributed randomness beacons (DRBs) to generate randomness for leader selection. This work analyses the manipulability of Ethereum's DRB implementation, RANDAO, in its current consensus mechanism. Even with its efficiency, RANDAO remains vulnerable to manipulation through the deliberate omission of blocks from the canonical chain. Previous research has shown that economically rational players can withhold blocks known as a block withholding attack or selfish mixing when the manipulated RANDAO outcome yields greater financial rewards. We introduce and evaluate a new manipulation strategy, the RANDAO forking attack. Unlike block withholding, whereby validators opt to hide a block, this strategy relies on selectively forking out an honest proposer's block to maximise transaction fee revenues and block rewards. In this paper, we draw attention to the fact that the forking attack is significantly more harmful than selfish mixing for two reasons. Firstly, it exacerbates the unfairness among validators. More importantly, it significantly undermines the reliability of the blockchain for the average user by frequently causing already published blocks to be forked out. By doing so, the attacker can fork the chain without losing slots, and we demonstrate that these are later fully compensated for. Our empirical measurements, investigating such manipulations on Ethereum mainnet, revealed no statistically significant traces of these attacks to date.",
    "status": "done"
  },
  {
    "id": 1573,
    "year": 2025,
    "title": "Mining in Logarithmic Space with Variable Difficulty",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744874",
    "abstract": "This paper presents the first non-interactive, succinct, and secure representation of a PoW-based blockchain that operates under variable mining difficulty while satisfying both completeness and onlineness properties. Completeness ensures that provers can update an existing NIPoPoW by incorporating a newly mined block, whereas onlineness ensures that miners can extend the chain directly from a NIPoPoW. The time complexity for both the prover (to update a NIPoPoW with a new block) and the verifier is logarithmic in the number of blocks of the underlying PoW blockchain. The communication complexity required for synchronization is polylogarithmic in the length of the blockchain. We prove the correctness of our scheme in the presence of a 1/3-bounded PPT adversary.",
    "status": "done"
  },
  {
    "id": 1574,
    "year": 2025,
    "title": "Bitcoin Under Volatile Block Rewards: How Mempool Statistics Can Influence Bitcoin Mining",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744875",
    "abstract": "The security of Bitcoin protocols is deeply dependent on the incentives provided to miners, which come from a combination of block rewards and transaction fees. As Bitcoin experiences more halving events, the protocol reward converges to zero, making transaction fees the primary source of miner rewards. This shift in Bitcoin's incentivization mechanism, which introduces volatility into block rewards, leads to the emergence of new security threats or intensifies existing ones. Previous security analyses of Bitcoin have either considered a fixed block reward model or a highly simplified volatile model, overlooking the complexities of Bitcoin's mempool behavior.In this paper, we present a reinforcement learning-based tool for analyzing mining strategies under a more realistic volatile reward model. The tool leverages the Asynchronous Advantage Actor-Critic (A3C) algorithm to derive near-optimal strategies while interacting with an environment that simulates the behavior of the Bitcoin mempool during any specified period, enabling analysis based on actual historical patterns. It supports the evaluation of adversarial mining strategies, such as selfish mining and undercutting, both before and after the difficulty adjustment, offering insights into the effects of mining attacks in both the short and long term.We revisit the Bitcoin security threshold presented in the WeRLman paper and demonstrate that the implicit predictability of valuable transaction arrivals in this model leads to an underestimation of the reported threshold. Additionally, we show that, while adversarial strategies like selfish mining under the fixed reward model incur an initial loss period of at least two weeks, the transition toward a transaction-fee era incentivizes mining pools to abandon honest mining for immediate profits. This incentive is expected to become more significant as the protocol reward approaches zero in the future.",
    "status": "done"
  },
  {
    "id": 1575,
    "year": 2025,
    "title": "On Frontrunning Risks in Batch-Order Fair Systems for Blockchains",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744879",
    "abstract": "In timing-sensitive blockchain applications, such as decentralized finance (DeFi), achieving first-come-first-served (FCFS) transaction ordering among decentralized nodes is critical to prevent frontrunning attacks. Themis [CCS'23], a state-of-the-art decentralized FCFS ordering system, has become a key reference point for high-throughput fair ordering systems for real-world blockchain applications, such as rollup chains and decentralized sequencing, and has influenced the design of several subsequent proposals. In this paper, we critically analyze its core system property of practical batch-order fairness and evaluate the frontrunning resistance claim of Themis. We present the Ambush attack, a new frontrunning technique that achieves nearly 100\\% success against the practical batch-order fair system with only a single malicious node and negligible attack costs. This attack causes a subtle temporary information asymmetry among nodes, which is allowed due to the heavily optimized communication model of the system. A fundamental trade-off we identify is a challenge in balancing security and performance in these systems; namely, enforcing timely dissemination of transaction information among nodes (to mitigate frontrunning) can easily lead to non-negligible network overheads (thus, degrading overall throughput performance). We show that it is yet possible to balance these two by delaying transaction dissemination to a certain tolerable level for frontrunning mitigation while maintaining high throughput. Our evaluation demonstrates that the proposed delayed gossiping mechanism can be seamlessly integrated into existing systems with only minimal changes.",
    "status": "done"
  },
  {
    "id": 1576,
    "year": 2025,
    "title": "Aegis: Tethering a Blockchain with Primary-Chain Stake",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744885",
    "abstract": "Blockchains implement decentralized monetary systems and applications. Recent advancements enable what we call tethering a blockchain to a primary blockchain, securing the tethered chain by nodes that post primary-chain tokens as collateral. The collateral ensures nodes behave as intended, until they withdraw it. Unlike a Proof of Stake blockchain which uses its own token as collateral, using primary-chain tokens shields the tethered chain from the volatility of its own token. State-of-the-art tethered blockchains either rely on centralization, or make extreme assumptions: that all communication is synchronous, that operators remain correct even post-withdrawal, or that withdrawals can be indefinitely delayed by tethered-chain failures. We prove that with partial synchrony, there is no solution to the problem. However, under the standard assumptions that communication with the primary chain is synchronous and communication among the tethered chain nodes is partially synchronous, there is a solution. We present a tethered-chain protocol called Aegis. Aegis uses references from its blocks to primary blocks to define committees, checkpoints on the primary chain to perpetuate decisions, and resets to establish new committees when previous ones become obsolete. It ensures safety at all times and rapid progress when latency among Aegis nodes is low.",
    "status": "done"
  },
  {
    "id": 1577,
    "year": 2025,
    "title": "Split Unlearning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744787",
    "abstract": "We introduce Split Unlearning, a novel machine unlearning technology designed for Split Learning (SL), enabling the first-ever implementation of Sharded, Isolated, Sliced, and Aggregated (SISA) unlearning in SL frameworks. Particularly, the tight coupling between clients and the server in existing SL frameworks results in frequent bidirectional data flows and iterative training across all clients, violating the ''Isolated'' principle and making them struggle to implement SISA for independent and efficient unlearning. To address this, we propose SplitWiper with a new one-way-one-off propagation scheme, which leverages the inherently ''Sharded'' structure of SL and decouples neural signal propagation between clients and the server, enabling effective SISA unlearning even in scenarios with absent clients. We further design SplitWiper+ to enhance client label privacy, which integrates differential privacy and label expansion strategy to defend the privacy of client labels against the server and other potential adversaries. Experiments across diverse data distributions and tasks demonstrate that SplitWiper achieves 0\\% accuracy for unlearned labels, and 8\\% better accuracy for retained labels than non-SISA unlearning in SL. Moreover, the one-way-one-off propagation maintains constant overhead, reducing computational and communication costs by 99\\%. SplitWiper+ preserves 90\\% of label privacy when sharing masked labels with the server.",
    "status": "done"
  },
  {
    "id": 1578,
    "year": 2025,
    "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744788",
    "abstract": "Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations. Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks. To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning. Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining. Slot implements semi-supervised learning with limited labels through efficient label similarity computation, significantly enhancing both detection performance and model robustness. By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks. Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies. Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods. Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection.",
    "status": "done"
  },
  {
    "id": 1579,
    "year": 2025,
    "title": "Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744792",
    "abstract": "Machine learning-based Android malware classifiers struggle with concept drift: the rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has largely centered on detecting drift samples, with expert-led label revisions on these samples to guide model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to high human labeling costs. To combat concept drift, we propose DREAM a novel system that establishes an explanatory drift detection and adaptation process. Our core idea is to integrate classifier and expert knowledge within a unified model. To achieve this, we embed malware behavioral concepts within the latent space of a contrastive autoencoder, while constraining sample reconstruction based on classifier predictions. This approach enhances retraining in two key ways: 1) capturing the target classifier's characteristics to select more effective samples in detection and 2) enabling concept revisions that extend the classifier's semantics to provide explainable guidance for adaptation. Additionally, Dream eliminates reliance on training data during real-time drift detection and provides a behavior-based explainer to support concept revision. Our evaluation shows that Dream effectively improves the drift detection accuracy and reduces the expert analysis effort in adaptation across different malware datasets and classifiers. Notably, when updating a widely-used Drebin classifier, Dream achieves the same accuracy with 76.6\\% fewer newly labeled samples compared to the best existing methods.",
    "status": "done"
  },
  {
    "id": 1580,
    "year": 2025,
    "title": "Rethinking Machine Unlearning in Image Generation Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744793",
    "abstract": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Data, source code, and models are available at https://github.com/ryliu68/IGMU.",
    "status": "done"
  },
  {
    "id": 1581,
    "year": 2025,
    "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744798",
    "abstract": "To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35\\texttimes{} (avg. 5.85\\texttimes{}) faster than the state-of-the-art work, without accuracy loss.",
    "status": "done"
  },
  {
    "id": 1582,
    "year": 2025,
    "title": "PoisonSpot: Precise Spotting of Clean-Label Backdoors via Fine-Grained Training Provenance Tracking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744802",
    "abstract": "Relying on untrusted data exposes machine learning models to backdoor attacks, where adversaries poison training data to embed hidden behaviors. Existing defenses struggle against increasingly stealthy attacks, particularly clean-label backdoor attacks, due to their inability to monitor fine-grained impact of individual training samples on model updates.In this paper, we present PoisonSpot, a novel system that precisely detects clean-label backdoor attacks by using fine-grained training provenance tracking, inspired by dynamic taint tracking. PoisonSpot captures and analyzes the impact of individual training samples on model parameter updates throughout the training process. By attributing poisoning scores to suspect samples based on their impact lineage, PoisonSpot allows for accurate identification and rejection of samples carrying backdoor triggers.We evaluate PoisonSpot on multiple benchmark datasets and attack scenarios, demonstrating its superior performance compared to the state-of-the-art clean-label backdoor poisoning defense. PoisonSpot consistently achieves high true positive rates, low false positive rates, and effectively mitigates backdoor attacks, even under adaptive adversarial strategies. Furthermore, PoisonSpot operates efficiently in various training settings, including retraining and fine-tuning regimes, demonstrating its robustness and scalability.",
    "status": "done"
  },
  {
    "id": 1583,
    "year": 2025,
    "title": "Pool: A Practical OT-based OPRF from Learning with Rounding",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765054",
    "abstract": "We propose Pool: a conceptually simple post-quantum (PQ) oblivious pseudorandom function (OPRF) protocol, that is round-optimal (with input-independent preprocessing), practically efficient, and has security based on the well-understood hardness of the learning with rounding (LWR) problem. Specifically, our design permits oblivious computation of the LWR-based pseudorandom function Fsk(x) = ⌉ H(x)⊤ ⋅ sk ⌋q,p, for random oracle H: {0,1} * → ℤ qn and uniformly chosen sk∈ {0,1} n. For 128-bits of semi-honest security, the Pool OPRF has an online communication cost of 11.9 kB, and a computational runtime of less than 3 ms on a single thread (via an open-source software implementation). This is more efficient (in either online communication cost or runtime) than constructions from well-known PQ PRFs, and is competitive even with constructions that only conjecture PQ security on lesser-known assumptions. As a result, our design gives high-performance, post-quantum variants of established OPRF applications in multi-party computation and private set operation protocols.",
    "status": "done"
  },
  {
    "id": 1584,
    "year": 2025,
    "title": "Zero-Knowledge AI Inference with High Precision",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765056",
    "abstract": "Artificial Intelligence as a Service (AIaaS) enables users to query a model hosted by a service provider and receive inference results from a pre-trained model. Although AIaaS makes artificial intelligence more accessible, particularly for resource-limited users, it also raises verifiability and privacy concerns for the client and server, respectively. While zero-knowledge proof techniques can address these concerns simultaneously, they incur high proving costs due to the non-linear operations involved in AI inference and suffer from precision loss because they rely on fixed-point representations to model real numbers.In this work, we present ZIP, an efficient and precise commit and prove zero-knowledge SNARK for AIaaS inference (both linear and non-linear layers) that natively supports IEEE-754 double-precision floating-point semantics while addressing reliability and privacy challenges inherent in AIaaS. At its core, ZIP introduces a novel relative-error-driven technique that efficiently proves the correctness of complex non-linear layers in AI inference computations without any loss of precision, and hardens existing lookup-table and range proofs with novel arithmetic constraints to defend against malicious provers. We implement ZIP and evaluate it on standard datasets (e.g., MNIST, UTKFace, and SST-2). Our experimental results show, for non-linear activation functions, ZIP reduces circuit size by up to three orders of magnitude while maintaining the full precision required by modern AI workloads.",
    "status": "done"
  },
  {
    "id": 1585,
    "year": 2025,
    "title": "New Permutation Decomposition Techniques for Efficient Homomorphic Permutation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765078",
    "abstract": "Homomorphic permutation is fundamental to privacy-preserving computations based on batch-encoding homomorphic encryption. It underpins nearly all homomorphic matrix operations and predominantly influences their complexity. Permutation decomposition as a potential approach to optimize this critical component remains underexplored. In this paper, we propose novel decomposition techniques to optimize homomorphic permutations, advancing homomorphic encryption-based privacy-preserving computations. We start by defining an ideal decomposition form for permutations and propose an algorithm searching for depth-1 ideal decompositions. Based on this, we prove the full-depth ideal decomposability of permutations used in specific homomorphic matrix transposition (HMT) and multiplication (HMM) algorithms, allowing them to achieve asymptotic improvement in speed and rotation key reduction. As a demonstration of applicability, substituting the HMM components in the best-known inference framework of encrypted neural networks with our enhanced version shows up to a 3.9\\texttimes{} reduction in latency. We further devise a new method for computing arbitrary homomorphic permutations, specifically those with weak structures that cannot be ideally decomposed. We design a network structure that deviates from the conventional scope of decomposition and outperforms the state-of-the-art technique under a limited rotation key budget, achieving a speed-up of up to 1.69 \\texttimes{}.",
    "status": "done"
  },
  {
    "id": 1586,
    "year": 2025,
    "title": "Leveraging Discrete CKKS to Bootstrap in High Precision",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765080",
    "abstract": "The CKKS fully homomorphic encryption (FHE) scheme enables computations on vectors of approximate complex numbers. A moderate precision of ≈ 20 bits often suffices but, in many applications, a higher precision is required for functionality and/or security. Indeed, to obtain IND-CPA-D security [Li-Micciancio; Eurocrypt'21], secure threshold-FHE [Asharov et al; Eurocrypt'12] and circuit privacy [Gentry; STOC'09], all known approaches require a precision that supports noise flooding. This may lead to a precision of ≈ 80 bits, or more. High-precision CKKS is hard to achieve, notably because of bootstrapping. The main difficulty is modulus consumption: every homomorphic multiplication consumes some, out of an overall modulus budget. Unfortunately, in high precision, most known bootstrapping algorithms consume so much modulus that one needs to increase the parameters to increase the budget. The state-of-the-art approach, Meta-BTS [Bae et al; CCS'22], performs moderate-precision bootstrapping several times to enable high-precision bootstrapping, with similar modulus consumption as the base bootstrapping it builds upon. It however damages latency.We introduce a new approach for high-precision CKKS bootstrapping, whose cost is almost independent of the precision (as opposed to Meta-BTS) and whose modulus consumption increases significantly more slowly than with classical bootstrapping algorithms. Our design relies on the EvalRound bootstrapping [Kim et al; Asiacrypt'22], which we improve in the high-precision context by leveraging and improving recent techniques for handling discrete data with CKKS. We obtain for the first time a non-iterative 80-bit precise bootstrapping algorithm which can be run in ring degree N=216, with 494 bits of remaining modulus for computations. In terms of throughput, and for 80-bit precision, our implementation shows an acceleration of 64\\% compared to Meta-BTS.",
    "status": "done"
  },
  {
    "id": 1587,
    "year": 2025,
    "title": "Grafting: Decoupled Scale Factors and Modulus in RNS-CKKS",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765083",
    "abstract": "The CKKS Fully Homomorphic Encryption (FHE) scheme enables approximate arithmetic on encrypted complex numbers for a desired precision. Most implementations use RNS with carefully chosen parameters to balance precision, efficiency, and security. However, a key limitation in RNS-CKKS is the rigid coupling between the scale factor, which determines numerical precision, and the modulus, which ensures security. Since these parameters serve distinct roles—one governing arithmetic correctness and the other defining cryptographic structure—this dependency imposes design constraints, such as a lack of suitable NTT primes and limited precision flexibility, ultimately leading to inefficiencies.We propose Grafting, a novel approach to decouple scale factors from the modulus by introducing (universal) sprouts, reusable modulus factors that optimize word-sized packing while allowing flexible rescaling. The universal sprouts allow rescaling by arbitrary bit-lengths and key-switching at any modulus bit-length without requiring additional key-switching keys, and thus enable universal use of the underlying parameters. Decoupling the scale factor from the modulus in Grafting yields significant efficiency gains: (1) Optimized RNS packing by decomposing the modulus into machine word-sized components, accelerating computations and reducing the ciphertext and encryption/evaluation key sizes; and (2) A freely adjustable scale factor independent of the modulus, unifying the ring structure across applications and reducing modulus consumption through adaptive scalings.Our experiments demonstrate that Grafting improves performance across standard SHE/FHE parameter sets for ring dimensions 214 -216 by up to 1.83X and 2.01X for key-switchings and multiplications, respectively, and up to 1.92X for bootstrapping. Grafting also reduces public key and ciphertext sizes by up to 62\\% without compression, maintaining the same number of public keys as before. As an application, we showcase the CKKS gate bootstrapping for bits (Bae et al; Eurocrypt'24), achieving 1.89X speed-up due to the reduced number of RNS factors. Finally, we revisit the homomorphic comparison (Cheon et al; Asiacrypt'20), evaluating it with carefully chosen scale factors for each iteration, reporting up to 204-bit fewer modulus consumption (27\\% reduction) in the standard parameter set, without precision loss.",
    "status": "done"
  },
  {
    "id": 1588,
    "year": 2025,
    "title": "Towards Verifiable FHE in Practice: Proving Correct Execution of TFHE's Bootstrapping using plonky2",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765097",
    "abstract": "In this work we demonstrate for the first time that a full FHE bootstrapping operation can be proven using a SNARK in practice. We do so by designing an arithmetic circuit for the bootstrapping operation and prove it using plonky2. We are able to prove the circuit on an AWS Hpc7a instance in under 20 minutes. Proof size is about 200 kB and verification takes less than 10 ms. As the basis of our bootstrapping operation we use TFHE's programmable bootstrapping and modify it in a few places to more efficiently represent it as an arithmetic circuit (while maintaining full functionality and security). In order to achieve our results in a memory-efficient way, we take advantage of the structure of the computation and plonky2's ability to efficiently prove its own verification circuit to implement a recursion-based IVC scheme. Lastly, we present a security proof in the UC model that captures active attacks in real world applications of verifiable FHE and augment our prototype to fit such applications.",
    "status": "done"
  },
  {
    "id": 1589,
    "year": 2025,
    "title": "Probabilistic Skipping-Based Data Structures with Robust Efficiency Guarantees",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765149",
    "abstract": "Probabilistic data structures like hash tables, skip lists, and treaps support efficient operations through randomized hierarchies that enable ''skipping'' elements, achieving sub-linear query complexity on average for perfectly correct responses. They serve as critical components in performance-sensitive systems where correctness is essential and efficiency is highly desirable. While simpler than deterministic alternatives like balanced search trees, these structures traditionally assume that input data are independent of the structure's internal randomness and state -- an assumption questionable in malicious environments -- potentially leading to a significantly increased query complexity. We present adaptive attacks on all three aforementioned structures that, in the case of hash tables and skip lists, cause exponential degradation compared to the input-independent setting. While efficiency-targeting attacks on hash tables are well-studied, our attacks on skip lists and treaps provide new insights into vulnerabilities of skipping-based probabilistic data structures. Next, we propose simple and efficient modifications to the original designs of these data structures to provide provable security against adaptive adversaries. Our approach is formalized through Adaptive Adversary Property Conservation (AAPC), a general security notion that captures deviation from the expected efficiency guarantees in adversarial scenarios. We use this notion to present rigorous robustness proofs for our versions of the data structures. Lastly, we perform experiments whose empirical results closely agree with our analytical results.",
    "status": "done"
  },
  {
    "id": 1590,
    "year": 2025,
    "title": "ShiftPIR: An Efficient PIR System with Gravity Shifting from Client to Server",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765153",
    "abstract": "We present ShiftPIR, a single-server Private Information Retrieval (PIR) protocol that gravity shifts both computation and communication overhead from the client to the server, thereby significantly improving overall efficiency. This shift is driven by the growing asymmetry between resource-constrained clients and compute-intensive servers, where server-side tasks can be effectively parallelized and scaled. To achieve this, ShiftPIR introduces a novel request generation method in which the client transmits only a compact plaintext offset derived from pre-uploaded seed ciphertexts. The server then reconstructs the full query ciphertexts using homomorphic rotations, eliminating the need for costly ciphertext generation and transmission on the client side. We further design a highly parallelizable query expansion mechanism that removes data dependencies between ciphertext rotations, enabling efficient GPU-based execution. Our experiments demonstrate that ShiftPIR reduces client-side latency to microseconds while maintaining a communication cost within 4X of the non-private baseline—far outperforming prior protocols with 104 -105 \\texttimes{} overhead. Compared to the state-of-the-art protocol YPIR, ShiftPIR achieves up to 26X lower end-to-end latency.",
    "status": "done"
  },
  {
    "id": 1591,
    "year": 2025,
    "title": "Updatable aPAKE: Security Against Bulk Precomputation Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765155",
    "abstract": "Asymmetric Password-Authenticated Key Exchange (aPAKE) enables secure key establishment between a client and a server using a pre-shared password, while providing security against offline attacks. However, aPAKE does not guarantee any precomputation resistance, and considers passwords to become immediately available upon server compromise. A recent work by Dayanikli and Lehmann (EuroS&amp;P'24) observed that many existing aPAKE protocols provide stronger precomputation attack resistance than what is guaranteed through the aPAKE model: they often rely on salted password hashes, where a unique salt makes precomputation attacks more difficult. While these salts are sent in clear to the client during authentication, and thus trivial to obtain for an attacker, this makes a difference in multi-user settings with millions of user accounts per server. In order to run bulk precomputation attacks on all users' passwords, the attacker needs to start an authentication session on behalf of every user to obtain their salts. However, this protection is still limited as salts are static, and the attacker can gradually extract all salt values for precomputation attacks.In this work, we build upon the observation that many aPAKE protocols include salts for their password protection, and propose a new aPAKE variant that makes such bulk precomputation attacks practically infeasible. We propose updatable aPAKE which employs updatable salts. In updatable aPAKE, the salt is implicitly refreshed with each successful user authentication, forcing an attacker to rebuild their precomputation table after every honest user's login - offering a level of precomputation resistance similar to that of strong aPAKE protocols. We formalize the security of updatable aPAKE in the Universal Composability framework and show how OKAPE-HMQV, the currently most efficient aPAKE protocol, can be lifted to the updatable aPAKE setting in a provably secure way. The core idea is that this salt update can be integrated through relying on the password-based server-side authentication, that is already guaranteed through aPAKE. We also observe that OKAPE-HMQV is very similar to SRP-6a, the currently most widely deployed aPAKE protocol, and explain how the same idea can be used to upgrade this legacy protocol to achieve strong bulk precomputation attack resistance with minimal overhead.",
    "status": "done"
  },
  {
    "id": 1592,
    "year": 2025,
    "title": "Founding Zero-Knowledge Proof of Training on Optimum Vicinity",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744862",
    "abstract": "Zero-knowledge proofs of training (zkPoT) allow a party to prove that a model is trained correctly on a committed dataset without revealing any additional information about the model or the dataset. Existing zkPoT protocols prove the entire training process in zero knowledge; i.e., they prove that the final model was obtained in an iterative fashion starting from the training data and a random seed (and potentially other parameters) and applying the correct algorithm at each iteration. This approach inherently requires the prover to perform work linear to the number of iterations.In this paper, we take a different approach to proving the correctness of model training. Our approach is motivated by efficiency but also more urgently by the observation that the prover's ability to pick the random seed used for training introduces the potential for it to bias the model. In other words, if the input to the training algorithm is biased, the resulting model will be biased even if the prover correctly ran the training algorithm. Rather than prove the correctness of the training process, we thus directly prove the correctness of the training model using a notion we call optimum vicinity, which bounds the distance between the trained model and the mathematically optimal model for models that can be viewed as the solution to a convex optimization problem. We show both theoretically and experimentally that this ensures the trained model behaves similarly to the optimal model, and show this is not true for existing approaches. We also demonstrate significant performance improvements as compared to the existing zkPoT paradigm: the statement proven in ZK in our protocol has a size independent of the number of training iterations, and our Boolean (respectively arithmetic) circuit size is up to 246\\texttimes{} (respectively 5\\texttimes{}) smaller than that of a baseline zkPoT protocol that verifies the whole training process.",
    "status": "done"
  },
  {
    "id": 1593,
    "year": 2025,
    "title": "Revisiting Keyed-Verification Anonymous Credentials",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765163",
    "abstract": "Keyed-verification anonymous credentials (KVACs) have demonstrated their practicality through large-scale deployments in Apple, Google, Signal, and Tor. Despite their widespread adoption, the theoretical framework underlying KVACs lacks the flexibility needed to support diverse applications, which in general require different security properties. For instance, rate-limiting credentials only need a weaker unforgeability notion (one-more unforgeability), yet the framework cannot easily accommodate this relaxation. Similarly, digital identity protocols require stronger properties than unforgeability -—specifically, extractability for security proofs when adversaries can observe other users' credentials. We address these limitations by introducing new notions of extractability and one-more unforgeability. We improve two foundational works in the space: The scheme by Chase et al. (CCS 2014), commonly referred to as CMZ or PS MAC can be made statistically anonymous, and issuance cost reduced from (O(n)) to (O(1)).The scheme by Barki et al. (SAC 2016), known as BBDT or BBS MAC can be issued more efficiently (one less group element). We provide a security proof for both schemes in the algebraic group model (CRYPTO 2018) and describe how these core credential schemes can be extended to construct more complex anonymous credential systems such as time-based policies, pseudonyms, and rate-limiting extensions.",
    "status": "done"
  },
  {
    "id": 1594,
    "year": 2025,
    "title": "Subversion-resilient Key-exchange in the Post-quantum World",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765165",
    "abstract": "Subversion-resilient Authenticated key-exchange (AKE) aims to achieve the guarantees of secure AKE even in the presence of an adversary that has tampered with parts of the protocol's implementation. One way to achieve subversion-resilient AKE is the use of Reverse Firewalls (RFs), an untrusted third-party that can restore security. Recent work[17] highlights the challenges of designing RFs for practical secure channel-establishment.This paper extends existing RF-based subversion-resilient AKE at three levels: security definitions, constructions, and the use of formal verification. First, we introduce a useful relaxation of the notion of security in subversion-resilient AKE with RFs: the goal is no longer to prevent all exfiltration, but rather to restore to the AKE protocol a property lost upon subversion. We focus specifically on authenticating and (key-)securing RFs. We also discuss subversion-resilience against a spectrum of compromises, designing a flexible framework in which protocols are proved secure with respect to adversaries that can tamper with some components of the implementation, but perhaps not others.Our ultimate goal is to achieve post-quantum secure subversion-resilient key-exchange. Far from being trivial, this requires the introduction of a malleable-yet-secure notion of key-encapsulation, which we dub re-randomizable Key Encapsulation Mechanism. We carefully formalize this new primitive and instantiate it first based on a classical Diffie-Hellman KEM and one based on Kyber.Finally, we lay the foundations for the formal verification of RF based protocols, by formally proving our protocol with the CryptoVerif prover, in addition to computational-security proofs in usual Bellare-Rogaway methodology.",
    "status": "done"
  },
  {
    "id": 1595,
    "year": 2025,
    "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744821",
    "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to provide plausible deniability of their data. However, this also makes LDP vulnerable to poisoning attacks. In this paper, we first introduce novel poisoning attacks for ranking estimation. These attacks are intricate, as fake attackers do not merely adjust the frequency of target items. Instead, they leverage a limited number of fake users to precisely modify frequencies, effectively altering item rankings to maximize gains. To tackle this challenge, we introduce the concepts of attack cost and optimal attack item (set), and propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we iteratively select optimal attack items and allocate suitable fake users. For OUE, we iteratively determine optimal attack item sets and consider the incremental changes in item frequencies across different sets. Regarding OLH, we develop a harmonic cost function based on the pre-image of a hash to select that supporting a larger number of effective attack items. Lastly, we present an attack strategy based on confidence levels to quantify the probability of a successful attack and the number of attack iterations more precisely. We demonstrate the effectiveness of our attacks through theoretical and empirical evidence, highlighting the necessity for defenses against these attacks. The source code and data have been made available at https://github.com/LDP-user/LDP-Ranking.git.",
    "status": "done"
  },
  {
    "id": 1596,
    "year": 2025,
    "title": "Mitigating Data Poisoning Attacks to Local Differential Privacy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744839",
    "abstract": "The distributed nature of local differential privacy (LDP) invites data poisoning attacks and poses unforeseen threats to the underlying LDP-supported applications. In this paper, we propose a comprehensive mitigation framework for popular frequency estimation, which contains a suite of novel defenses, including malicious user detection, attack pattern recognition, and damaged utility recovery. In addition to existing attacks, we explore new adaptive adversarial activities for our mitigation design. For detection, we present a new method to precisely identify bogus reports, and thus LDP aggregation can be performed over the ''clean'' data. When the attack behavior becomes stealthy and direct filtering out malicious users is difficult, we further propose a detection that can effectively recognize hidden adversarial patterns, thus facilitating the decision-making of service providers. These detection methods require no additional data or attack information and incur minimal computational cost. Our experiment demonstrates their excellent performance and substantial improvement over previous work in various settings. In addition, we conduct an empirical analysis of LDP post-processing for corrupted data recovery and propose a new post-processing method, through which we reveal new insights into protocol recommendations in practice and key design principles for future research.",
    "status": "done"
  },
  {
    "id": 1597,
    "year": 2025,
    "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744840",
    "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present \\o{}urattackfull (\\o{}urattack), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76\\texttimes{} more frequently than those generated by our attack. We observe a 2\\texttimes{} improvement in TPR@1\\%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.",
    "status": "done"
  },
  {
    "id": 1598,
    "year": 2025,
    "title": "Exploiting the Shared Storage API",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744848",
    "abstract": "As part of an effort to replace third-party cookies, Google introduced the Shared Storage API as one of their ''Privacy Sandbox'' proposals. The Shared Storage API seeks to replace some of the benign functionalities that third-party cookies facilitate while mitigating the potential privacy harms that they can cause, such as reidentifying users across websites. Shared Storage seeks to do this by allowing third parties to store data that is not partitioned by top-level website, but limiting read access to those data. We find that the implementation and design of the API have flaws that allow for both the reidentification of users across sites and the leakage of more data than intended by Google. With the API being deployed in Google Chrome and major advertisers and trackers having completed the processes required to gain access to the API, the Shared Storage API may not do as much as intended to improve the state of privacy on the web. We present several attacks on the API that circumvent the key goals laid out by Google as well as discuss potential extensions and mitigation strategies. While we have responsibly disclosed our attacks to Google, most attacks remain possible in Chrome.",
    "status": "done"
  },
  {
    "id": 1599,
    "year": 2025,
    "title": "Fingerprinting SDKs for Mobile Apps and Where to Find Them: Understanding the Market for Device Fingerprinting",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744877",
    "abstract": "This paper presents a large-scale analysis of fingerprinting-like behavior in the mobile application ecosystem. We take a market-based approach, focusing on third-party tracking as enabled by applications' common use of third-party SDKs. Our dataset consists of over 228,000 SDKs from popular Maven repositories, 178,000 Android applications collected from the Google Play store, and our static analysis pipeline detects exfiltration of over 500 individual signals. To the best of our knowledge, this represents the largest-scale analysis of SDK behavior undertaken to date.We find that Ads SDKs (the ostensible focus of industry efforts such as Apple's App Tracking Transparency and Google's Privacy Sandbox) appear to be the source of only 30.56\\% of the fingerprinting behaviors. A surprising 23.92\\% originate from SDKs whose purpose was unknown or unclear. Furthermore, Security and Authentication SDKs are linked to only 11.7\\% of likely fingerprinting instances. These results suggest that addressing fingerprinting solely in specific market-segment contexts like advertising may offer incomplete benefit. Enforcing anti-fingerprinting policies is also complex, as we observe a sparse distribution of signals and APIs used by likely fingerprinting SDKs. For instance, only 2\\% of exfiltrated APIs are used by more than 75\\% of SDKs, making it difficult to rely on user permissions to control fingerprinting behavior.",
    "status": "done"
  },
  {
    "id": 1600,
    "year": 2025,
    "title": "PAnDA: Rethinking Metric Differential Privacy Optimization at Scale with Anchor-Based Approximation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765042",
    "abstract": "Metric Differential Privacy (mDP) extends the local differential privacy (LDP) framework to metric spaces, enabling more nuanced privacy protection for data such as geo-locations. However, existing mDP optimization methods, particularly those based on linear programming (LP), face scalability challenges due to the quadratic growth in decision variables.In this paper, we propose Perturbation via Anchor-based Distributed Approximation (PAnDA), a scalable two-phase framework for optimizing metric differential privacy (mDP). To reduce computational overhead, PAnDA allows each user to select a small set of anchor records, enabling the server to solve a compact linear program over a reduced domain. We introduce three anchor selection strategies, exponential decay (PAnDA-e), power-law decay (PAnDA-p), and logistic decay (PAnDA-l), and establish theoretical guarantees under a relaxed privacy notion called probabilistic mDP (PmDP). Experiments on real-world geo-location datasets demonstrate that PAnDA scales to secret domains with up to 5,000 records, two times larger than prior LP-based methods, while providing theoretical guarantees for both privacy and utility.",
    "status": "done"
  },
  {
    "id": 1601,
    "year": 2025,
    "title": "Lock the Door But Keep the Window Open: Extracting App-Protected Accessibility Information from Browser-Rendered Websites",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744822",
    "abstract": "The Android accessibility (a11y) service has been widely utilized by malware to abuse benign services. To prevent such abuse, developers need to secure a11y content access in both their apps and mobile websites. However, a misalignment of a11y protection mechanisms exists between them. Prior research has focused on attacking and defending a11y information embedded in native Android apps. However, our research found that a11y malware can retrieve app-protected a11y information in its mobile browser-rendered website counterpart, leaving mobile browser users more vulnerable to a11y attacks than app users. To help benign service developers vet this attack surface, we developed SOMBRA, an automated analysis pipeline to vet browser-side leakage of a11y information that is a11y-protected in apps. Using SOMBRA, we analyzed 294 benign services and found 29 of them deploy app-side a11y protection mechanisms to secure 256 views. SOMBRA discovered that 241, 402, 244, and 251 elements corresponding to their protected app-side views are a11y-exposed in their websites rendered by Chrome, Firefox, Brave, and Edge browsers, respectively. The leaked elements contain sensitive personal identifiable information. Finally, SOMBRA discovered that most developers do not adopt browser-side a11y protections because existing mechanisms either have ineffective protection or hinder the usability of their content.",
    "status": "done"
  },
  {
    "id": 1602,
    "year": 2025,
    "title": "BACScan: Automatic Black-Box Detection of Broken-Access-Control Vulnerabilities in Web Applications",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744825",
    "abstract": "Broken-Access-Control (BAC) vulnerabilities have consistently been ranked among the most critical security risks in web applications, occupying the top positions in the OWASP Top 10 over the past several years. These vulnerabilities allow attackers to bypass access control mechanisms and perform unauthorized operations, posing serious security and privacy threats to sensitive business and user data. Despite substantial attention given to BAC vulnerabilities, effective and reliable approaches to detecting these issues remain limited. In this work, we present BACScan, a novel black-box approach to detect BAC vulnerabilities in web applications. Unlike existing response similarity-based oracles that check only unauthorized read accesses, BACScan introduces an innovative feedback-driven oracle, which determines whether unauthorized read or modification operations have occurred by inferring operationally-dependent web pages and analyzing the operational feedback. We evaluated BACScan on 20 real-world applications and successfully identified 89 vulnerabilities, including 54 previously unreported ones, outperforming state-of-the-art tools. We reported all newly identified vulnerabilities to the affected vendors. To date, 35 new CVE IDs have been assigned.",
    "status": "done"
  },
  {
    "id": 1603,
    "year": 2025,
    "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744829",
    "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability. To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07\\% on baseline dataset compared to state-of-the-art PDF malware classifiers.",
    "status": "done"
  },
  {
    "id": 1604,
    "year": 2025,
    "title": "Local Frames: Exploiting Inherited Origins to Bypass Content Blockers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744843",
    "abstract": "We present a study of how local frames (i.e., iframes loading content like ''about:blank'') are mishandled by a wide range of popular Web security and privacy tools. As a result, users of these tools remain vulnerable to the very attack techniques against which they seek to protect themselves, including browser fingerprinting, cookie-based tracking, and data exfiltration. The tools we study are vulnerable in different ways, but all share a root cause: legacy Web functionality interacts with browser privacy boundaries in unexpected ways, leading to systemic vulnerabilities in tools developed, maintained, and recommended by privacy experts and activists. We consider four core capabilities supported by most privacy tools and develop tests to determine whether each can be evaded through the use of local frames. We apply our tests to six popular Web privacy and security tools—identifying at least one vulnerability in each for a total of 19—and extract common patterns regarding their mishandling of local frames. Our measurement of popular websites finds that 56\\% employ local frames and that 73.7\\% of the requests made by these local frames should be blocked by popular filter lists but instead trigger the vulnerabilities we identify. From another perspective, 14.3\\% of all sites that we crawl make requests that should be blocked inside of local frames. We disclosed these vulnerabilities to the tool authors and discuss both our experiences working with them to patch their products and the implications of our findings for other privacy and security research.",
    "status": "done"
  },
  {
    "id": 1605,
    "year": 2025,
    "title": "Enhanced Web Application Security Through Proactive Dead Drop Resolver Remediation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744860",
    "abstract": "Dead Drop Resolver (DDR) malware evades traditional Command and Control (C&amp;C) server takedowns by dynamically resolving C&amp;C addresses hosted on popular web applications, such as Dropbox and Pastebin. These addresses are often manipulated (i.e., encoded or encrypted), rendering existing detection techniques largely ineffective. To tackle this challenge, we introduce VADER, a malware forensics system specifically designed for the proactive detection of dead drops. Analyzing a dataset of 100k malware samples collected in the wild, VADER identified 8,906 DDR malware samples from 110 families that leverage 273 dead drops across seven web applications. Additionally, it proactively uncovered 57.1\\% more dead drops spanning 11 web applications. Case studies revealed that over 40\\% of DDR malware samples employ sophisticated, layered de-manipulation algorithms, highlighting the prevalence and complexity of this evasion technique. Beyond detection, VADER enabled proactive remediation by discovering 13 previously unknown dead drops from a single DDR malware sample. This approach empowers web application providers to systematically scan their platforms, enabling the early detection and mitigation of dead drops.",
    "status": "done"
  },
  {
    "id": 1606,
    "year": 2025,
    "title": "ForeDroid: Scenario-Aware Analysis for Android Malware Detection and Explanation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765207",
    "abstract": "Android malware continues to evolve, posing significant challenges in generalization, fine-grained detection, and interpretability for existing detection systems. Existing methods struggle to generalize to unseen malware, lack fine-grained behavioral understanding, and provide limited interpretability due to their reliance on rigid rules or the inability to recover complete causal behavior paths. To this end, we present ForeDroid, a unified and interpretable framework for Android malware detection and explanation via scenario-aware analysis. ForeDroid models malicious intent as behavioral inconsistencies within functional scenarios. It clusters semantically coherent scenarios, extracts sensitive API call chains, and summarizes them into natural language using LLMs. These summaries are embedded and compared against benign behavior distributions within the same scenario for unsupervised anomaly detection. High-risk behaviors showing strong semantic inconsistency are further interpreted by an LLM-driven module that generates fine-grained anomaly reports. We evaluated ForeDroid on two challenging tasks: zero-day malware detection and fine-grained behavior analysis. The result shows ForeDroid outperforms MaMaDroid, MalScan, DeepRefiner, and a continuous learning-based approach in zero-day malware detection under the temporal-split setting. Besides, ForeDroid achieves an F1-score of 0.94 in fine-grained behavior detection on the manually annotated GPMalware dataset, surpassing ProMal. Our results demonstrate ForeDroid's ability to bridge low-level call graph analysis with high-level semantic reasoning, making it a practical, interpretable solution for malware detection.",
    "status": "done"
  },
  {
    "id": 1607,
    "year": 2025,
    "title": "Logical Relations for Formally Verified Authenticated Data Structures",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744801",
    "abstract": "Authenticated data structures allow untrusted third parties to carry out operations which produce proofs that can be used to verify an operation's output. Such data structures are challenging to develop and implement correctly. This paper gives a formal proof of security and correctness for a library that generates authenticated versions of data structures automatically. The proof is based on a new relational separation logic for reasoning about programs that use collision-resistant cryptographic hash functions. This logic provides a basis for constructing two semantic models of a type system, which are used to justify how the library makes use of type abstraction to enforce security and correctness. Using these models, we also prove the correctness of several optimizations to the library and then show how optimized, hand-written implementations of authenticated data structures can be soundly linked with automatically generated code. All of the results in this paper have been mechanized in the Rocq prover using the Iris framework.",
    "status": "done"
  },
  {
    "id": 1608,
    "year": 2025,
    "title": "Jazzline: Composable CryptoLine Functional Correctness Proofs for Jasmin Programs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744814",
    "abstract": "Jasmin is a programming language for high-speed and high-assurance cryptography. Correctness proofs of Jasmin programs are typically carried out deductively in EasyCrypt. This allows generality, modularity and composable reasoning, but does not scale well for low-level architecture-specific routines. CryptoLine offers a semi-automatic approach to formally verify algebraically-rich low-level cryptographic routines. CryptoLine proofs are self-contained: they are not integrated into higher-level formal verification developments. This paper shows how to soundly use CryptoLine to discharge subgoals in functional correctness proofs for complex Jasmin programs. We extend Jasmin with annotations and provide an automatic translation into a CryptoLine model, where most complex transformations are certified. We also formalize and implement the automatic extraction of the semantics of a CryptoLine proof to EasyCrypt. Our motivating use-case is the X-Wing hybrid KEM, for which we present the first formally verified implementation.",
    "status": "done"
  },
  {
    "id": 1609,
    "year": 2025,
    "title": "Breaking and Provably Restoring Authentication: A Formal Analysis of SPDM 1.2 including Cross-Protocol Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744865",
    "abstract": "The SPDM (Security Protocol and Data Model) protocol is a standard under development by the DMTF consortium, and supported by major industry players including Broadcom, Cisco, Dell, Google, HP, IBM, Intel, and NVIDIA. SPDM 1.2 is a complex protocol that aims to provide platform security, for example for communicating hardware components or cloud computing scenarios. SPDM is the core security mechanism of PCI Express (PCIe) and Compute Express Link (CXL). In this work, we provide the first holistic, formal analysis of SPDM 1.2: we model the full protocol flow of SPDM considering all of its modes -- especially the complex interaction between its different key-exchange modes -- in the framework of the Tamarin prover, making our resulting model one of the most complex Tamarin models to date. To our surprise, Tamarin finds a cross-protocol attack that allows a network attacker to completely break authentication of the pre-shared key mode. We implemented our attack on the SPDM reference implementation, and reported the issue to the SPDM developers. DMTF registered our attack as a CVE with CVSS rating 9 (critical). We propose a fix and develop the first formal symbolic proof using the Tamarin prover for the fixed SPDM 1.2 protocol as a whole. The resulting model of the main modes and their interactions is highly complex, and we develop supporting lemmas to enable proving properties in the Tamarin prover, including the absence of all cross-protocol attacks. Our fix has been incorporated into both the reference implementation and the newest version of the standard. Our results highlight the need for a holistic analysis of other internet standards and the importance of providing generalized security guarantees across entire protocols.",
    "status": "done"
  },
  {
    "id": 1610,
    "year": 2025,
    "title": "Approximate Algorithms for Verifying Differential Privacy with Gaussian Distributions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765043",
    "abstract": "The verification of differential privacy algorithms that employ Gaussian distributions is little understood. This paper tackles the challenge of verifying such programs by introducing a novel approach to approximating probability distributions of loop-free programs that sample from both discrete and continuous distributions with computable probability density functions, including Gaussian and Laplace. We establish that verifying (∈, δ)-differential privacy for these programs is almost decidable, meaning the problem is decidable for all values of δ except those in a finite set. Our verification algorithm is based on computing probabilities to any desired precision by combining integral approximations, and tail probability bounds. The proposed methods are implemented in the tool, DipApprox, using the FLINT library for high-precision integral computations, and incorporate optimizations to enhance scalability. We validate DiPApprox on fundamental privacy-preserving algorithms, such as Gaussian variants of the Sparse Vector Technique and Noisy Max, demonstrating its effectiveness in both confirming privacy guarantees and detecting violations.",
    "status": "done"
  },
  {
    "id": 1611,
    "year": 2025,
    "title": "ILA: Correctness via Type Checking for Fully Homomorphic Encryption",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765068",
    "abstract": "RLWE-based Fully Homomorphic Encryption (FHE) schemes add some small noise to the message during encryption. The noise accumulates with each homomorphic operation. When the noise exceeds a critical value, the FHE circuit produces an incorrect output. This makes developing FHE applications quite subtle, as one must closely track the noise to ensure correctness. However, existing libraries and compilers offer limited support to statically track the noise. Additionally, FHE circuits are also plagued by wraparound errors that are common in finite modulus arithmetic. These two limitations of existing compilers and libraries make FHE applications too difficult to develop with confidence.In this work, we present a correctness-oriented IR, Intermediate Language for Arithmetic Circuits (ILA), for type-checking circuits intended for homomorphic evaluation. Our IR is backed by a type system that tracks low-level quantitative bounds (e.g., ciphertext noise) without using the secret key. Using our type system, we identify and prove a strong functional correctness criterion for ILA circuits. Additionally, we have designed ILA to be maximally general: our core type system does not directly assume a particular FHE scheme, but instead axiomitizes a model of FHE. We instantiate this model with the exact FHE schemes (BGV, BFV and TFHE), and obtain functional correctness for free.We implement a concrete type checker ILA, parameterized by the noise estimators for three popular FHE libraries (OpenFHE, SEAL and TFHE-rs). We also use the type checker to infer the optimal placement of modulus switching, a common noise management operation. Evaluation shows that ILA type checker is sound (always detects noise overflows), practical (noise estimates are tight) and efficient.",
    "status": "done"
  },
  {
    "id": 1612,
    "year": 2025,
    "title": "Protocols to Code: Formal Verification of a Secure Next-Generation Internet Router",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765104",
    "abstract": "We present the first formally-verified Internet router, which is part of the SCION Internet architecture. SCION routers run a cryptographic protocol for secure packet forwarding in an adversarial environment. We verify both the protocol's network-wide security properties and the low-level properties of its implementation. Namely, we develop a series of protocol models by refinement in Isabelle/HOL and we use an automated program verifier to prove that the router's Go code satisfies crash freedom, freedom from data races, and adheres to the most concrete model in our series of refinements. Both verification efforts are soundly linked together.Our work demonstrates the feasibility of coherently verifying a security-critical network component from high-level protocol models down to performance-optimized production code, developed by an independent team. In the process, we uncovered critical attacks and bugs in both the protocol and its implementation, which were confirmed by the code developers, and we strengthened the protocol's security properties. This paper presents the challenges we faced when verifying an existing real-world system, explains our approach to tackling these challenges, summarizes the main results, and distills valuable lessons for the verification of secure systems, in particular for the techniques and tools employed.",
    "status": "done"
  },
  {
    "id": 1613,
    "year": 2025,
    "title": "SyzParam: Incorporating Runtime Parameters into Kernel Driver Fuzzing",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744838",
    "abstract": "Under the monolithic architecture of the Linux kernel, all its components operate within the same address space. Notably, device drivers constitute over half of the kernel codebase yet are particularly prone to bugs. Therefore, exploring vulnerabilities in drivers is critical for ensuring kernel security. Extensive research has been done to fuzz kernel drivers through system calls and hardware interrupts. Through a comprehensive study of the Linux Kernel Device Model, we identified that the execution of device drivers is also influenced by runtime parameters, including device attributes and kernel module parameters. Our analysis reveals that large portions of the uncovered code are masked by these parameters, which are exposed to the userspace through a specialized virtual file system known as sysfs. Furthermore, adjacent devices interconnected within the same device tree also impact drivers' behavior.This paper introduces a novel fuzzing framework, SyzParam, which incorporates runtime parameters into the fuzzing process. Achieving this objective requires addressing several key challenges, including valid value extraction, inter-device relation construction, and fuzz engine integration. By inspecting the data structures and functions associated with the LKDM, our tool can extract runtime parameters across various drivers through static analysis. Additionally, SyzParam collects inter-device relations and identifies associations between runtime parameters and drivers. Furthermore, SyzParam proposes a novel mutation strategy, which leverages these relations and prioritizes parameter modification during related driver execution. Our evaluation demonstrates that SyzParam outperforms existing fuzzing works in driver code coverage and bug-detection capabilities. To date, we have identified 30 unique bugs in the latest kernel upstreams, including 10 CVEs.",
    "status": "done"
  },
  {
    "id": 1614,
    "year": 2025,
    "title": "Reviving Discarded Vulnerabilities: Exploiting Previously Unexploitable Linux Kernel Bugs Through Control Metadata Fields",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744841",
    "abstract": "Linux kernel vulnerabilities represent a critical security threat in modern computing systems, with hundreds of new vulnerabilities discovered annually. Traditional security practices often discard vulnerabilities offering only weak primitives as ''unexploitable'', creating a significant blind spot in kernel security. This paper presents a novel approach to revive these previously discarded vulnerabilities by exploiting Control Metadata Fields (CMFs) within Linux objects, rather than traditional pointer manipulation.Our CMF-based method overcomes two major limitations of existing approaches: it bypasses modern security measures like pointer authentication code and eliminates the need for precise pointer alignment that weak primitives cannot reliably achieve. Using MetaXploit, our automated analysis tool, we identified 54 exploitable CMFs across Ubuntu and Debian distributions. Empirical testing against 20 real-world vulnerabilities demonstrated successful exploitation in 18 cases, including 12 previously discarded vulnerabilities. These results challenge traditional assumptions about vulnerability exploitability and suggest that many discarded vulnerabilities in the Linux kernel warrant reevaluation.",
    "status": "done"
  },
  {
    "id": 1615,
    "year": 2025,
    "title": "BASTAG: Byte-level Access Control on Shared Memory using ARM Memory Tagging Extension",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744849",
    "abstract": "As software grows in size and complexity, modular designs are increasingly adopted, leading to frequent interactions via shared memory between components. This design however increases the risk of vulnerabilities from uncontrolled memory access to shared memory. Enforcing byte-level access control can mitigate these risks by enabling byte-level permissions on complex shared objects and their sub-elements. However, existing approaches face performance limitations as they increase the granularity of control to byte level. In this paper, we present BASTAG, a novel system that leverages ARM's Memory Tagging Extension (MTE) to tack this challenge. Although MTE enforces tag-matching between pointers and memory, its hardware-defined granularity is too coarse to support byte-level control on its own. To address the inherent limitations of applying MTE for nuanced access control, BASTAG incorporates a technique known as shadow memory tagging that places separate, but associated MTE tags for the actual memory targets, allowing for more flexible and finer access control with efficiency. We implemented a BASTAG prototype on AArch64 hardware with MTE support and evaluated it on three real-world use cases. Our results demonstrate that BASTAG significantly outperforms existing byte-level access control mechanisms.",
    "status": "done"
  },
  {
    "id": 1616,
    "year": 2025,
    "title": "Intent-aware Fuzzing for Android Hardened Application",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744858",
    "abstract": "The widespread adoption of app hardening techniques in Android applications makes it more challenging for current analysis techniques to analyze hardened apps, leading to limited analysis coverage. This limitation is mainly due to the difficulties in obtaining detailed information about Intents, which is essential for component communication and the execution of specific events in Android applications.In this paper, we introduce eBPF-based AHA-Fuzz, the first intent-aware greybox fuzzing framework for Android hardened applications. AHA-Fuzz proposes a valid intent generator to create valid intent inputs that can trigger diverse Android app behaviors. To precisely evaluate the impact of these inputs, AHA-Fuzz presents a selective coverage feedback approach. Additionally, AHA-Fuzz introduces approaches for efficiently triggering hard-to-trigger bugs (e.g., scheduled malware) and detecting information leaks in hardened applications. Our evaluation results demonstrate that AHA-Fuzz triggers 92.3\\% more intents 3.45\\texttimes{} faster and executes 23.9\\% more methods than previous approaches. Additionally, AHA-Fuzz has discovered 47 previously unknown bugs that existing approaches cannot detect. The developers of Google, Firefox, and Facebook have acknowledged 6 out of 47 bugs, and have already fixed three of them.",
    "status": "done"
  },
  {
    "id": 1617,
    "year": 2025,
    "title": "Securing Mixed Rust with Hardware Capabilities",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744861",
    "abstract": "The Rust programming language enforces three basic Rust principles, namely ownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security bugs such as memory safety violations and data races. However, Rust projects often have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign Function Interfaces), and inline assembly for low-level control. The Rust compiler is unable to statically enforce Rust principles in mixed Rust code which can lead to many security vulnerabilities. In this paper, we propose CapsLock, a security enforcement mechanism that can run at the level of machine code and detect Rust principle violations at run-time in mixed code. CapsLock is kept simple enough to be implemented into recent capability-based hardware abstractions that provide low-cost spatial memory safety. CapsLock introduces a novel revoke-on-use abstraction for capability-based designs, wherein accessing a memory object via a capability implicitly invalidates certain other capabilities pointing to it, thereby also providing temporal memory safety automatically, without requiring software to explicitly specify such invalidation. Thus, CapsLock is the first mechanism capable of providing cross-language enforcement of Rust principles. We implemented a prototype of CapsLock on QEMU. Evaluation results show that CapsLock is highly compatible with existing Rust code (passing 99.7\\% of the built-in test cases of the 100 most popular crates) and flags Rust principle violations in real-world Rust projects that use FFI or inline assembly. We discovered 8 previously unknown bugs in such crates in our experiments.",
    "status": "done"
  },
  {
    "id": 1618,
    "year": 2025,
    "title": "PromeFuzz: A Knowledge-Driven Approach to Fuzzing Harness Generation with Large Language Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765222",
    "abstract": "API-level fuzzing has become increasingly important for discovering subtle bugs in modern software, yet generating effective fuzzing harnesses remains a complex and error-prone task. Existing approaches often rely on limited consumer code or shallow program analysis, which fail to capture deep API semantics and interdependencies, resulting in poor coverage and high false positive rates. Recent methods incorporating Large Language Models (LLMs) have improved harness generation by leveraging pretrained knowledge, but they still struggle with hallucinations and lack domain-specific understanding.To address the challenges, we present PromeFuzz, a knowledge-driven framework for automatic fuzzing harness generation using LLMs. PromeFuzz constructs a structured knowledge base by combining code metadata, API documentation, and real-world call correlations to enhance the semantic accuracy and coverage of generated harnesses. It further integrates retrieval-augmented generation and a dedicated sanitizer module to refine harness quality and triage crashes. We evaluate PromeFuzz on 22 open-source projects, demonstrating significant improvements over state-of-the-art tools. Specifically, PromeFuzz achieves 1.50\\texttimes{}, 3.88\\texttimes{}, 1.91\\texttimes{} and 1.40\\texttimes{} higher branch coverage than 3 LLM-based baselines (PromptFuzz, CKGFuzzer and OSS-Fuzz-Gen) and manually crafted harnesses (OSS-Fuzz), respectively. It also discovers more unique crashes with 89.7\\% precision and uncovers 25 previously unknown vulnerabilities (21 confirmed by the developer and 3 assigned with CVE IDs).",
    "status": "done"
  },
  {
    "id": 1619,
    "year": 2025,
    "title": "Swallow: A Transfer-Robust Website Fingerprinting Attack via Consistent Feature Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744795",
    "abstract": "Website fingerprinting (WF) attacks on Tor networks can analyze traffic patterns to identify the websites Tor users are visiting, and thus pose a significant threat to user privacy. In a real-world environment, Tor users face diverse network conditions and can also employ WF defenses, raising new challenges to launch WF attacks. The state-of-the-art (SOTA) WF attacks either rely on a strong assumption that WF classifiers are trained and deployed under the same network condition, or suffer from significant performance degradation against WF defenses. In this paper, we propose Swallow, a transfer-robust WF attack that can quickly transfer to new network conditions while maintaining robustness against various WF defenses. Specifically, we propose a novel trace representation named Consistent Interaction Feature (CIF), which aligns traffic distributions across different network conditions to capture consistent features. Then we design three data augmentation algorithms to simulate potential variations under various network conditions. We extensively evaluate Swallow using ten datasets, including both self-collected and public datasets. The closed- and open-world evaluation results demonstrate that Swallow significantly outperforms the SOTA attacks. In particular, with only 5 labeled instances per website for model fine-tuning, Swallow achieves an average improvement in accuracy of 17.50\\% over the SOTA WF attacks.",
    "status": "done"
  },
  {
    "id": 1620,
    "year": 2025,
    "title": "FlowSentry: Accelerating NetFlow-based DDoS Detection",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744800",
    "abstract": "Distributed Denial of Service (DDoS) attacks threaten the stability of online services by overwhelming them with excessive traffic. NetFlow-based DDoS detection systems are widely adopted by Internet Service Providers (ISPs) in upstream multi-point detection scenarios to provide robust detection for volumetric DDoS attacks. However, these systems face inherent delays, as NetFlow detection is non-instantaneous—routers aggregate and summarize flow records over a period before reporting, which impacts timely detection. Existing research primarily focuses on optimizing the NetFlow reporting mechanism at the router side. Unfortunately, the need for either software or hardware upgrades for routers would incur a high deployment cost, which is impractical for ISPs in the short term. In this paper, we propose FlowSentry, a novel NetFlow detection framework to accelerate DDoS attack identification at the server side. The system operates on a dual-layer filtering paradigm to handle the high-frequency NetFlow records, incorporating two core technologies: ADWindow and STAnalyzer. ADWindow is a sketch-based sliding window mechanism designed to retain possibly anomalous flow information, filtering out benign flows to reduce the computational overhead. STAnalyzer leverages the cross-router traffic correlation to efficiently infer abnormal growth patterns of potential malicious traffic based on partially reported flow records, thus significantly reducing the detection delay. Our extensive experiments in simulated backbone network environments demonstrate that FlowSentry achieves better detection accuracy while reducing the detection delay by up to 65.63\\% compared to existing methods.",
    "status": "done"
  },
  {
    "id": 1621,
    "year": 2025,
    "title": "1BIT: Persistent Path Validation with Customized Noise Signal Characteristics",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744819",
    "abstract": "Path-aware networks have garnered significant attention as an emerging research area. It allows network senders to actively select or influence transmission paths to meet specific requirements, which necessitates the support of path validation mechanisms. Supported by the path-aware networking research group under the Internet Engineering Task Force (IETF), path validation plays a crucial role in enhancing end hosts' control over packet forwarding. However, existing methods face trade-offs among security, protocol header overhead, and computational cost, forming a ''trilemma.'' Drawing inspiration from persistent validation in zero-trust architecture, we propose the 1BIT protocol. This protocol reduces protocol header overhead by more than 57\\% while providing robust data flow security. The packet demand for path fault detection is reduced by more than 72\\%, and fault locations can be precisely identified. By employing hash algorithms and few binary operations, the 1BIT protocol achieves high throughput and supports routers capable of adapting to high-speed, multi-interface environments. On a 16-core CPU, the 1BIT protocol can handle throughput exceeding 100 Gbps. This lightweight and efficient solution introduces anomaly signal detection techniques into the field of path validation. Benefiting from in-depth research on anomaly signal detection, this technology offers a richer set of solutions for path validation and lays the foundation for future research and implementation in areas such as multi-path validation and path privacy protection.",
    "status": "done"
  },
  {
    "id": 1622,
    "year": 2025,
    "title": "RebirthDay Attack: Reviving DNS Cache Poisoning with the Birthday Paradox",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744832",
    "abstract": "DNS cache poisoning is a persistent game of attack and defense, posing an enduring challenge for the DNS community. Significant efforts have been made to uncover, detect, and mitigate vulnerabilities that increase the risk of cache poisoning. However, no work has systematically revisited whether the original cache poisoning attack based on the Birthday Paradox remains effective. In this work, we introduce RebirthDay, a novel DNS cache poisoning attack targeting recursive resolvers and forwarders, reviving the classic DNS Birthday attack that no longer works since 2002. RebirthDay exploits newly uncovered, protocol-compliant vulnerabilities in DNS extension implementations to bypass the query aggregation mechanism intended to prevent DNS Birthday attacks that has not been well understood. We uncovered that 18 out of 22 mainstream DNS software are vulnerable due to weaknesses in the processing of a DNS extension (i.e., ECS option), specifically lacking or incorrectly implemented ECS coherence checks when handling DNS queries and responses, demonstrating the widespread susceptibility to RebirthDay. These flaws could be exploited to circumvent the query aggregation mechanism and launch RebirthDay attacks. Through comprehensive evaluation, we showed that RebirthDay attacks are highly practical and can have significant real-world impact, affecting 16 router vendors, 14 public DNS services, and 365K (15\\%) open DNS resolvers. We have reported the identified vulnerabilities to affected vendors and discussed mitigation solutions with them. To date, we have received acknowledgments from 8 vendors, including BIND, Unbound, PowerDNS, and Quad9, and have been assigned 50 CVE-ids. Our study emphasizes the need for greater attention to the importance of ECS verification and DNS extension implementations, revealing new security risks introduced by them.",
    "status": "done"
  },
  {
    "id": 1623,
    "year": 2025,
    "title": "5G-RNAKA : A Random Number-based Authentication and Key Agreement Protocol for 5G Systems",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744844",
    "abstract": "The 5G-AKA protocol, defined by 3GPP for authentication and key agreement in 5G networks, remains vulnerable to linkability, synchronization failure, and Sequence Number (SQN) exposure attacks. These issues threaten user privacy and service availability. Existing improvements often retain these flaws or cause high overhead due to continued use of the legacy SQN mechanism from 3G. In this paper, we propose 5G-RNAKA, a secure and efficient AKA protocol for 5G systems. Unlike 5G-AKA, 5G-RNAKA eliminates SQN counters and instead utilizes random numbers generated by the Universal Subscriber Identity Module (USIM) in 5G User Equipment (UE) for session identification. This random number is embedded in the reply message from the service network (SN) to prevent replay attacks against the UE. Additionally, by removing the SQN mechanism, 5G-RNAKA enhances user privacy by preventing attackers from linking challenge-response sessions. It also enables the UE to authenticate the SN, effectively mitigating the risk of SN impersonation. We formally verify that 5G-RNAKA achieves its security goals of privacy, authentication, and secrecy using the state-of-the-art formal verification tool, Tamarin Prover. Our implementation and evaluation further demonstrate that 5G-RNAKA improves communication efficiency and reduces storage overhead. While primarily designed for 5G, 5G-RNAKA's features align with emerging trends in 6G authentication, suggesting its potential for adaptation to future 6G architectures.",
    "status": "done"
  },
  {
    "id": 1624,
    "year": 2025,
    "title": "Discovering and Exploiting IoT Device Hidden Attributes: A New Vulnerability in Smart Homes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744847",
    "abstract": "With the growing popularity and pervasive adoption of smart home Internet of Things (IoT) platforms, IoT security and privacy issues are gaining more attention. In this work, we reveal a new vulnerability inherent in most smart home IoT automation platforms and systems but previously unnoticed by the security community: the hidden attributes, i.e., attributes that are configurable by knowledgeable attackers through IoT APIs to effectively change device behaviors, but these attributes are not manageable or observable by users. An IoT device with compromised hidden attributes may behave differently from user expectations and cause severe security and safety consequences (e.g., burglary or fire). We present the root causes of the vulnerability and develop an approach to systematically discover hidden attributes. We evaluate a total of 31 commodity IoT devices of various types from 16 manufacturers and identify hidden attributes in all of them. Furthermore, we select several IoT devices with security and safety-critical hidden attributes and demonstrate the end-to-end hidden attribute attack on two popular IoT platforms: Samsung SmartThings and Amazon Alexa. In addition, we develop a tool that can automatically patch edge drivers and fix the hidden attribute issue. The source code of the auto-patching tool can be found in the https://anonymous.4open.science/r/SmartThings-Edge-Driver-Auto-Patching-49CE/README.md Anonymous GitHub.",
    "status": "done"
  },
  {
    "id": 1625,
    "year": 2025,
    "title": "MM4flow: A Pre-trained Multi-modal Model for Versatile Network Traffic Analysis",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744804",
    "abstract": "Network traffic analysis is a critical research area, playing an essential role in enhancing network security and ensuring high-quality network services. Existing methods, which primarily rely on a single modality, face two significant limitations. First, while existing approaches may achieve strong performance in specific tasks, they often lack sufficient adaptability for diverse tasks. Second, existing pre-trained models are only trained with GB-scale traffic, with which increases the risk of over-fitting and limiting the models' overall performance. To address these challenges, we propose MM4flow, a pre-trained multi-modal model designed for versatile network traffic analysis. We divide network flows into two modalities: raw byte streams and transmission patterns, which encapsulate the content and behavior information, respectively. MM4flow is composed of two key stages: uni-modal pre-training and multi-modal fine-tuning. We develop an efficient data collection scheme enabling TB-scale traffic pre-training. Leveraging a real-world traffic that exceeds 70 TB, MM4flow conducts uni-modal pre-training on each modality with a modified BERT architecture tailored for network flows. For specific downstream tasks, we introduce a modal fusion module based on cross-attention mechanisms. The fusion module facilitates effective integration of multi-modal information, enabling MM4flow to fully utilize both content and behavior cues during fine-tuning with minimal labeled dataset. We evaluate MM4flow on six public datasets covering six various tasks. Extensive experiments demonstrate that MM4flow achieves superior accuracy than baselines. Especially, compared to existing pre-trained models, MM4flow achieves an 84\\% improvement in accuracy for website identification under encrypted tunnels. Moreover, the pre-trained MM4flow significantly reduces the reliance on high-quality labeled training data for downstream tasks.",
    "status": "done"
  },
  {
    "id": 1626,
    "year": 2025,
    "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744805",
    "abstract": "The growing adoption of large pre-trained models in edge computing has made deploying model inference on mobile clients both practical and popular. These devices are inherently vulnerable to direct adversarial attacks, which pose a substantial threat to the robustness and security of deployed models. Federated adversarial training (FAT) has emerged as an effective solution to enhance model robustness while preserving client privacy. However, FAT frequently produces a generalized global model, which struggles to address the diverse and heterogeneous data distributions across clients, resulting in insufficiently personalized performance, while also encountering substantial communication challenges during the training process. In this paper, we propose Sylva, a personalized collaborative adversarial training framework designed to deliver customized defense models for each client through a two-phase process. In Phase 1, Sylva employs LoRA for local adversarial fine-tuning, enabling clients to personalize model robustness while drastically reducing communication costs by uploading only LoRA parameters during federated aggregation. In Phase 2, a game-based layer selection strategy is introduced to enhance accuracy on benign data, further refining the personalized model. This approach ensures that each client receives a tailored defense model that balances robustness and accuracy effectively. Extensive experiments on benchmark datasets demonstrate that Sylva can achieve up to 50\\texttimes{} improvements in communication efficiency compared to state-of-the-art algorithms, while achieving up to 29.5\\% and 50.4\\% enhancements in adversarial robustness and benign accuracy, respectively.",
    "status": "done"
  },
  {
    "id": 1627,
    "year": 2025,
    "title": "Unmask Tampering: Efficient Document Tampering Localization under Recapturing Attacks with Real Distortion Knowledge",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744809",
    "abstract": "Document tampering localization (DTL) aims to detect tampering traces and ensure the integrity of document images. However, recapturing attacks ( i.e., printing and scanning the altered document images) can effectively conceal tampering traces due to distortions such as halftoning, blurring, and noise. Compounding this challenge, the collection of real recaptured document samples is both time-consuming and resource-intensive. It is important to investigate an efficient method that adapt the existing DTL models to unmask the threat from recapturing attack. In this work, we tackle these challenges by first proposing a Real Halftone-based Document Synthesis (RHSyn) method to generate realistic recaptured document images. RHSyn exploits reference halftone patterns with a novel table look-up operation, which incorporates real-world distortions from printers and scanners to produce high-fidelity synthetic data. To improve DTL performance, we introduce a Masked Parameter-Efficient Fine-Tuning (M-PEFT) technique to facilitate extracting distinctive forensic features from text and background regions under recapturing attacks. In the experiment, we gather two extensive testing datasets comprising over 6,600 real recaptured document images from 9 printers and 7 scanners. Experimental results under recapturing attacks demonstrate that the performances of the existing DTL models are significantly improved with RHSyn-generated data via M-PEFT. Specifically, our approach achieves an average F1-Score of 0.611 across three test datasets, increased by 0.496 compared to models without fine-tuning, demonstrating its capacity to effectively counter the threat of recapturing attacks.",
    "status": "done"
  },
  {
    "id": 1628,
    "year": 2025,
    "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744813",
    "abstract": "In recent years, tremendous success has been witnessed in Retrieval-Augmented Generation (RAG), widely used to enhance Large Language Models (LLMs) in domain-specific, knowledge-intensive, and privacy-sensitive tasks. However, attackers may steal those valuable RAGs and deploy or commercialize them, making it essential to detect Intellectual Property (IP) infringement. Most existing ownership protection solutions, such as watermarks, are designed for relational databases and texts. They cannot be directly applied to RAGs because relational database watermarks require white-box access to detect IP infringement, which is unrealistic for the knowledge base in RAGs. Meanwhile, post-processing by the adversary's deployed LLMs typically destructs text watermark information. To address those problems, we propose a novel black-box ''knowledge watermark'' approach, named RAG-WM, to detect IP infringement of RAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark Generator, Shadow LLM \\&amp; RAG, and Watermark Discriminator, to create watermark texts based on watermark entity-relationship tuples and inject them into the target RAG. We evaluate RAG-WM across three domain-specific and two privacy-sensitive tasks on four benchmark LLMs. Experimental results show that RAG-WM effectively detects the stolen RAGs in various deployed LLMs. Furthermore, RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also evade watermark detection approaches, highlighting its promising application in detecting IP infringement of RAG systems.",
    "status": "done"
  },
  {
    "id": 1629,
    "year": 2025,
    "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744818",
    "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR—either by developing new methods to enhance these metrics or using them to evaluate privacy solutions—we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.",
    "status": "done"
  },
  {
    "id": 1630,
    "year": 2025,
    "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744820",
    "abstract": "The inference process of modern large language models (LLMs) demands prohibitive computational resources, rendering them infeasible for deployment on consumer-grade devices. To address this limitation, recent studies propose distributed LLM inference frameworks, which employ split learning principles to enable collaborative LLM inference on resource-constrained hardware. However, distributing LLM layers across participants requires the transmission of intermediate outputs, which may introduce privacy risks to the original input prompts --- a critical issue that has yet to be thoroughly explored in the literature. In this paper, we rigorously examine the privacy vulnerabilities of distributed LLM inference frameworks by designing and evaluating three prompt inference attacks aimed at reconstructing input prompts from intermediate LLM outputs. These attacks are developed under various query and data constraints to reflect diverse real-world LLM service scenarios. Specifically, the first attack assumes an unlimited query budget and access to an auxiliary dataset sharing the same distribution as the target prompts. The second attack also leverages unlimited queries but uses an auxiliary dataset with a distribution differing from the target prompts. The third attack operates under the most restrictive scenario, with limited query budgets and no auxiliary dataset available. We evaluate these attacks on a range of LLMs, including state-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used models like GPT-2 and BERT for comparative analysis. Our experiments show that the first two attacks achieve reconstruction accuracies exceeding 90\\%, while the third achieves accuracies typically above 50\\%, even under stringent constraints. These findings highlight substantial privacy risks in distributed LLM inference frameworks, issuing a strong alert on their deployment in real-world applications. Additionally, our analysis uncovers distinct distributional properties of intermediate embeddings across LLM layers, providing valuable insights into the LLM inference process and the development of effective defense mechanisms for distributed LLM frameworks.",
    "status": "done"
  },
  {
    "id": 1631,
    "year": 2025,
    "title": "Differentially Private Access in Encrypted Search: Achieving Privacy at a Small Cost?",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765038",
    "abstract": "Encrypted search focuses on protecting sensitive data in outsourced environments while enabling private queries. Although standard encrypted search algorithms are efficient, they often leak some information about the queries and data. One such leakage is the access pattern on the outsourced storage. Recent leakage-abuse attacks have exploited this seemingly harmless leakage to successfully recover both queries and data, shifting research priorities towards finding the right balance between privacy and performance. While some proposals leverage oblivious RAM or other oblivious data structures to hide the access pattern, they typically incur significant bandwidth costs. In response, researchers have developed new schemes that ensure access leakage satisfies differential privacy (DP). Yet the security implications of these new guarantees remain unclear. Especially, compared with conventional differential privacy, the application and threat model are significantly different. To understand these implications, we investigate two concrete instances of (encrypted) range-query schemes (appeared in SODA '19 and CCS '22) that achieve differentially private access. We analyze their security guarantees using inference attacks to recover queries and data on real-world datasets. Our findings raise a critical concern that ensuring access leakage is differentially private either falls short of providing strong security for the queries and data, diverging from the initial goals, or offers only weak security but at a high efficiency/correctness cost. As part of our analysis, we also propose a generic security definition for DP access, and identify two general techniques for leakage mitigation, bucketization and partitioning, that may be of independent interest.",
    "status": "done"
  },
  {
    "id": 1632,
    "year": 2025,
    "title": "Silent Threshold Traitor Tracing \\&amp; Enhancing Mempool Privacy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765099",
    "abstract": "The rising commerciality of cryptocurrencies and blockchains in DeFi applications raises the importance of implementing robust methods to protect its regular users against parties with enormous amounts of resources that allows them to balefully influence the market through various Maximal Extractable Value (MEV) strategies. There are typical situations in such scenarios where an end-user may want to hide its transaction details till it has been executed. Ensuring the privacy of pending mempool transactions thus becomes an important goal. To this end, various decentralized versions of threshold encryption (TE) systems have been recently shown particularly effective.In this work we aim to enhance the privacy of such mempool transactions' through a new primitive that we call silent threshold traitor-tracing (ST3). In addition to providing a thresholdized decryption facility through a quorum of some T parties, ST3 enables public tracing of (at least one) member from such a malicious quorum in TE systems (thereby adding accountability to TE), but without the presence of any trusted authority. For mempool transactions, such a tracing functionality allows to trace member(s) of a malicious decryption committee who are in-charge of validating pending mempool transactions together. Compared to recent works in this space on silent TE (STE) by Garg et al. (CRYPTO 2024) and threshold traitor-tracing (T3) by Boneh et al. (CRYPTO 2024), ST3 achieves the best of both worlds: It is endowed with a silent setup (like STE) and also allows public tracing in a malicious decryption committee (unlike T3). Our work resolves two important problems left open by Boneh et al. (CRYPTO 2024). We present two constructions of ST3. The first one is a generic compiler whereas the second one is built from the notion of a distributed predicate encryption with pooled decryption (DPEPD) -- a primitive we define and build from bilinear pairings, proving its security in the GGM. DPEPD may be of independent interest with applications beyond ST3. Our techniques are inspired from two recent works: one is STE (as above) and the other one is a concurrent work of Branco et al. (ASIACRYPT 2024) that builds traitor-tracing (TT) schemes without a trusted authority. Our second ST3 scheme achieves a transparent setup -- a property that is deemed largely beneficial for any practical deployment. Further, both our schemes have sublinear ciphertext sizes O(√L), where L is the number of users in the system at any time. We also benchmark a prototype of the second ST3 scheme in Go to demonstrate its application in encrypted mempools. Our results showcase the practicality of our schemes: For L= 256 parties with a threshold T = ⌊ 2L/3⌋ = 170, encryption, partial decryption, decryption aggregation and tracing algorithms take 166.8 ms, 15.8 ms, 589 ms and 9195 ms respectively with a ciphertext size of 12.57 KB.",
    "status": "done"
  },
  {
    "id": 1633,
    "year": 2025,
    "title": "PULSE: Parallel Private Set Union for Large-Scale Entities",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765108",
    "abstract": "Multi-party private set union (mPSU) allows multiple parties to compute the union of their private input sets without revealing any additional information. Existing efficient mPSU protocols can be categorized into symmetric key encryption (SKE)-based and public key encryption (PKE)-based approaches. However, neither type of mPSU protocol scales efficiently to a large number of parties, as they fail to fully utilize available computational resources, leaving participants idle during various stages of the protocol execution.This work examines the limitation of existing protocols and proposes a unified framework for designing efficient mPSU protocols. We then introduce an efficient Parallel mPSU for Large-Scale Entities (PULSE) that enables parallel computation, allowing all parties/entities to perform computations without idle time, leading to significant efficiency improvements, particularly as the number of parties increases. Our protocol is based on PKE and secure even when up to n-1 semi-honest parties are corrupted. We implemented PULSE and compared it to state-of-the-art mPSU protocols under different settings, showing a speedup of 1.91 to 3.57X for n=8 parties for various set sizes.",
    "status": "done"
  },
  {
    "id": 1634,
    "year": 2025,
    "title": "Velox: Scalable Fair Asynchronous MPC from Lightweight Cryptography",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765114",
    "abstract": "Multi-party computation (MPC) enables a set of mutually n distrusting parties to compute any function on their private inputs. Mainly, MPC facilitates agreement on the function's output while preserving the secrecy of honest inputs, even against a subset of t parties controlled by an adversary. With applications spanning from anonymous broadcast to private auctions, MPC is considered a cornerstone of distributed cryptography, and significant research efforts have been aimed at making MPC practical in the last decade. However, most libraries either make strong assumptions like the network being bounded synchronous, or incur high computation overhead from the extensive use of expensive public-key operations that prevent them from scaling beyond a few dozen parties.This work presents Velox, an asynchronous MPC protocol that offers fairness against an optimal adversary corrupting up to t &lt; nover 3 parties. Velox significantly enhances practicality by leveraging lightweight cryptographic primitives - such as symmetric-key encryption and hash functions - which are 2-3 orders of magnitude faster than public-key operations, resulting in substantial computational efficiency. Moreover, Velox is highly communication-efficient, with linear amortized communication relative to circuit size and only O (n3) field elements of additive overhead. Concretely, Velox requires just 9.33 field elements per party per multiplication gate, more than 10X reduction compared to the state of the art. Moreover, Velox also offers Post-Quantum Security as lightweight cryptographic primitives retain their security against a quantum adversary.We implement Velox comprehensively, covering both offline and online phases, and evaluate its performance on a geographically distributed testbed through a real-world application: anonymous broadcast. Our implementation securely shuffles a batch of k=256 messages in 4 seconds with n=16 parties and 18 seconds with n=64 parties, a 36X and 28.6X reduction in latency compared to the prior best work. At scale with n112 parties, Velox is able to shuffle the same batch of messages in under 50 seconds from end to end, illustrating its effectiveness and scalability. Overall, our work removes significant barriers faced by prior asynchronous MPC solutions, making asynchronous MPC practical and efficient for large-scale deployments involving 100s of parties.",
    "status": "done"
  },
  {
    "id": 1635,
    "year": 2025,
    "title": "How to Recover a Cryptographic Secret From the Cloud",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765127",
    "abstract": "Clouds have replaced most local backup systems as they offer strong availability and reliability guarantees. Clouds, however, are not (and should not be) used as backup for cryptographic secrets. Cryptographic secrets might control financial assets (e.g., crypto wallets), hence, storing such secrets on the cloud corresponds to sharing ownership of the financial assets with the cloud, and makes the cloud a more attractive target for insider attacks. Can we have the best of the two worlds, where a user, Alice, can conveniently store a copy of her cryptographic secrets on the cloud and she is the only one who can recover them? Can she do so even when she loses her devices and forgets all credentials, while at the same time retaining full ownership of her secrets? In this paper, we provide a cloud-based secret-recovery mechanism using trusted execution environments (TEE) where confidentiality is always guaranteed when Alice has not lost her credentials, even in the presence of a malicious cloud fitted with a TEE. If Alice loses all her credentials, she can still recover her secrets (in most circumstances). This is in contrast with all previous work that relies on the assumption that Alice remembers some authentication secret. We prove our system secure in the Universally Composable framework. Further, we implement our protocols and evaluate their performance.",
    "status": "done"
  },
  {
    "id": 1636,
    "year": 2025,
    "title": "ML-Cube: Accelerating Module-Lattice-Based Cryptography using Machine Learning Accelerators with a Memory-Less Design",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765130",
    "abstract": "The rapid advancement of AI technologies has led to a dramatic surge in computational demands, driving significant breakthroughs in ML accelerators. The powerful performance of these accelerators has attracted the attention of cryptography researchers, and recent studies have begun to explore their use in accelerating cryptographic operations. However, treating these accelerators as black boxes leads to high latency, and strict concurrency requirements, which hinder their practical deployment. In this paper, we go beyond the black-box treatment of ML accelerators and introduce ML-Cube (ML3), a novel memory-less framework that leverages ML accelerators to implement module-lattice-based PQC, FIPS 203 ML-KEM, and FIPS 204 ML-DSA. The performance benefits of ML-Cube arise from our thorough analysis of ML accelerator internals. Rather than treating the accelerators as black boxes, we dissect their operating mechanisms and design tailored mathematical transformations for cryptographic acceleration. This enables memory-less (I)NTT and polynomial multiplication that minimizes external memory dependencies and reduces latency. We further address the high latency and excessive parallelism demands of traditional SIMT-based implementations by fully parallelizing both ML-KEM and ML-DSA schemes. Our experiments show that our Tensor Core-based (I)NTT achieves a 2.03x--3.56x speedup over a highly-optimized CUDA-core implementation. Moreover, our memory-less polynomial multiplication attains a 10x speedup, and the full ML-KEM reaches up to a 3.58x speedup with only less than one-tenth of the latency compared with SOTA approach (CHES '24). Additionally, our enhanced ML-DSA implementation offers a 30\\% to 55\\% throughput improvement over the previous SOTA methods (TDSC '24) under the server-oriented model. Importantly, by confining core computations within registers, our approach inherently mitigates memory disclosure and cache-based side-channel attacks, thereby enhancing overall security.",
    "status": "done"
  },
  {
    "id": 1637,
    "year": 2025,
    "title": "NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765136",
    "abstract": "The software supply chain is an increasingly common attack vector for malicious actors. The Node.js ecosystem has been subject to a wide array of attacks, likely due to its size and prevalence. To counter such attacks, the research community and practitioners have proposed a range of static and dynamic mechanisms, including process- and language-level sandboxing, permission systems, and taint tracking. Drawing on valuable insight from these works, this paper studies a runtime protection mechanism for (the supply chain of) Node.js applications with the ambitious goals of compatibility, automation, minimal overhead, and policy conciseness. Specifically, we design, implement and evaluate NodeShield, a protection mechanism for Node.js that enforces an application's dependency hierarchy and controls access to system resources at runtime. We leverage the up-and-coming SBOM standard as the source of truth for the dependency hierarchy of the application, thus preventing components from stealthily abusing undeclared components. We propose to enhance the SBOM with a notion of capabilities that represents a set of related system resources a component may access. Our proposed SBOM extension, the Capability Bill of Materials or CBOM, records the required capabilities of each component, providing valuable insight into the potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime via code outlining (as opposed to inlining) with no modifications to the original code or Node.js runtime, thus preventing unexpected, potentially malicious behavior. Our evaluation shows that NodeShield can prevent over 98\\% out of 67 known supply chain attacks while incurring minimal overhead on servers at less than 1ms per request. We achieve this while maintaining broad compatibility with vanilla Node.js and a concise policy language that consists of at most 7 entries per dependency.",
    "status": "done"
  },
  {
    "id": 1638,
    "year": 2025,
    "title": "Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765195",
    "abstract": "Password managers provide significant security benefits to users. However, malicious client-side scripts and browser extensions can steal passwords after the manager has autofilled them into the web page. In this paper, we extend prior work by Stock and Johns, showing how password autofill can be hardened to prevent these local attacks. We implement our design in the Firefox browser and conduct experiments demonstrating that our defense successfully protects passwords from XSS attacks and malicious extensions. We also show that our implementation is compatible with 97\\% of the Alexa top 1000 websites. Next, we generalize our design, creating a second defense that prevents recently discovered local attacks against the FIDO2 protocols. We implement this second defense into Firefox, demonstrating that it protects the FIDO2 protocol against XSS attacks and malicious extensions. This defense is compatible with all websites, though it does require a small change (2-3 lines) to web servers implementing FIDO2.",
    "status": "done"
  },
  {
    "id": 1639,
    "year": 2025,
    "title": "Be Aware of What You Let Pass: Demystifying URL-based Authentication Bypass Vulnerability in Java Web Applications",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765199",
    "abstract": "URL-based authentication provides a centralized and flexible way to safeguard sensitive resources in Java web applications by enforcing authentication checks based on URL paths. However, inconsistencies in handling flexible routing features (e.g., removing /../) between URL routing and authentication can be exploited to bypass authentication checks, resulting in URL-based Authentication Bypass Vulnerabilities (UABVulns). These vulnerabilities allow attackers to access sensitive resources without authentication, leading to serious security breaches. In this paper, we conduct the first in-depth study of 53 real-world UABVulns in Java web applications. Our study uncovers the root causes of UABVulns and identifies three key findings regarding URL routing, authentication, and sanitization. Guided by these findings, we design and implement UABScan, a static analysis tool that detects UABVulns by matching routing and authentication inconsistencies through pattern-based analysis. We evaluate UABScan on 529 popular Java web applications and successfully report 94 UABVulns across 72 applications, including 35 verified high-risk 0-days. Through manual investigation, UABScan achieves a recall of 87.50\\% and a precision of 80.00\\%, and significantly outperforms the state-of-the-art tool. To date, 31 CVE IDs have been assigned.",
    "status": "done"
  },
  {
    "id": 1640,
    "year": 2025,
    "title": "Same Script, Different Behavior: Characterizing Divergent JavaScript Execution Across Different Device Platforms",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765202",
    "abstract": "JavaScript drives dynamic content across modern web platforms, yet differences in browser engines, hardware, and APIs create distinct execution environments on mobile and desktop devices. This divergence raises important concerns about platform-specific tracking, but its scope and impact remain underexplored. In this paper, we present a hybrid analysis of JavaScript execution across mobile and desktop to uncover behavioral differences in how identical code operates. By combining static analysis of script structure with dynamic tracing of runtime behavior, we identify execution path divergences tied to the user's device. Our study shows that 20.6\\% of scripts on the top 10K Tranco-ranked websites exhibit platform-specific execution. Our tracing algorithm pinpoints the sources of divergence for 92.8\\% of conditional Web API calls, with 76\\% involving known fingerprinting APIs and 6\\% relying on lesser-known but platform-revealing interfaces. We further categorize divergent paths, finding asymmetric tracking patterns: desktop flows are dominated by fingerprinting and bot detection, while mobile flows focus more on behavioral profiling.",
    "status": "done"
  },
  {
    "id": 1641,
    "year": 2025,
    "title": "Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765210",
    "abstract": "Accurately assessing software vulnerabilities is essential for effective prioritization and remediation. While various scoring systems exist to support this task, their differing goals, methodologies and outputs often lead to inconsistent prioritization decisions. This work provides the first large-scale, outcome-linked empirical comparison of four publicly available vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS), the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit Prediction Scoring System (EPSS), and the Exploitability Index. We use a dataset of 600 real-world vulnerabilities derived from four months of Microsoft's Patch Tuesday disclosures to investigate the relationships between these scores, evaluate how they support vulnerability management task, how these scores categorize vulnerabilities across triage tiers, and assess their ability to capture the real-world exploitation risk. Our findings reveal significant disparities in how scoring systems rank the same vulnerabilities, with implications for organizations relying on these metrics to make data-driven, risk-based decisions. We provide insights into the alignment and divergence of these systems, highlighting the need for more transparent and consistent exploitability, risk, and severity assessments.",
    "status": "done"
  },
  {
    "id": 1642,
    "year": 2025,
    "title": "On Hyperparameters and Backdoor-Resistance in Horizontal Federated Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765211",
    "abstract": "Horizontal Federated Learning (HFL) is particularly vulnerable to backdoor attacks as adversaries can easily manipulate both the training data and processes to execute sophisticated attacks. In this work, we study the impact of training hyperparameters on the effectiveness of backdoor attacks and defenses in HFL. More specifically, we show both analytically and by means of measurements that the choice of hyperparameters by benign clients does not only influence model accuracy but also significantly impacts backdoor attack success. This stands in sharp contrast with the multitude of contributions in the area of HFL security, which often rely on custom ad-hoc hyperparameter choices for benign clients---leading to more pronounced backdoor attack strength and diminished impact of defenses. Our results indicate that properly tuning benign clients' hyperparameters---such as learning rate, batch size, and number of local epochs---can significantly curb the effectiveness of backdoor attacks, regardless of the malicious clients' settings. We support this claim with an extensive robustness evaluation of state-of-the-art attack-defense combinations, showing that carefully chosen hyperparameters yield across-the-board improvements in robustness without sacrificing main task accuracy. For example, we show that the 50\\%-lifespan of the strong A3FL attack can be reduced by 98.6\\%, respectively---all without using any defense and while incurring only a 2.9 percentage points drop in clean task accuracy.",
    "status": "done"
  },
  {
    "id": 1643,
    "year": 2025,
    "title": "Empowering Parents to Support Children's Online Security and Privacy: Findings from a Randomized Controlled Trial",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765214",
    "abstract": "In the ubiquitous computing society, parenting ''digital natives'' presents unprecedented challenges. Parents often rely on online resources to support and guide their children in security and privacy (S&amp;P) related topics. However, the abundance of online resources makes it challenging for parents to find high-quality and relevant resources that align with their S&amp;P needs. Further, the longitudinal development of parental competence and coping strategies in S&amp;P topics remains largely unexplored.We conducted a formative study with 210 U.S. parents of children (Mage = 11.73 years, SD = 3.15) to investigate the challenges parents face in educating children about online S&amp;P topics and to inform the design of a remote intervention program (six short videos). In the main study, we evaluated this intervention's efficacy using a 14-week longitudinal randomized controlled trial, which consisted of 201 U.S. parents, with 113 assigned to the control group and 88 to the intervention group.We found that short videos significantly enhanced parents' security awareness and their conversation strategies. Notably, parents who initially exhibited lower levels of these measurements benefited the most from the intervention. Moreover, short videos were effective in enhancing parents' self-efficacy in protecting their children from online risks. This study provides valuable insights into various challenges parents face and respective coping strategies that could be implemented to address S&amp;P concerns in family settings. The design and evaluation of the intervention program serve as a foundation for future S&amp;P researchers and educational stakeholders.",
    "status": "done"
  },
  {
    "id": 1644,
    "year": 2025,
    "title": "Security and Privacy Perceptions of Pakistani Facebook Matrimony Group Users",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765221",
    "abstract": "In Pakistan, where dating apps are subject to censorship, Facebook matrimony groups---also referred to as marriage groups---serve as alternative virtual spaces for members to search for potential life partners. To participate in these groups, members often share sensitive personal information such as photos, addresses, and phone numbers, which exposes them to risks such as fraud, blackmail, and identity theft. To better protect users of Facebook matrimony groups, we need to understand aspects related to user safety, such as how users perceive risks, what influences their trust in sharing personal information, and how they navigate security and privacy concerns when seeking potential partners online. In this study, through 23 semi-structured interviews, we explore how Pakistani users of Facebook matrimony groups perceive and navigate risks of sharing personal information, and how cultural norms and expectations influence their behavior in these groups. We find elevated privacy concerns among participants, leading them to share limited personal information and creating mistrust among potential partners. Many also expressed concerns about the authenticity of profiles and major security risks, such as identity theft, harassment, and social judgment. Our work highlights the challenges of safely navigating Facebook matrimony groups in Pakistan and offers recommendations for such as implementing stronger identity verification by group admins, enforcing stricter cybersecurity laws, clear platform guidelines to ensure accountability, and technical feature enhancements---including restricting screenshots, picture downloads, and implementing anonymous chats---to protect user data and build trust.",
    "status": "done"
  },
  {
    "id": 1645,
    "year": 2025,
    "title": "DiveFuzz: Enhancing CPU Fuzzing via Diverse Instruction Construction",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765167",
    "abstract": "Comprehensive exploration of the CPU architectural states in fuzzing is akin to generating diverse test cases, which include a reasonable distribution of opcode and diversity in instruction execution results (typically measured through write-back data). However, our analysis of state-of-the-art CPU fuzzers reveals that they exhibit high repetition in write-back data and an imbalanced distribution of opcodes during fuzzing. This paper presents DiveFuzz, which diversifies write-back data by finely controlling the operands of instructions at runtime, coupled with correlated contextual semantics, to generate instruction streams with diverse write-back data and semantic associations. Furthermore, DiveFuzz introduces a novel mutator that monitors the fuzzing process to dynamically adjust opcode distribution and accurately eliminate false positives. Our evaluations show that DiveFuzz significantly increases the diversity of instruction write-back data and achieves a more balanced opcode distribution compared to state-of-the-art fuzzers. Across five common coverage metrics, DiveFuzz achieves coverage 204\\texttimes{} faster than DifuzzRTL and 114\\texttimes{} faster than Cascade. We evaluated DiveFuzz on four well-known open-source RISC-V CPUs—XiangShan, CVA6, Rocket, and NutShell—uncovering 26 new bugs, 15 of which have CVE identifiers.",
    "status": "done"
  },
  {
    "id": 1646,
    "year": 2025,
    "title": "Threat from Windshield: Vehicle Windows as Involuntary Attack Sources on Automotive Voice Assistants",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765171",
    "abstract": "As automotive voice assistants (AVAs) become increasingly cen- tral to modern vehicles, their vulnerability to attacks exploiting inaudible sounds should raise security concerns. However, such concerns are often deemed low priority, because it is widely be- lieved that an attacker to AVAs should be strategically positioned inside the concerned vehicle for two main reasons: i) inaudible signals can barely penetrate vehicle hulls and ii) a line-of-sight (LoS) path is needed between the attacker (sound source) and the AVA's microphone. In this paper, we disprove this common belief by proposing ShieldSpear to launch AVA attacks outside vehicle hulls. ShieldSpear exploits a tiny piezo-element placed on the exterior of the windshield to convert it into both a speaker and microphone. While this setting naturally brings the attacking sound source into a vehicle, strategically placing this compactly integrated element may further yield i) covertness (blended into stickers), ii) LoS path to AVA's microphones, and iii) real-time attacking capability dur- ing vehicle motion. To maintain sufficient volume while evading detection, we design novel hardware and signal carriers for deliver- ing attack (voice) commands. Moreover, ShieldSpear leverages the windshield-converted microphone to acquire drivers' voiceprint so as to accurately emulate it in the faked commands. Extensive experiments involving five mainstream vehicles have demonstrated the effectiveness of ShieldSpear by a 90.9\\% end-to-end success rate in injecting faked voice commands into AVAs.",
    "status": "done"
  },
  {
    "id": 1647,
    "year": 2025,
    "title": "mUOV: Masking the Unbalanced Oil and Vinegar Digital Signature Scheme at First- and Higher-Order",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765188",
    "abstract": "In the recent search for additional post-quantum designs, multivariate quadratic equations (MQE) based designs have been receiving attention due to their small signature sizes. Unbalanced Oil and Vinegar (UOV) is an MQE-based digital signature (DS) scheme proposed over two decades ago. Although the mathematical security of UOV has been thoroughly analyzed, several practical side-channel attacks (SCA) have been shown on UOV based DS schemes. In this work, we perform a thorough analysis to identify the variables in UOV based DS schemes that can be exploited with passive SCA, specifically differential power attacks (DPA). Secondly, we introduce masking as a countermeasure to protect the sensitive components of UOV based schemes. We propose efficient masked gadgets for all the critical operations, including the masked dot-product and matrix-vector multiplication. We show that our gadgets are secure in the t-probing model through formal proofs, mechanically verified using the maskVerif tool. We implemented and demonstrated the practical feasibility of our arbitrary-order masking algorithms for UOV-Ip and UOV-III. We show that the masked signature generation of UOV-Ip performs up to 62\\% better than ML-DSA-44 and 99\\% better than Falcon-512. In addition, the security of our implementation is practically validated using the test vector leakage assessment (TVLA) methodology.",
    "status": "done"
  },
  {
    "id": 1648,
    "year": 2025,
    "title": "Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765193",
    "abstract": "Sensor fusion in multi-agent systems, including smart cities, often lacks security awareness and is vulnerable to attacks. We propose a security-aware sensor fusion framework that estimates and incorporates probabilistic trust with uncertainty to defend against compromised insider agents. Trust is modeled as a hidden Markov process and updated via Bayesian inference using novel trust pseudomeasurements (PSMs), which map discrepancies between expected and observed sensor data into trust evidence. Trust estimates reweight agent contributions during fusion and identify corrupted information, mitigating the influence of compromised nodes. Our system includes a dynamic field-of-view estimator using LiDAR ray tracing, novel logic for PSM generation, and efficient Bayesian updates with conjugate priors. Evaluated in adversarial scenarios, our method significantly reduces fusion error and accurately detects compromised agents. These results show trust-guided fusion enables resilient situational awareness under attack, making it suitable for adversarial cyber-physical environments.",
    "status": "done"
  },
  {
    "id": 1649,
    "year": 2025,
    "title": "Adversarially Robust Assembly Language Model for Packed Executables Detection",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765157",
    "abstract": "Detecting packed executables is a critical component of large-scale malware analysis and antivirus engine workflows, as it identifies samples that warrant computationally intensive dynamic unpacking to reveal concealed malicious behavior. Traditionally, packer detection techniques have relied on empirical features, such as high entropy or specific binary patterns. However, these empirical, feature-based methods are increasingly vulnerable to evasion by adversarial samples or unknown packers (e.g., low-entropy packers). Furthermore, the dependence on expert-crafted features poses challenges in sustaining and evolving these methods over time.In this paper, we examine the limitations of existing packer detection methods and propose Pack-ALM, a novel deep-learning-based approach for detecting packed executables. Inspired by the linguistic concept of distinguishing between real and pseudo words, we reformulate packer detection as a task of differentiating between legitimate and pseudo instructions. To achieve this, we preprocess native data and packed data into pseudo instructions and design a pre-trained assembly language model that recognizes features indicative of packed data. We evaluate Pack-ALM against leading industrial packer detection tools and state-of-the-art assembly language models. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM effectively identifies packed binaries, including samples created with adversarial or previously unseen packing techniques. Moreover, Pack-ALM outperforms traditional entropy-based methods and advanced assembly language models in both detection accuracy and adversarial robustness.",
    "status": "done"
  },
  {
    "id": 1650,
    "year": 2025,
    "title": "BLACKOUT: Data-Oblivious Computation with Blinded Capabilities",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765169",
    "abstract": "Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5\\% geometric mean) compared to the baseline CHERI-Toooba processor.",
    "status": "done"
  },
  {
    "id": 1651,
    "year": 2025,
    "title": "Needle in a Haystack: Automated and Scalable Vulnerability Hunting in the Windows ALPC Sea",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765180",
    "abstract": "Windows services utilizing Remote Procedure Call (RPC) and Component Object Model (COM) technology over the underlying Advanced Local Procedure Call (ALPC) transport present a significant attack surface. However, previous research often focused on known vulnerability patterns or required time-consuming reverse engineering, which hinders scalable vulnerability discovery. We developed a tool designed to automate and scale the fuzzing of ALPC communications. It employs a record-and-replay based strategy, capturing live system-wide ALPC traffic and replaying mutated payloads directly at the ALPC layer, thereby overcoming the scalability barrier posed by the manual preparation required with conventional methods. Furthermore, it integrates dedicated detection techniques to identify information leakage vulnerabilities that crash-centric fuzzers often miss. After evaluating various versions of Windows operating systems, we discovered 12 vulnerabilities confirmed by Microsoft, 10 of which have already been assigned CVE numbers.",
    "status": "done"
  },
  {
    "id": 1652,
    "year": 2025,
    "title": "Styled to Steal: The Overlooked Attack Surface in Email Clients",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765189",
    "abstract": "Email is still a widely used communication medium, particularly in professional contexts. Standards such as OpenPGP and S/MIME offer encryption while maintaining compatibility with existing infrastructure. Within the end-to-end encryption threat model, email servers are untrusted, which creates opportunities for attackers to inject malicious HTML or CSS into encrypted emails---either live during email transport, or by re-sending leaked emails. In this paper, we show that isolation mechanisms in widely used email client software remain inadequate. We present a novel scriptless attack that extracts arbitrary plaintext from encrypted emails using only CSS without requiring JavaScript. Once the email is opened, three benign-looking CSS features — container queries, lazy-loaded web fonts, and contextual font ligatures — map each character of the ciphertext-carried plaintext to a unique network request to the attacker's server. This attack technique can incrementally reconstruct the entire plaintext in a single rendering pass, with no JavaScript, no visual artifacts, and, depending on the configuration, even without any user interaction. The technique differs considerably from prior work: it achieves complete plaintext recovery without script execution, evades state-of-the-art sanitizers such as DOMPurify, and succeeds across multiple browser engines. We demonstrate the severity of this threat on Mozilla Thunderbird and KMail, with end-to-end attacks successfully exfiltrating PGP-encrypted text from an email rendered in the latest version of the respective clients. Furthermore, we show that our technique affects code integrity tools and sanitization techniques reused in software stacks, including Meta's Code Verify. Our findings led to practical mitigations in Thunderbird, as well as a revision of Meta's threat model to include CSS. These results underline the need for robust content isolation in email client software and challenge the assumption that existing mitigations fully prevent encrypted content leakage.",
    "status": "done"
  },
  {
    "id": 1653,
    "year": 2025,
    "title": "Denial of Sequencing Attacks in Ethereum Layer 2 Rollups",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765100",
    "abstract": "Layer 2 rollups offer promising solutions to address Ethereum's scalability issues. However, the centralized nature of the sequencer in these rollups makes them vulnerable to denial of service attacks, in which adversaries overwhelm the sequencer with invalid transactions that cannot be included in blocks, thereby exhausting its computational resources for transaction processing. To mitigate such threat, layer 2 rollups implement the legality check mechanism to filter out invalid transactions before they reach the sequencer.In this work, we unveil a novel denial of sequencing attack that disrupts the liveness of layer 2 rollups at zero cost by bypassing the legality check. Specifically, our attack enables an adversary to craft malicious invalid transactions that bypass the legality check but are ultimately discarded by the sequencer after execution. As a result, the adversary can exhaust the sequencer's computational resources without incurring any fees. To construct such malicious transactions, we propose two approaches: a side-channel based approach and an incomplete check based approach, both of which rely on underlying vulnerabilities in rollups. Additionally, we investigate two widely used rollups, i.e., Arbitrum and Polygon zkEVM, and uncover four unknown vulnerabilities within them, which can be exploited to launch our attack using the two proposed approaches. Through extensive experiments conducted in a local environment, we demonstrate that all our attack variants, each exploiting distinct vulnerabilities, lead to severe attack effects at zero cost. Moreover, we discuss three feasible mitigations against our attack. At the time of writing, both the vulnerabilities and our attack have been acknowledged by the respective official teams, who have awarded us bug bounties to highlight the severity of our findings.",
    "status": "done"
  },
  {
    "id": 1654,
    "year": 2025,
    "title": "Realizing Corrupted-Shard Tolerance: A Sharding Blockchain with Preserving Global Resilience",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765132",
    "abstract": "Blockchain sharding is a promising approach to enhancing scalability by partitioning the network into smaller, parallel shards. However, existing sharding blockchains that rely on Byzantine fault tolerance protocols require large shard sizes to meet strict security thresholds, limiting scalability, while relaxing security parameters can lead to liveness and safety violations. In this work, we present Camael, a secure sharding blockchain that achieves corrupted-shard tolerance through effective detection and processing mechanisms for both liveness and safety violations. Specifically, fake liveness violations forged by malicious nodes are accurately detected via a two-phase reporting and confirmation mechanism, while concealed safety violations are efficiently identified using a lightweight snapshot mechanism. Furthermore, a state determination process ensures overall system consistency. Malicious nodes are precisely identified through a conviction mechanism, which enables the replacement of the targeted nodes and the reconfiguration of the shards. Notably, Camael ensures security while preserving a global fault tolerance of 1/3 and tolerating corrupted shards, with each shard accommodating up to 2/3 malicious nodes. Extensive experiments conducted on 2000 AWS EC2 nodes across 4 regions demonstrate that Camael improves throughput by 3.56 times compared to the baseline (Kronos, NDSS'25), achieving a throughput of 109.3 ktx/sec, while the violation processing requires only 1.64 sec.",
    "status": "done"
  },
  {
    "id": 1655,
    "year": 2025,
    "title": "Lite-PoT: Practical Powers-of-Tau Setup Ceremony",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765182",
    "abstract": "Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) schemes have gained significant adoption in privacy-preserving applications, in decentralized systems (e.g., blockchain), and in verifiable computation due to their efficiency. However, the most efficient zk-SNARKs often rely on a one-time trusted setup to generate public parameters, often known as the ''Powers of Tau'' (PoT) string. The leakage of the secret parameter τ in the string would allow attackers to generate false proofs, compromising the soundness of all zk-SNARK systems built on it. Prior proposals for decentralized setup ceremonies have utilized blockchain-based smart contracts to allow any party to contribute randomness to τ while also preventing censorship of contributions. For a d-degree PoT string generated using randomness from m contributors, these solutions require a total of O(md) on-chain operations (i.e., in terms of both storage and cryptographic operations). These operations primarily consist of costly group operations, particularly scalar multiplication on pairing curves, which discourage participation and limit the impact of decentralization.In this work, we present Lite-PoT, which includes two key protocols designed to reduce participation costs: (i) a fraud-proof protocol to reduce the number of expensive on-chain cryptographic group operations to O(1) per contributor. Our experimental results show that (with one transaction per update) our protocol enables decentralized ceremonies for PoT strings up to a 215-degree (limited by Ethereum's 30M block gas limit), a ≈16\\texttimes{} improvement over existing on-chain solutions; (ii) a proof aggregation technique that batches m randomness contributions into one on-chain update with only O(d) on-chain operations, independent of m. This significantly reduces the monetary cost of on-chain updates by m-fold via amortization.",
    "status": "done"
  },
  {
    "id": 1656,
    "year": 2025,
    "title": "A Secure Sequencer and Data Availability Committee for Rollups",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765187",
    "abstract": "Blockchains face a scalability limitation, partly due to the throughput limitations of consensus protocols, especially when aiming to obtain a high degree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to conventional blockchains. L2s perform most computations offchain using minimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer is a service that receives offchain L2 transaction requests, batches these transactions, and commits compressed or hashed batches to L1. Using hashing needs less L1 space—which is beneficial for gas cost—but requires a data availability committee (DAC) service to translate hashes into their corresponding batches of transaction requests. The behavior of sequencers and DACs influence the evolution of the L2 blockchain, presenting a potential security threat and delaying L2 adoption. We propose in this paper fraud-proof mechanisms, arbitrated by L1 contracts, to detect and generate evidence of dishonest behavior of the sequencer and DAC. We study how these fraud-proofs limit the power of adversaries that control different number of sequencer and DACs members, and provide incentives for their honest behavior. We designed these fraud-proof mechanisms as two player games. Unlike the generic fraud-proofs in current L2s (designed to guarantee the correct execution of transactions), our fraud-proofs are over pre-determined algorithms that verify the properties that determine the correctness of the DAC. Arbitrating over concrete algorithms makes our fraud-proofs more efficient, easier to understand, and simpler to prove correct. We provide as an artifact a mechanization in LEAN4 of our fraud-proof games, including (1) the verified strategies that honest players should play to win all games as well as (2) mechanisms to detect dishonest claims.",
    "status": "done"
  },
  {
    "id": 1657,
    "year": 2025,
    "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765173",
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, their widespread adoption has raised serious privacy concerns. While prior research has primarily focused on edge-level privacy, a critical yet underexplored threat lies in topology privacy — the confidentiality of the graph's overall structure. In this work, we present a comprehensive study on topology privacy risks in GNNs, revealing their vulnerability to graph-level inference attacks. To this end, we propose a suite of Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph using only black-box access to a GNN model. Our findings show that GNNs are highly susceptible to these attacks, and that existing edge-level differential privacy mechanisms are insufficient as they either fail to mitigate the risk or severely compromise model accuracy. To address this challenge, we introduce Private Graph Reconstruction (PGR), a novel defense framework designed to protect topology privacy while maintaining model accuracy. PGR is formulated as a bi-level optimization problem, where a synthetic training graph is iteratively generated using meta-gradients, and the GNN model is concurrently updated based on the evolving graph. Extensive experiments demonstrate that PGR significantly reduces topology leakage with minimal impact on model accuracy. Our code and full paper are available at https://github.com/JeffffffFu/PGR.",
    "status": "done"
  },
  {
    "id": 1658,
    "year": 2025,
    "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765174",
    "abstract": "The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multi-modal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture-of-Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures—where input tokens are dynamically directed to specialized experts based on their semantic meaning—inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho (MoE-Echo), discovering a side-channel analysis-based attack surface that compromises user privacy on MoE-based systems. Specifically, in MoEcho, we introduce four novel architectural side-channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large-language models (LLMs) and vision-language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. We evaluate MoEcho on four open-source MoE-based models at different scales, with a specific focus on the DeepSeek architecture. Our end-to-end experiments on both CPU- and GPU-deployed MoE models demonstrate a 99.8\\% success rate in inferring the patient's private inputs in healthcare records and 92.8\\% in reconstructing LLM responses. MoEcho is the first run-time architecture-level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE-based models for developing efficient large-scale AI services.",
    "status": "done"
  },
  {
    "id": 1659,
    "year": 2025,
    "title": "Removal Attack and Defense on AI-generated Content Latent-based Watermarking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765175",
    "abstract": "Digital watermarks can be embedded into AI-generated content (AIGC) by initializing the generation process with starting points sampled from a secret distribution. When combined with pseudorandom error-correcting codes, such watermarked outputs can remain indistinguishable from unwatermarked objects, while maintaining robustness under whitenoise. In this paper, we go beyond indistinguishability and investigate security under removal attacks. We demonstrate that indistinguishability alone does not necessarily guarantee resistance to adversarial removal. Specifically, we propose a novel attack that exploits boundary information leaked by the locations of watermarked objects. This attack significantly reduces the distortion required to remove watermarks—by up to a factor of 15 \\texttimes{} compared to a baseline whitenoise attack under certain settings. To mitigate such attacks, we introduce a defense mechanism that applies a secret transformation to hide the boundary, and prove that the secret transformation effectively rendering any attacker's perturbations equivalent to those of a na\\\"{\\i}ve whitenoise adversary. Our empirical evaluations, conducted on multiple versions of Stable Diffusion, validate the effectiveness of both the attack and the proposed defense, highlighting the importance of addressing boundary leakage in latent-based watermarking schemes.",
    "status": "done"
  },
  {
    "id": 1660,
    "year": 2025,
    "title": "VillainNet: Targeted Poisoning Attacks Against SuperNets Along the Accuracy-Latency Pareto Frontier",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765185",
    "abstract": "State-of-the-art (SOTA) weight-shared SuperNets dynamically activate subnetworks at runtime, enabling robust adaptive inference under varying deployment conditions. However, we find that adversaries can take advantage of the unique training and inference paradigms of SuperNets to selectively implant backdoors that activate only within specific subnetworks, remaining dormant across billions of other subnetworks. We present VillainNet (VNET), a novel poisoning methodology that restricts backdoor activation to attacker-chosen subnetworks, tailored either to specific operational scenarios (e.g., specific vehicle speeds or weather conditions) or to specific subnetwork configurations. VNET's core innovation is a novel, distance-aware optimization process that leverages architectural and computational similarity metrics between subnetworks to ensure that backdoor activation does not occur across non-target subnetworks. This forces defenders to confront a dramatically expanded search space for backdoor detection. We show that across two SOTA SuperNets, trained on the CIFAR10 and GTSRB datasets, VNET can achieve attack success rates comparable to traditional poisoning approaches (approximately 99\\%), while significantly lowering the chances of attack detection, thereby stealthily hiding the attack. Consequently, defenders face increased computational burdens, requiring on average 66 (and up to 250 for highly targeted attacks) sampled subnetworks to detect the attack, implying a roughly 66-fold increase in compute cost required to test the SuperNet for backdoors.",
    "status": "done"
  },
  {
    "id": 1661,
    "year": 2025,
    "title": "Efficient Fuzzy PSI Based on Prefix Representation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765203",
    "abstract": "Fuzzy PSI is a variant of PSI, which on input a set of points from the receiver and sender respectively, allows the receiver to learn which of the sender's points lie within a threshold distance δ under a specific distance metric.Baarsen and Pu (EUROCRYPT'24) first proposed efficient fuzzy PSI protocols for general Lp distances (where p∈[1, ∞]) in d-dimensional space, achieving communication complexity linear in the input size, δ, and 2d d. However, they leave open the question of whether the prefix technique of Chakraborti et al. (USENIX Security'23) can further reduce the communication complexity of their fuzzy PSI protocols in both low and high dimensions.In this work, we thoroughly explore using the prefix technique to reduce the complexity of fuzzy PSI. First, we propose fuzzy matching protocols for L∞ and Lp distances, where the communication complexity is improved from O(δ d) to O(log δ, d) for L∞, and from O((δp) to O((log δ)d p) for Lp distance. By applying our fuzzy matching protocol in conjunction with spatial hashing, we propose fuzzy PSI protocols for low-dimensional space. For high-dimensional space, we present the first fuzzy PSI protocols achieving communication and computation complexity that scales logarithmically in δ and linearly in dimension d and input set sizes.We implement our fuzzy PSI protocols and compare them with state-of-the-art protocols. Experimental results demonstrate that our protocols achieve superior performance for large δ: for input size N=28, d = 5, and δ = 256, our protocol requires 10--36X less running time and 3--4.5X lower communication than existing protocols.",
    "status": "done"
  },
  {
    "id": 1662,
    "year": 2025,
    "title": "Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765216",
    "abstract": "This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, for an aggregation on n clients they either require high cost per client (Chowdhury et al. CCS '22) or concretely many rounds that is logarithmic in n (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo runs in 3 rounds per aggregation (our round complexity is independent of n) with computationally lightweight server and clients.",
    "status": "done"
  },
  {
    "id": 1663,
    "year": 2025,
    "title": "The OCH Authenticated Encryption Scheme",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765224",
    "abstract": "We specify OCH, the first authenticated encryption with associated data scheme built to provide 128-bit multi-user AE security, 128-bit context commitment security, and 256-bit nonces with optional nonce privacy. It therefore addresses pressing limitations of currently widely-deployed schemes. We construct and formally analyze the security of OCH in a modular fashion, with transforms that are of broader applicability. On Intel Raptor Lake CPUs, OCH using the Areion permutation family has a peak encryption speed of 0.62 cycles per byte (cpb), not far off from AES128-GCM (0.38cpb) and outperforming both ChaCha20/Poly1305 (1.63cpb) and TurboSHAKE128-Wrap (3.52cpb).",
    "status": "done"
  },
  {
    "id": 1664,
    "year": 2025,
    "title": "From OT to OLE with Subquadratic Communication",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765225",
    "abstract": "Oblivious Linear Evaluation (OLE) is an algebraic generalization of oblivious transfer (OT) that forms a critical part of a growing number of applications. An OLE protocol over a modulus q enables the receiver party to securely evaluate a line a⋅ X+b chosen by the sender party on a secret point x∈ ℤq. Motivated by the big efficiency gap between OLE and OT and by fast OT extension techniques, we revisit the question of reducing OLE to OT, aiming to improve the communication cost of known reductions.We start by observing that the Chinese Remainder Theorem (CRT) can be combined with a prior protocol of Gilboa (Crypto '99) to reduce its communication cost from O(ℓ2) to \\~{O}(ℓ) bits, for ℓ=log q. Unfortunately, whereas Gilboa's protocol is secure against a semi-honest sender and a malicious receiver, a direct application of the CRT technique is only semi-honest secure (it is insecure against malicious receivers). Thus, we employ number-theoretic techniques to protect our CRT-based protocol against malicious receivers, while still retaining a concrete advantage over Gilboa's protocol (e.g., 10.2X less communication for ℓ=256). Furthermore, we obtain a fully malicious OLE-to-OT reduction by applying either information-theoretic techniques with moderate overhead, or RSA-based cryptographic techniques with very low overhead.We demonstrate the usefulness of our results in the context of OLE applications, including a post-quantum oblivious pseudorandom function (OPRF) and distributed signatures. In particular, assuming pre-existing random OT correlations, we can use our malicious-receiver OLE protocol to realize (a single instance of) the power-residue based OPRF candidate with security against a malicious client and a semi-honest server using only 1.14 KB of communication, a 16X improvement over the best previous protocol in this setting. Using our RSA-based fully malicious OLE protocol, we achieve a 5X communication improvement over previous OT and EC-based distributed ECDSA protocols. Compared to other ECDSA protocols (including ones that use Paillier and class groups), the communication gains are more modest, but come at only a fraction of the computational cost as we avoid all expensive group operations.",
    "status": "done"
  },
  {
    "id": 1665,
    "year": 2025,
    "title": "Interoperable Symmetric Message Franking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744864",
    "abstract": "The recent Digital Markets Act (DMA), a regulation passed by the European Union in 2022, requires messaging applications with large user bases to support interoperable end-to-end encrypted (E2EE) communication. This raises numerous questions about how to adapt cryptographic protocols to this setting in a way that preserves security and privacy. This question is not only limited to the main messaging protocols, but also extends to protocols for abuse mitigation such as the symmetric message franking protocol first proposed by Facebook. The latter uses symmetric cryptography to enable reporting abusive E2EE messages in a way that allows the platform to cryptographically verify the report's veracity. In this paper, we initiate a formal treatment of interoperable symmetric message franking (IMF). We focus on a server-to-server messaging flow, where messages are routed sequentially through the sender's and recipient's service providers, but allow the recipient to dynamically choose who to send a report to. We formalize the security definitions for IMF including adapting the sender and recipient binding definitions into various reportability and unforgeability definitions that take into account one of the service providers misbehaving. We also prove relations among these new definitions. Finally, we detail an IMF construction that satisfies the security definitions, and include a discussion of users' identity privacy goals and deployment considerations.",
    "status": "done"
  },
  {
    "id": 1666,
    "year": 2025,
    "title": "Gibbon: Faster Secure Two-party Training of Gradient Boosting Decision Tree",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744866",
    "abstract": "Gradient Boosting Decision Tree (GBDT) and its variants are widely used in industry. They have achieved remarkable success in numerous machine learning competitions and practical applications. Secure Multi-Party Computation (MPC) allows multiple data owners to compute a function jointly while keeping their input private. In this work, we present Gibbon, a secure two-party GBDT training framework on a vertically split dataset, where two data owners each hold different features of the same data samples. Compared with the state-of-the-art Squirrel (USENIX'Sec 2023), for most parameter settings, Gibbon achieves 2\\texttimes{}-4\\texttimes{} reduction in running time and 2\\texttimes{}-3\\texttimes{} reduction in communication. Gibbon achieves its impressive performance through a series of innovative co-designs of the GBDT algorithms and advanced cryptography. Specifically, 1) we optimize the GBDT algorithm to eliminate the majority of MPC-unfriendly inversion operations. 2) We propose a novel protocol to evaluate the MPC-unfriendly sigmoid function, demonstrating 13\\texttimes{} communication reduction compared to Squirrel's sigmoid protocol. 3) Using RLWE-based and MLWE-based homomorphic encryption, we propose a highly efficient binary matrix multiplication protocol tailored for GBDT training. Our empirical results show that our protocol is about two orders of magnitude faster than Squirrel's.",
    "status": "done"
  },
  {
    "id": 1667,
    "year": 2025,
    "title": "Breaking and Fixing Content-Defined Chunking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744870",
    "abstract": "Content-defined chunking (CDC) algorithms split streams of data into smaller blocks, called chunks, in a way that preserves chunk boundaries when the data is partially changed. CDC is ubiquitous in applications that deduplicate data such as backup solutions, software patching systems, and file hosting platforms. Much like compression, CDC can introduce leakage when combined with encryption: fingerprinting attacks can exploit chunk length patterns to infer information about the data.To address these risks, many systems---mainly in the cloud backup setting---have developed bespoke mitigations by mixing a cryptographic key into the chunking process. We study these keyed CDC (KCDC) schemes ''in the wild'', presenting efficient key recovery attacks against five different KCDC schemes, deployed in the backup solutions Borg, Bupstash, Duplicacy, Restic, and Tarsnap. Our attacks are in a realistic threat model that relies only on weak known- or chosen-plaintext capabilities. This shows, in particular, that they fail to protect against fingerprinting attacks. To demonstrate practical exploitability, we also present ''end-to-end'' attacks on three complete encrypted backup applications, namely Borg, Restic and Tarsnap. These build on our attacks on the underlying KCDC schemes.In an effort to tackle these problems, we introduce the first formal treatment for KCDC schemes and propose a provably secure construction that fulfills a strong notion of security. We benchmark our construction against existing (broken) approaches, showing that it has competitive performance. In doing so, we take a step towards making real-world systems that rely on KCDC more resilient to attacks.",
    "status": "done"
  },
  {
    "id": 1668,
    "year": 2025,
    "title": "Refined TFHE Leveled Homomorphic Evaluation and Its Application",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744873",
    "abstract": "TFHE is a fully homomorphic encryption scheme over the torus that supports fast bootstrapping. Its primary evaluation mechanism is based on gate bootstrapping and programmable bootstrapping (PBS), which computes functions while simultaneously refreshing noise. PBS-based evaluation is user-friendly and efficient for small circuits; however, the number of bootstrapping operations increases exponentially with the circuit depth. To address the challenge of efficiently evaluating large-scale circuits, Chillotti et al. introduced a leveled homomorphic evaluation (LHE) mode at Asiacrypt 2017. This mode decouples circuit evaluation from bootstrapping, resulting in a speedup of hundreds of times over PBS-based methods. However, the remaining circuit bootstrapping (CBS) becomes a performance bottleneck, even though its frequency is linear with the circuit depth.In this paper, we refine the LHE mode by mitigating the high cost of CBS. First, we patch the NTT-based CBS algorithm proposed by Wang et al. [WWL+, Eurocrypt 2024], accelerating their algorithm by up to 2.6\\texttimes{}. Then, observing the suboptimal parallelism and high complexity of modular reduction in NTT under CBS parameters, we extend WWL+ to an FFT-based algorithm by redesigning the pre-processing method and introducing a split FFT technique. This achieves the fastest CBS implementation with the smallest key size, outperforming the open-source WWL+ implementation by up to 12.1\\texttimes{} (resp. 5.12\\texttimes{} compared to our patched algorithm), and surpassing TFHEpp [MBM+, USENIX 2021] by 3.42\\texttimes{} with a key size reduction of 33.2\\texttimes{}. Furthermore, we proposed an improved integer input LHE mode by extending our CBS algorithm to support higher precision and combining it with additional optimizations such as multi-bit extraction. Compared to the previous integer input LHE mode proposed by Bergerat et al. [BBB+, JoC 2023], our approach is up to 10.7\\texttimes{} faster with a key size reduction of up to 4.4\\texttimes{}.To demonstrate the practicality of our improved LHE mode, we apply it to AES transciphering and general homomorphic look-up table (LUT) evaluation. For AES evaluation, our method is 4.8\\texttimes{} faster and reduces the key size by 31.3\\texttimes{} compared to the state-of-the-art method, Thunderbird [WLW+, TCHES 2024]. For LUT evaluation, we compare our results with the recent work of Trama et al. [TCBS, ePrint 2024/1201], which constructs a general 8-bit processor of TFHE. Our method not only achieves faster 8-to-8 LUT evaluation but also improves the efficiency of most heavy 8-bit bivariate instructions by up to 21\\texttimes{} and the 16-bit sigmoid function by more than 26\\texttimes{}.",
    "status": "done"
  },
  {
    "id": 1669,
    "year": 2025,
    "title": "CCA-Secure Traceable Threshold (ID-based) Encryption and Application",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744876",
    "abstract": "A recent work by Boneh, Partap, and Rotem [Crypto'24] introduced the concept of traceable threshold encryption, in that if t or more parties collude to construct a decryption box, which performs decryptions, then at least one party's identity can be traced by making a few black-box queries to the box. This has important applications, e.g., in blockchain mempool privacy, where collusion yields high financial gain through MEVs without any consequence -- the possibility of tracing discourages collusion. Nevertheless, their definitions leave room for exploitation as they only achieve CPA security and do not consider inconsistency in decryption via different participating sets. This paper proposes stronger definitions of traceable threshold encryption, which supports CCA-security and consistency. Our main approach considers identity-based variants of traceable encryption (which we also define). It converts that to a CCA-secure construction, adapting two generic transformations, first using a one-time signature and then a fingerprinting code. We put forward two efficient instantiations of our identity-based scheme with different merits: our first construction is based on Boneh-Franklin IBE [Crypto'01] and has constant size ciphertexts but quadratic size public keys -- this is proven secure based on XDH and BDDH. Our second construction is based on Boneh-Boyen IBE [Eurocrypt'04]. It supports both constant-size ciphertexts and constant-size public keys -- this is proven secure based on a variant of the uber assumption over bilinear pairings. Our concrete analysis shows that the first construction's ciphertext is much (~6x) smaller than the second construction. Finally, we extend the definitions to support consistency and achieve it by adjoining an efficient, non-interactive proof of correct encryption.",
    "status": "done"
  },
  {
    "id": 1670,
    "year": 2025,
    "title": "High-Throughput Universally Composable Threshold FHE Decryption",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744884",
    "abstract": "Threshold Fully Homomorphic Encryption (FHE) enables arbitrary computation on encrypted data, while distributing the decryption capability across multiple parties. A primary application of interest is low-communication multi-party computation (MPC), which benefits from a fast and secure threshold FHE decryption protocol. Several works have addressed this problem, but all existing solutions rely on ''noise flooding'' for security. This incurs significant overhead and necessitates large parameters in practice, making it unsuitable for many real-world deployments. Some constructions have somewhat better efficiency, but at the cost of weaker, non-simulation-based security definitions, which limits their usability and composability. In this work, we propose a novel threshold FHE decryption protocol that avoids ''noise flooding'' altogether, and provides strong simulation-based security. Rather than masking the underlying ciphertext noise, our technique securely removes it via an efficient MPC rounding procedure. The cost of this MPC is mitigated by an offline/online design that preprocesses special gates for secure comparisons in the offline phase, and has low communication and computation in the online phase. This approach is of independent interest, and should also benefit other MPC protocols (e.g., secure machine learning) that make heavy use of non-linear comparison operations. We prove our protocol secure in the Universal Composability (UC) framework, and it can be generally instantiated for a variety of adversary models (e.g., security-with-abort against a dishonest majority, or guaranteed output delivery with honest majority). Compared to the state of the art, our protocol offers significant gains both in the adversary model (i.e., dishonest vs. honest majority) and practical performance: empirically, our online phase obtains approximately 20,000\\texttimes{} better throughput, and up to a 37\\texttimes{} improvement in latency.",
    "status": "done"
  },
  {
    "id": 1671,
    "year": 2025,
    "title": "Harnessing Sparsification in Federated Learning: A Secure, Efficient, and Differentially Private Realization",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765044",
    "abstract": "Federated learning (FL) enables multiple clients to jointly train a model by sharing only gradient updates for aggregation instead of raw data. Due to the transmission of very high-dimensional gradient updates from many clients, FL is known to suffer from a communication bottleneck. Meanwhile, the gradients shared by clients as well as the trained model may also be exploited for inferring private local datasets, making privacy still a critical concern in FL. We present Clover, a novel system framework for communication-efficient, secure, and differentially private FL. To tackle the communication bottleneck in FL, Clover follows a standard and commonly used approach---top-k gradient sparsification, where each client sparsifies its gradient update such that only k largest gradients (measured by magnitude) are preserved for aggregation. Clover provides a tailored mechanism built out of a trending distributed trust setting involving three servers, which allows to efficiently aggregate multiple sparse vectors (top-k sparsified gradient updates) into a dense vector while hiding the values and indices of non-zero elements in each sparse vector. This mechanism outperforms a baseline built on the general distributed ORAM technique by several orders of magnitude in server-side communication and runtime, with also smaller client communication cost. We further integrate this mechanism with a lightweight distributed noise generation mechanism to offer differential privacy (DP) guarantees on the trained model. To harden Clover with security against a malicious server, we devise a series of lightweight mechanisms for integrity checks on the server-side computation. Extensive experiments show that Clover can achieve utility comparable to vanilla FL with central DP and no use of top-k sparsification. Meanwhile, achieving malicious security introduces negligible overhead in client-server communication, and only modest overhead in server-side communication and runtime, compared to the semi-honest security counterpart.",
    "status": "done"
  },
  {
    "id": 1672,
    "year": 2025,
    "title": "Counting Subgraphs under Shuffle Differential Privacy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765047",
    "abstract": "To understand the complex structures and relationships in graph data while safeguarding personal privacy, subgraph counting under differential privacy (DP) has received a lot of attention recently. The problem is particularly important in a distributed setting, where each node holds only its local neighboring information and the analyst is untrusted. In the literature, two DP models are tailored for this scenario, known as local DP and shuffle DP, whereas the latter is equipped with a trusted shuffler that random shuffles the messages before handing them to the analyst. Since the shuffler introduces no additional privacy risk, any local DP protocol automatically satisfies shuffle DP, and the key question is whether shuffle DP can offer any improvement, especially for utility. While positive results have been obtained for a number of basic problems, such as basic counting, frequency estimation, and distinct count, it still remains elusive if this is the case for any graph problem. In this paper, we advance the understanding of this question by presenting new shuffle DP protocols for counting various subgraphs, including triangles, 4-cycles, and 3-hop paths, which improve upon the existing local DP and shuffle DP protocols, both asymptotically and concretely.",
    "status": "done"
  },
  {
    "id": 1673,
    "year": 2025,
    "title": "Managing Correlations in Data and Privacy Demand",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765048",
    "abstract": "Previous works in the differential privacy literature that allow users to choose their privacy levels typically operate under the heterogeneous differential privacy (HDP) framework with the simplifying assumption that user data and privacy levels are not correlated. Firstly, we demonstrate that the standard HDP framework falls short when user data and privacy demands are allowed to be correlated. Secondly, to address this shortcoming, we propose an alternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that jointly accounts for user data and privacy preference. We show that AHDP is robust to possible correlations between data and privacy. Thirdly, we formalize the guarantees of the proposed AHDP framework through an operational hypothesis testing perspective. The hypothesis testing setup may be of independent interest in analyzing other privacy frameworks as well. Fourthly, we show that there exists non-trivial AHDP mechanisms that notably do not require prior knowledge of the data-privacy correlations. We propose some such mechanisms and apply them to core statistical tasks such as mean estimation, frequency estimation, and linear regression. The proposed mechanisms are simple to implement with minimal assumptions and modeling requirements, making them attractive for real-world use. Finally, we empirically evaluate proposed AHDP mechanisms, highlighting their trade-offs using LLM-generated synthetic datasets, which we release for future research.",
    "status": "done"
  },
  {
    "id": 1674,
    "year": 2025,
    "title": "BFId: Identity Inference Attacks Utilizing Beamforming Feedback Information",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765062",
    "abstract": "Beamforming, as introduced in WiFi 5, requires clients to broadcast observations of their channel characteristics. This introduces a new information source for WiFi sensing with privacy threats that have not been explored, so far. With WiFi networks being ubiquitous in our everyday lives, the impact of unknown privacy threats is likely severe. To investigate this concern, we introduce BFId, the first identity inference attack using BFI-based sensing and evaluate its efficacy on a novel dataset containing WiFi recordings of 197 individuals. We show that we can infer the identity of individuals with very high accuracy, across different walking styles and perspectives, even with large sample sizes.",
    "status": "done"
  },
  {
    "id": 1675,
    "year": 2025,
    "title": "Systematic Assessment of Tabular Data Synthesis",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765067",
    "abstract": "Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. In recent years, a plethora of tabular data synthesis algorithms (i.e., synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to drawbacks in evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art statistical synthesizers. In this paper, we present a systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. We conducted extensive evaluations of 8 different types of synthesizers on 12 real-world datasets and identified some interesting findings, which offer new directions for privacy-preserving data synthesis.",
    "status": "done"
  },
  {
    "id": 1676,
    "year": 2025,
    "title": "Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765075",
    "abstract": "Dynamic Searchable Symmetric Encryption (DSSE) allows secure searches over a dynamic encrypted database but suffers from inherent information leakage. Existing passive attacks against DSSE rely on persistent leakage monitoring to infer leakage patterns, whereas this work targets intermittent observation - a more practical threat model. We propose Peekaboo - a new universal attack framework - and the core design relies on inferring the search pattern and further combining it with auxiliary knowledge and other leakage. We instantiate Peekaboo over the SOTA attacks, Sap (USENIX' 21) and Jigsaw (USENIX' 24), to derive their ''+'' variants (Sap+ and Jigsaw+). Extensive experiments demonstrate that our design achieves &gt;0.9 adjusted rand index for search pattern recovery and ∼90\\% query accuracy vs. FMA's ∼30\\% (CCS' 23). Peekaboo's accuracy scales with observation rounds and the number of observed queries but also it resists SOTA countermeasures, with &gt;40\\% accuracy against file size padding and &gt;80\\% against obfuscation.",
    "status": "done"
  },
  {
    "id": 1677,
    "year": 2025,
    "title": "'Is this a scam?': The Nature and Quality of Reddit Discussion about Scams",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765030",
    "abstract": "People often use social media platforms to seek advice about scams like ecommerce fraud or phishing; however, little research has investigated the nature of such discussion. We conducted a multi-stage thematic analysis of 1,525 posts made to four communities focused on scam discussion on Reddit, primarily from /r/Scams. We found that posters use Reddit to identify scams, discuss the strategies employed by scammers, and obtain advice on coping with victimization. The scams discussed are primarily mediated by the internet or related technologies. Users in the communities we studied especially provide informational support and reassurance to victims, although some comments reinforce victim-blaming attitudes. We also observed qualitative differences in the types of support sought and given based on the community, with the board /r/Sextortion especially being used for emotional support. We conclude that Reddit's scam discussion communities serve as a valuable resource for scam prevention and remediation. Additionally, we discuss the potential for future research and law enforcement engagement on Reddit.",
    "status": "done"
  },
  {
    "id": 1678,
    "year": 2025,
    "title": "Leaky Apps: Large-scale Analysis of Secrets Distributed in Android and iOS Apps",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765033",
    "abstract": "Mobile apps store various types of secrets to support their functionalities. These include API keys, and cryptographic material to authenticate users and access backend services. Once distributed, attackers can reverse-engineer the apps, and these secrets become accessible, posing risks such as data leaks, and service abuse.In this paper, we conduct a large-scale analysis of 10,331 Android and iOS apps to study how secrets are embedded in mobile apps. Our methodology involves extracting and validating credentials from app bundles and comparing the types and frequency of embedded secrets across Android and iOS to identify systematic differences between the two ecosystems. To assess temporal dynamics, we re-analyze apps released in 2023 after their updates in 2024. Our findings show that apps not only leak secrets required for functionality but also unintentionally include sensitive information like markdown documentation, and dependency management files.We discovered 416 functional credentials across 65 services, including 13 Git credentials that grant access to 218 public and 2,440 private repositories. Our analysis reveals that iOS apps are more likely to expose secrets, although information leaks exist in both Android and iOS apps. Finally, we show that even if developers remove embedded credentials in later versions, they frequently forget to revoke them, leaving the credentials exploitable.",
    "status": "done"
  },
  {
    "id": 1679,
    "year": 2025,
    "title": "'We just did not have that on the embedded system': Insights and Challenges for Securing Microcontroller Systems from the Embedded CTF Competitions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765039",
    "abstract": "Microcontroller systems are integral to our daily lives, powering mission-critical applications such as vehicles, medical devices, and industrial control systems. Therefore, it is essential to investigate and outline the challenges encountered in developing secure microcontroller systems. While previous research has focused solely on microcontroller firmware analysis to identify and characterize vulnerabilities, our study uniquely leverages data from the 2023 and 2024 MITRE eCTF team submissions and post-competition interviews. This approach allows us to dissect the entire lifecycle of secure microcontroller system development from both technical and perceptual perspectives, providing deeper insights into how these vulnerabilities emerge in the first place. Through the lens of eCTF, we identify fundamental conceptual and practical challenges in securing microcontroller systems. Conceptually, it is difficult to adapt from a microprocessor system to a microcontroller system, and participants are not wholly aware of the unique attacks against microcontrollers. Practically, security-enhancing tools, such as the memory-safe language Rust, lack adequate support on microcontrollers. Additionally, poor-quality entropy sources weaken cryptography and secret generation. Our findings articulate specific research, developmental, and educational deficiencies, leading to targeted recommendations for researchers, developers, vendors, and educators to enhance the security of microcontroller systems.",
    "status": "done"
  },
  {
    "id": 1680,
    "year": 2025,
    "title": "Walking The Last Mile: Studying Decompiler Output Correction in Practice",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765040",
    "abstract": "The increasing prevalence of Python has spurred interest in decompiling Python PYC bytecode. This work presents the first large-scale study on human-assisted Python decompilation in the wild, leveraging extensive data from pylingual.io, spanning 181,646 PYC binaries, 9,003 user-submitted patches, and 393 accuracy-verified patches. We investigate how reverse engineers respond to inaccurate decompilation and identify factors influencing their efforts to achieve accurate decompilation. We complement this unprecedented observational data with a controlled user study that isolates the technical difficulty of patching imperfect Python decompilations.By contrasting real-world patching behavior with that of the controlled setting, we discover that reversers' decision to repair a decompilation result is more strongly driven by the semantic content of the program (e.g., malware binaries or malicious tools) than by the technical difficulty of the patch. That is, a reverser's motivation is more important than their expertise.Our study reveals common patterns observed in the patching process, including how users approached the patching task, the types of errors they encountered, and the strategies they employed to resolve them. We also examine the strengths and limitations of assistive tools in the pursuit of perfect decompilation. Our findings offer unique insights into the practical dynamics of human-decompiler interaction, providing actionable recommendations for integrating human intelligence into the decompilation workflow and demonstrating the research potential of reliable decompilation accuracy verification.",
    "status": "done"
  },
  {
    "id": 1681,
    "year": 2025,
    "title": "A Qualitative Analysis of Fuzzer Usability and Challenges",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765055",
    "abstract": "Fuzzing is a widely adopted technique for uncovering software vulnerabilities by generating random or mutated test inputs to trigger unexpected behavior. However, little is known about how developers actually use fuzzing tools in practice, the challenges they face, and where current tools fall short. This study investigates the human side of fuzzing via 18 semi-structured interviews with fuzzing users across diverse domains. These interviews explore participants' workflows, frustrations, and expectations around fuzzing, revealing critical usability gaps and design opportunities. Our results can inform the next generation of fuzzing tools to improve user experience, reduce manual effort, and enable more effective integration of fuzzing into real-world workflows.",
    "status": "done"
  },
  {
    "id": 1682,
    "year": 2025,
    "title": "It Should Be Easy but... New Users' Experiences and Challenges with Secret Management Tools",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765065",
    "abstract": "Software developers face risks of leaking their software secrets, such as API keys or passwords, which can result in significant harm. Secret management tools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly recommended by industry, academia, and security guidelines to manage secrets securely. SMTs are designed to help developers secure their secrets in a central location, yet secrets leaks are still commonplace, and developers report difficulty in learning how to setup and use SMTs. While SMTs typically come with publicly available help resources (e.g., tool documentation and interfaces), it is unclear if these actually help developers learn to effectively use SMTs. Without usable help resources that onboards developers, quick adoption and effective use of SMTs may be unrealistic. In a qualitative two-step study, we observed 21 new users in person while they used SMTs to perform two secret management tasks: secret storage and access, then secret injection. We interviewed participants after each task to identify their challenges and experiences using SMTs, with the assistance of help resources. While our study sample is narrow, it serves as a reasonable proxy for new developers who are likely to adopt SMTs early in their careers. We found that even in a laboratory setting where new users found tool functionality and interface flexibility helpful, they still experienced increased difficulty to effectively use SMTs to securely remediate a hard-coded secret when they felt tool documentation was insufficient. Insufficient tool documentation motivated participants to deviate from official tool documentation to access secondary sources or attempt workaround methods. Specific challenges reported by participants were tool documentation content quality, navigation difficulties with both tool documentation and web interfaces for finding helpful content, and supportive tool features. We explain how these challenges negatively affect participant experiences adopting SMTs, and suggest recommendations on tool documentation and interfaces for SMT developers. If developers cannot simply and quickly manage secrets securely, secret leakage will continue to be commonplace.",
    "status": "done"
  },
  {
    "id": 1683,
    "year": 2025,
    "title": "CHaRM: Checkpointed and Hashed Counters for Flexible and Efficient Rowhammer Mitigation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765021",
    "abstract": "Despite efforts by DRAM vendors to mitigate Rowhammer, it is still a potent attack vector. CPU vendors are reluctant to deploy deterministic mitigations against Rowhammer due to the high cost that needs to be paid for the most vulnerable DRAM device, even though an average DRAM device is considerably less vulnerable. The main reason for this high cost is the need to track an increasing number of aggressor rows with the worsening Rowhammer threshold. Our proposed in-CPU mitigation, called CHaRM, breaks this dependency by efficiently mapping a large number of rows to a fixed number of hashed counters. Since multiple rows are now mapped to a limited number of counters, collisions can occur. To avoid excessive mitigative refreshes upon collisions, CHaRM deploys a checkpointing mechanism that saves the state of rows evicted from the table. When a row is activated again, CHaRM restores its checkpointed value and resumes tracking. Our evaluation shows that CHaRM incurs negligible slowdown, below 1\\% across all Rowhammer thresholds, while improving area, power, and energy by 3.8x, 4.4x, and 8.2x, respectively, for Rowhammer threshold of 1K compared to the state of the art.",
    "status": "done"
  },
  {
    "id": 1684,
    "year": 2025,
    "title": "ZVDetector: State-Guided Vulnerability Detection System for Zigbee Devices",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765035",
    "abstract": "Nowadays, Zigbee devices are widely used in smart home, smart agriculture and other industries. However, there are many vulnerabilities in Zigbee devices that could compromise their normal functionality. Existing research either analyzes firmware or fuzzes devices through Zigbee networks to discover potential vulnerabilities. However, they overlook the impact of device state and protocol state on firmware or explore only a limited state space. Thus, they fail to identify many vulnerabilities caused by hidden states within each of the two states, especially vulnerabilities triggered by the combination of these two states. In this paper, we design a state-guided fuzzing system, named ZVDetector, aimed at uncovering firmware vulnerabilities caused by hidden and combined states. Specifically, we design two state-aware modules that explore richer unknown protocol state transitions based on message relationships and gain a more complete understanding of the intrinsic device state attributes. We develop a fuzzing algorithm that incorporates message semantics awareness and correlation state analysis. By integrating the perceived state information, it can explore the combined state space more efficiently. We validate the performance of ZVDetector on 10 Zigbee devices and find 25 vulnerabilities (19 zero-day). Our experiments also demonstrate the ability to explore more device state attributes and discover more message relationships related to unknown protocol states.",
    "status": "done"
  },
  {
    "id": 1685,
    "year": 2025,
    "title": "ExfilState: Automated Discovery of Timer-Free Cache Side Channels on ARM CPUs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765061",
    "abstract": "Microarchitectural attacks and reverse-engineering efforts rely on inferring the cache state of cache lines. While high-resolution timers traditionally enable this, such timers are increasingly restricted or unavailable to unprivileged users on modern ARM64 systems. We introduce a fuzzing-based methodology to automatically discover instruction sequences that leak cache state into architectural state—without timing measurements. Our proof-of-concept, ExfilState, uses differential testing, F-score ranking, and covert-channel verification to identify architectural side channels on ARM64 CPUs. Across 160 devices with 37 microarchitectures—including smartphones, laptops, and cloud servers--ExfilState uncovers 5 undocumented side channels, 2 of which are reliably and widely exploitable. We demonstrate their practical impact with a timer-free Spectre variant, a cache-based AES key-recovery attack, and a novel defense mechanism that aborts sensitive algorithms on eviction of victim cache lines. Our findings show that architectural side channels are both real and exploitable, even in environments without timers, broadening the attack surface on modern ARM64 platforms.",
    "status": "done"
  },
  {
    "id": 1686,
    "year": 2025,
    "title": "MileSan: Detecting Exploitable Microarchitectural Leakage via Differential Hardware-Software Taint Tracking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765066",
    "abstract": "Microarchitectural performance optimizations introduce information flows inside CPU implementations that exceed those defined by the Instruction Set Architecture (ISA). Microarchitectural vulnerabilities, such as constant-time violations and various classes of transient execution attacks, are subsets of these excessive information flows. We observe that an exploitable microarchitectural leakage is an excessive information flow that can affect the time it takes for the CPU to execute a particular instruction, creating a timing covert channel. We design MileSan, the first RTL sanitizer that is capable of detecting exploitable microarchitectural leakage by checking for the architecturally-observable differences between architectural and microarchitectural information flows. For a given program and CPU implementation, MileSan computes architectural flows using software taint tracking and microarchitectural flows using RTL taint tracking. Evaluating the exploitability of proof of concepts generated by previous microarchitectural fuzzers, we find cases that are in fact not exploitable and discover the particular microarchitectural components that enable exploitation for the rest.In addition to assessing exploitability, MileSan enables the generation of random test programs with strictly-defined architectural information flows of secret data using a novel technique called taint-aware in-situ simulation. Leveraging this capability, we build RandOS, a new microarchitectural fuzzer that generates random programs traversing different privilege levels and address spaces, akin to random operating systems. Evaluation using five RISC-V CPUs shows that RandOS not only detects known exploitable vulnerabilities 4.5x faster than the state of the art, but also discovers 19 new constant-time violations and transient execution vulnerabilities in well-tested CPUs, such as BOOM, CVA6 and OpenC910.",
    "status": "done"
  },
  {
    "id": 1687,
    "year": 2025,
    "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765069",
    "abstract": "While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI '18] point out that eavesdropping on HBM is difficult—even for physical attackers—as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI '20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits.Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fasT OMAP accelerator that, for the first time, achieves O(1)+O(log2log2 N) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279X and 480X speedups in initialization and query time, respectively, over state-of-the-art OMAPs, which includes an industry implementation from Facebook.",
    "status": "done"
  },
  {
    "id": 1688,
    "year": 2025,
    "title": "FlexEmu: Towards Flexible MCU Peripheral Emulation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765086",
    "abstract": "Microcontroller units (MCUs) are widely used in embedded devices due to their low power consumption and cost-effectiveness. MCU firmware controls these devices and is vital to the security of embedded systems. However, performing dynamic security analyses for MCU firmware has remained challenging due to the lack of usable execution environments -- existing dynamic analyses cannot run on physical devices (e.g., insufficient computational resources), while building emulators is costly due to the massive amount of heterogeneous hardware, especially peripherals. Recent advances in automated peripheral emulation have made MCU emulation more scalable. However, these efforts only support limited peripherals and are hard to extend because they require ad-hoc adaptations. Our work is based on the insight that MCU peripherals can be modeled in a two-fold manner. At the structural level, peripherals have diverse implementations. But we can use a limited set of primitives to abstract peripherals because their hardware implementations are based on common hardware concepts. These primitives are abstract and can be instantiated with peripheral-specific implementation details to accommodate diverse peripheral implementations. At the semantic level, peripherals have diverse functionalities. However, we can use a single unified semantic model to describe the same kind of peripherals because they exhibit similar functionalities. Primitives serve as basic building blocks, allowing flexible semantic model construction. Building on this, we propose FlexEmu, a flexible MCU peripheral emulation framework. Once semantic models are created, FlexEmu automatically extracts peripheral-specific details to instantiate models and generate emulators accordingly. We have successfully applied FlexEmu to model 12 kinds of MCU peripherals. Our evaluation on 90 firmware samples across 15 different MCU platforms shows that the automatically generated emulators can faithfully replicate hardware behaviors and achieve a 98.48\\% unit test passing rate, outperforming state-of-the-art approaches. To demonstrate the implications of FlexEmu on firmware security, we use the generated emulators to fuzz three popular RTOSes and uncover 10 previously unknown bugs.",
    "status": "done"
  },
  {
    "id": 1689,
    "year": 2025,
    "title": "Rethinking Tamper-Evident Logging: A High-Performance, Co-Designed Auditing System",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765024",
    "abstract": "Existing tamper-evident logging systems suffer from high overhead and severe data loss in high-load settings, yet only provide coarse-grained tamper detection. Moreover, installing such systems requires recompiling kernel code. To address these challenges, we present Nitro, a high-performance, tamper-evident audit logging system that supports fine-grained detection of log tampering. Even better, our system avoids kernel recompilation by using the eBPF technology. To formally justify the security of Nitro, we provide a new definitional framework for logging systems, and give a practical cryptographic construction meeting this new goal. Unlike prior work that focus only on the cryptographic processing, we codesign the cryptographic part with the pre- and post-processing of the logs to exploit all system-level optimizations. Our evaluations demonstrate Nitro's superior performance, achieving 10X-25X improvements in high-stress conditions and 2X-10X in real-world scenarios while maintaining near-zero data loss. We also provide an advanced variant, Nitro-R that introduces in-kernel log reduction techniques to reduce runtime overhead even further.",
    "status": "done"
  },
  {
    "id": 1690,
    "year": 2025,
    "title": "Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765027",
    "abstract": "We use browsers daily to access all sorts of information. Because browsers routinely process scripts, media, and executable code from unknown sources, they form a critical security boundary between users and adversaries. A common attack vector is JavaScript, which powers complex web interactions but exposes a large attack surface due to the sheer complexity of modern JavaScript engines. To mitigate these threats, modern engines increasingly adopt software-based fault isolation (SFI). A prominent example is Google's V8 heap sandbox, which represents the most widely deployed SFI mechanism, protecting billions of users across all Chromium-based browsers and countless applications built on Node.js and Electron. The heap sandbox splits the address space into two parts: one part containing trusted, security-sensitive metadata, and a sandboxed heap containing memory accessible to untrusted code. On a technical level, the sandbox enforces isolation by removing raw pointers and using translation tables to resolve references to trusted objects. Consequently, an attacker cannot corrupt trusted data even with full control of the sandboxed data, unless there is a bug in how code handles data from the sandboxed heap. Despite their widespread use, such SFI mechanisms have seen surprisingly little security testing. In this work, we propose a new testing technique that faithfully models the security boundary of modern SFI implementations. Following the SFI threat model, we assume a powerful attacker who fully controls the sandbox's memory. We implement this by instrumenting memory loads originating in the trusted domain and accessing untrusted, attacker-controlled sandbox memory. We then inject faults into the loaded data, aiming to trigger memory corruption in the trusted domain that processes this untrusted input. We implement our approach in a tool called SbxBrk and evaluate it on the V8 heap sandbox. In a comprehensive evaluation, we identify 19 security bugs in V8 that enable an attacker to bypass the sandbox.",
    "status": "done"
  },
  {
    "id": 1691,
    "year": 2025,
    "title": "GPU Travelling: Efficient Confidential Collaborative Training with TEE-Enabled GPUs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765029",
    "abstract": "Confidential collaborative machine learning (ML) enables multiple mutually distrusted data holders to jointly train an ML model while preserving the confidentiality of their private datasets due to regulatory or competitive reasons. However, existing works need frequent data and model exchanges during training via slower conventional links. They face increasing challenges due to the exponentially growing sizes of models and datasets in modern training workloads like large language models (LLMs), resulting in prohibitively high communication costs. In this paper, we propose a novel mechanism called GPU Travelling that leverages recently emerged confidential GPUs. With our rigorous design, the GPU can securely travel to the specific data holder to load the dataset directly into the GPU's protected memory and then return for training, eliminating the need for data transmission while ensuring confidentiality up to a data-centre level. We developed a prototype using Intel TDX and NVIDIA H100 and evaluated its performance on llm.c, a CUDA-based LLM training project, and demonstrated the performance and feasibility while maintaining strong security guarantees. The results showed at least 4x speed improvement when transmitting a 512 MiB dataset chunk versus conventional transmission.",
    "status": "done"
  },
  {
    "id": 1692,
    "year": 2025,
    "title": "Sleeping Giants - Activating Dormant Java Deserialization Gadget Chains through Stealthy Code Changes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765031",
    "abstract": "Java deserialization gadget chains are a well-researched critical software weakness. The vast majority of known gadget chains rely on gadgets from software dependencies. Furthermore, it has been shown that small code changes in dependencies have enabled these gadget chains. This makes gadget chain detection a purely reactive endeavor. Even if one dependency's deployment pipeline employs gadget chain detection, a gadget chain can still result from gadgets in other dependencies. In this work, we assess how likely small code changes are to enable a gadget chain. These changes could either be accidental or intentional as part of a supply chain attack. Specifically, we show that class serializability is a strongly fluctuating property over a dependency's evolution. Then, we investigate three change patterns by which an attacker could stealthily introduce gadgets into a dependency. We apply these patterns to 533 dependencies and run three state-of-the-art gadget chain detectors both on the original and the modified dependencies. The tools detect that applying the modification patterns can activate/inject gadget chains in 26.08\\% of the dependencies we selected. Finally, we verify the newly detected chains. As such, we identify dormant gadget chains in 53 dependencies that could be added through minor code modifications. This both shows that Java deserialization gadget chains are a broad liability to software and proves dormant gadget chains as a lucrative supply chain attack vector.",
    "status": "done"
  },
  {
    "id": 1693,
    "year": 2025,
    "title": "Parcel Mismatch Demystified: Addressing a Decade-Old Security Challenge in Android",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765034",
    "abstract": "Parcel Mismatch vulnerabilities in Android's Inter-Process Communication (IPC) mechanism have been a persistent security challenge for over a decade, leading to numerous privilege escalation exploits. While Google has implemented various mitigation strategies, culminating in the Lazy Bundle mechanism in Android 13, there has been no systematic analysis of these vulnerabilities and mitigations. To fill the gap, in this paper, we conduct the first comprehensive study of Parcel Mismatch vulnerabilities, proposing ParcelTaint, a new static analysis approach for detecting these issues. We develop precise models for tracking Intent and Bundle transformations across processes, enabling the discovery of new attack vectors. We reveal 10 previously unknown high-severity vulnerabilities, and 5 of them have been assigned with CVEs, including new ways to bypass existing mitigations and new attack chains in system services. All of them have been confirmed. We find that Parcel Mismatch remains a significant security concern, particularly for Android versions prior to 13 and for Original Equipment Manufacturers (OEMs) implementing custom system components. Based on our findings, Google has revised its security strategy to address core vulnerability patterns rather than relying solely on system-level mitigations. The study provides crucial insights for improving Android's IPC security and highlights the importance of systematic analysis in addressing long-standing security challenges.",
    "status": "done"
  },
  {
    "id": 1694,
    "year": 2025,
    "title": "Deprivileging Low-Level GPU Drivers Efficiently with User-Space Processes and CHERI Compartments",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765036",
    "abstract": "Device drivers are a prominent source of operating system bugs and vulnerabilities, due to market pressures on hardware vendors and access to privileged system resources. OSes increasingly deprivilege drivers by moving them out of the kernel into user space, but this is widely understood to come with significant overhead. The perfect storm concerns GPU drivers, which are very large, complex and yet highly performance-sensitive. For performance reasons, large parts of these drivers run with full kernel privileges on major OSes. We deprivilege a GPU driver by moving it to user space. To avoid context-switching latency we run interrupt handlers inside eBPF sandboxes. Additionally, we take away the ability of the GPU driver to manage its own page tables and move this into an OS-vendor-vetted component. We create two variants. Firstly, a microkernel-inspired implementation which runs the driver in a standard Unix process. Secondly, we move the driver into a CHERI compartment. CHERI allows isolation of distrusting code in sandboxes without needing MMU-based separation. Compartments safely coexist within an address space, but efficiently share data by passing CHERI capabilities between each other, and incur reduced context-switching costs. To do this we use 'co-located processes', an existing framework which allows us to run graphics drivers and their applications as separate OS processes in a shared address space. Microkernel-like user-space drivers are still often believed to have high overheads, yet our Unix process-based implementation increases execution time on average by only 7.9\\% (geometric mean of the benchmark suite; max. 48.2\\%, min. 0.1\\%) for GPGPU and by 5.5\\% (max. 12.6\\%, min. -0.2\\%) for graphics workloads, while providing major security benefits. Despite these low costs, isolating processes with CHERI compartments, instead of address spaces, reduces average overheads to 6\\% (max. 36.6\\%, min. -0.2\\%) and 5\\% (max. 11.2\\%, min. 0.01\\%) respectively.",
    "status": "done"
  },
  {
    "id": 1695,
    "year": 2025,
    "title": "Towards a Formal Foundation for Blockchain ZK Rollups",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765115",
    "abstract": "Blockchains like Bitcoin and Ethereum have revolutionized digital transactions, yet scalability issues persist. Layer 2 solutions, such as validity proof Rollups (ZK-Rollups), aim to address these challenges by processing transactions off-chain and validating them on the main chain. However, concerns remain about security and censorship resistance, particularly regarding centralized control in Layer 2 and inadequate mechanisms for enforcing these properties through Layer 1 smart contracts. In their current form, L2s are susceptible to multisig attacks that can lead to total user funds loss. This work presents a formal analysis using the Alloy specification language to examine and design key Layer 2 functionalities, including forced transaction queues, safe blacklisting, and upgradeability. Through this analysis, we identify pitfalls in existing designs and introduce an enhanced model that has been model-checked to be correct. Finally, we propose a complete end-to-end methodology to analyze rollups' security and censorship resistance based on manually translating Alloy properties to property-based testing invariants, setting new standards.",
    "status": "done"
  },
  {
    "id": 1696,
    "year": 2025,
    "title": "Formal Security and Functional Verification of Cryptographic Protocol Implementations in Rust",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765213",
    "abstract": "We present an effective methodology for the formal verification of practical cryptographic protocol implementations written in Rust. Within a single proof framework, we show how to develop machine-checked proofs of diverse properties like runtime safety, parsing correctness, and cryptographic protocol security. All analysis tasks are driven by the software developer who writes annotations in the Rust source code and chooses a backend prover for each task, ranging from a generic proof assistant like F* to dedicated crypto-oriented provers like ProVerif and SSProve Our main contribution is a demonstration of this methodology on Bert13, a portable, post-quantum implementation of TLS 1.3 written in Rust and verified both for security and functional correctness. To our knowledge, this is the first security verification result for a protocol implementation written in Rust, and the first verified post-quantum TLS 1.3 library.",
    "status": "done"
  },
  {
    "id": 1697,
    "year": 2025,
    "title": "Secure Parsing and Serializing with Separation Logic Applied to CBOR, CDDL, and COSE",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765120",
    "abstract": "Incorrect handling of security-critical data formats, particularly in low-level languages, are the root cause of many security vulnerabilities. Provably correct parsing and serialization tools that target languages like C can help. Towards this end, we present PulseParse, a library of verified parser and serializer combinators for non-malleable binary formats. Specifications and proofs in PulseParse are in separation logic, offering a more abstract and compositional interface, with full support for data validation, parsing, and serialization. PulseParse also supports a class of recursive formats---with a focus on security and handling adversarial inputs, we show how to parse such formats with only a constant amount of stack space. We use PulseParse at scale by providing the first formalization of CBOR, a recursive, binary data format standard, with growing adoption in various other industrial standards. We prove that the deterministic fragment of CBOR is non-malleable and provide EverCBOR, a verified library in both C and Rust to validate, parse, and serialize CBOR objects implemented using PulseParse. Next, we provide the first formalization of CDDL, a schema definition language for CBOR. We identify well-formedness conditions on CDDL definitions to ensure that they yield unambiguous, non-malleable formats, and implement EverCDDL, a tool that checks the well-formedness of a CDDL definition and produces verified parsers and serializers for it. To evaluate our work, we use EverCDDL to generate verified parsers and serializers for various security-critical applications. Notably, we build a formally verified implementation of COSE signing, a standard for cryptographically signed objects. We also use our toolchain to generate verified code for other standards specified in CDDL, including DICE Protection Environment, a secure boot protocol standard. We conclude that PulseParse offers a powerful new foundation on which to build verified, secure data formatting tools for a range of applications.",
    "status": "done"
  },
  {
    "id": 1698,
    "year": 2025,
    "title": "Looping for Good: Cyclic Proofs for Security Protocols",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765131",
    "abstract": "Security protocols often involve loops, such as for ratcheting or for manipulating inductively-defined data structures. However, the automated analysis of security protocols has struggled to keep up with these features. The state-of-the-art often necessitates working with abstractions of such data structures or relies heavily on auxiliary, user-defined lemmas.In this work, we advance the state-of-the-art in symbolic protocol verification by adapting cyclic induction proof systems to the security protocol domain. We introduce reasoning rules for the Tamarin prover for cyclic proofs, enabling new, compact proofs, and we prove their soundness. Moreover, we implement new, simple, and effective proof search strategies that leverage these rules. With these additions, Tamarin can prove many lemmas that previously required, often complex, auxiliary lemmas. We showcase our approach on fourteen case studies, ranging from toy examples to a detailed model of the Signal protocol. Our work opens an exciting new research area where automatic induction helps scale security protocol verification, as we provide a fundamentally new and general induction mechanism.",
    "status": "done"
  },
  {
    "id": 1699,
    "year": 2025,
    "title": "Privacy-Preserving Runtime Verification",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765137",
    "abstract": "Runtime verification offers scalable solutions to improve the safety and reliability of systems. However, systems that require verification or monitoring by a third party to ensure compliance with a specification might contain sensitive information, causing privacy concerns when usual runtime verification approaches are used. Privacy is compromised if protected information about the system, or sensitive data that is processed by the system, is revealed. In addition, revealing the specification being monitored may undermine the essence of third-party verification.In this work, we propose two novel protocols for the privacy-preserving runtime verification of systems against formal sequential specifications. In our first protocol, the monitor verifies whether the system satisfies the specification without learning anything else, though both parties are aware of the specification. Our second protocol ensures that the system remains oblivious to the monitored specification, while the monitor learns only whether the system satisfies the specification and nothing more. Our protocols adapt and improve existing techniques used in cryptography, and more specifically, multi-party computation.The sequential specification defines the observation step of the monitor, whose granularity depends on the situation (e.g., banks may be monitored on a daily basis). Our protocols exchange a single message per observation step, after an initialisation phase. This design minimises communication overhead, enabling relatively lightweight privacy-preserving monitoring. We implement our approach for monitoring specifications described by register automata and evaluate it experimentally.",
    "status": "done"
  },
  {
    "id": 1700,
    "year": 2025,
    "title": "Generalized Security-Preserving Refinement for Concurrent Systems",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765138",
    "abstract": "Ensuring compliance with Information Flow Security (IFS) is known to be challenging, especially for concurrent systems with large codebases such as multicore operating system (OS) kernels. Refinement, which verifies that an implementation preserves certain properties of a more abstract specification, is promising for tackling such challenges. However, in terms of refinement-based verification of security properties, existing techniques are still restricted to sequential systems or lack the expressiveness needed to capture complex security policies for concurrent systems.In this work, we present a generalized security-preserving refinement technique, particularly for verifying the IFS of concurrent systems governed by potentially complex security policies. We formalize the IFS properties for concurrent systems and present a refinement-based compositional approach to prove that the generalized security properties (e.g., intransitive noninterference) are preserved between implementation and abstraction. The key intuition enabling such reasoning, compared to previous refinement work, is to establish a step-mapping relation between the implementation and the abstraction, which is sufficient to ensure that every paired step (in the abstraction and the implementation, respectively) is either permitted or prohibited by the security policy. We apply our approach to verify two non-trivial case studies against a collection of security policies. Our proofs are fully mechanized in Isabelle/HOL, during which we identified that two covert channels previously reported in the ARINC 653 single-core standard also exist in the ARINC 653 multicore standard. We subsequently proved the correctness of the revised mechanism, showcasing the effectiveness of our approach.",
    "status": "done"
  },
  {
    "id": 1701,
    "year": 2025,
    "title": "Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744827",
    "abstract": "The rapid advancements and widespread application of deep neural networks (DNNs), coupled with their reliance on sensitive and private data, have sparked growing concerns regarding data privacy and the ''right to be forgotten''. To address these concerns, machine unlearning has been proposed to efficiently eliminate the influence of specific training data from trained DNNs. However, existing machine unlearning methods struggle with the large number of parameters in trained DNNs, which lead to slow execution and high memory consumption, making them impractical for large-scale models. In this paper, we shift our focus to the small set of weights in the final classification layer of DNNs, which are defined as as ''prototypes'' for different classes. Our key observation is that the prototype associated with the unlearned training data undergoes a significant shift, whereas prototypes of unrelated classes exhibit only minor changes when comparing the prototypes of original and retrained models. Based on this observation, we propose a novel machine unlearning approach that efficiently achieves machine unlearning by directly adjusting the prototypes of DNNs. We first introduce Naive Prototype Surgery (Naive PS), a fast and simplified method that uses a closed-form solution to approximate unlearning effect by directly adjusting the prototype associated with the unlearned data. Next, we propose Prototype Surgery (PS), which incorporates soft label information to fine-tune the prototypes of all classes, to achieve a more effective unlearning. Both methods achieve data unlearning by only modifying the prototypes in the DNNs, thus avoiding the challenges posed by the large number of model parameters. Extensive experiments on four datasets demonstrate that our methods significantly accelerate the unlearning process while achieving comparable results to five existing methods in terms of both unlearning performance and privacy guarantee.",
    "status": "done"
  },
  {
    "id": 1702,
    "year": 2025,
    "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744835",
    "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
    "status": "done"
  },
  {
    "id": 1703,
    "year": 2025,
    "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744836",
    "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction.To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to &lt;10\\%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is here.",
    "status": "done"
  },
  {
    "id": 1704,
    "year": 2025,
    "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744845",
    "abstract": "Today's text-to-image generative models are trained on millions of images sourced from the Internet, each paired with a detailed caption produced by Vision-Language Models (VLMs). This part of the training pipeline is critical for supplying the models with large volumes of high-quality image-caption pairs during training. However, recent work suggests that VLMs are vulnerable to stealthy adversarial attacks, where adversarial perturbations are added to images to mislead the VLMs into producing incorrect captions. In this paper, we explore the feasibility of adversarial mislabeling attacks on VLMs as a mechanism to poisoning training pipelines for text-to-image models. Our experiments demonstrate that VLMs are highly vulnerable to adversarial perturbations, allowing attackers to produce benign-looking images that are consistently miscaptioned by the VLM models. This has the effect of injecting strong ''dirty-label'' poison samples into the training pipeline for text-to-image models, successfully altering their behavior with a small number of poisoned samples. We find that while potential defenses can be effective, they can be targeted and circumvented by adaptive attackers. This suggests a cat-and-mouse game that is likely to reduce the quality of training data and increase the cost of text-to-image model development. Finally, we demonstrate the real-world effectiveness of these attacks, achieving high attack success (over 73\\%) even in black-box scenarios against commercial VLMs (Google Vertex AI and Microsoft Azure).",
    "status": "done"
  },
  {
    "id": 1705,
    "year": 2025,
    "title": "Towards Backdoor Stealthiness in Model Parameter Space",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744846",
    "abstract": "Backdoor attacks maliciously inject covert functionality into machine learning models, which has been considered a security threat. The stealthiness of backdoor attacks is a critical research direction, focusing on adversaries' efforts to enhance the resistance of backdoor attacks against defense mechanisms. Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot that backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks. Our code is publicly available.",
    "status": "done"
  },
  {
    "id": 1706,
    "year": 2025,
    "title": "A Practical and Secure Byzantine Robust Aggregator",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744851",
    "abstract": "In machine learning security, one is often faced with the problem of removing outliers from a given set of high-dimensional vectors when computing their average. For example, many variants of data poisoning attacks produce gradient vectors during training that are outliers in the distribution of clean gradients, which bias the computed average used to derive the ML model. Filtering them out before averaging serves as a generic defense strategy. Byzantine robust aggregation is an algorithmic primitive which computes a robust average of vectors, in the presence of an ε fraction of vectors which may have been arbitrarily and adaptively corrupted, such that the resulting bias in the final average is provably bounded. In this paper, we give the first robust aggregator that runs in quasi-linear time in the size of input vectors and provably has near-optimal bias bounds. Our algorithm also does not assume any knowledge of the distribution of clean vectors, nor does it require pre-computing any filtering thresholds from it. This makes it practical to use directly in standard neural network training procedures. We empirically confirm its expected runtime efficiency and its effectiveness in nullifying 10 different ML poisoning attacks.",
    "status": "done"
  },
  {
    "id": 1707,
    "year": 2025,
    "title": "AD-MPC: Asynchronous Dynamic MPC with Guaranteed Output Delivery",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765166",
    "abstract": "MPC-as-a-Service (MPCaaS) systems enable clients to outsource privacy-preserving computations to distributed servers, offering flexibility by adapting and configuring MPC protocols to meet diverse security requirements. However, traditional MPC protocols rely on a fixed set of servers for the entire computation process, limiting scalability. Dynamic MPC (DMPC) addresses this limitation by permitting participants to join or leave during the computation. Nevertheless, existing DMPC protocols assume synchronous networks, which can lead to failures under unbounded network delays. In this paper, we present AD-MPC, the first asynchronous dynamic MPC protocol. Our protocol ensures guaranteed output delivery under optimal resilience ((n = 3t + 1)). To achieve this, we introduce two critical components: an asynchronous dynamic preprocessing protocol that facilitates the on-demand generation of Beaver triples for secure multiplication, and an asynchronous transfer protocol that maintains consistency during party hand-offs. These components collectively ensure computation correctness and transfer consistency across participants. We implement AD-MPC and evaluate its performance across up to 20 geographically distributed nodes. Experimental results demonstrate that the protocol not only offers strong security guarantees in dynamic and asynchronous network environments but also achieves performance comparable to state-of-the-art DMPC protocols.",
    "status": "done"
  },
  {
    "id": 1708,
    "year": 2025,
    "title": "IND-CPA-D of Relaxed Functional Bootstrapping: A New Attack, A General Fix, and A Stronger Model",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765170",
    "abstract": "Fully homomorphic encryption (FHE) is a powerful and widely used primitive in lots of real-world applications. Recently, Li and Micciancio [Eurocrypt'21] introduced IND-CPA-D security, which strengthens the standard IND-CPA security by allowing the attacker to access a decryption oracle for honestly generated ciphertexts. Recently, Jung et al. [CCS'24] and Checri et al. [Crypto'24] have shown that even exact FHE schemes like FHEW/TFHE/BGV/BFV may still not be IND-CPA-D secure, by exploiting the bootstrapping failure. However, such attacks can be mitigated by setting negligible bootstrapping failure probability. On the other hand, Liu and Wang [Asiacrypt'24] proposed relaxed functional bootstrapping, which has orders of magnitude performance improvement and furthermore allows a free function evaluation during bootstrapping. These efficiency advantages make it a competitive choice in many applications. In this work, we show that the underlying secret key could be recovered within 10 minutes against all existing relaxed functional bootstrapping constructions, and even within 1 minute for some of them. Moreover, our attack works even with a negligible bootstrapping failure probability. Additionally, we propose a general fix that mitigates all the existing modulus-switching-error-based attacks in the IND-CPA-D model. This is achieved by constructing a new modulus switching procedure with essentially no overhead. Lastly, we show that IND-CPA-D may not be sufficient even for passive adversary model. Thus, we extend this model to IND-CPA-D with randomness (IND-CPA-DR).",
    "status": "done"
  },
  {
    "id": 1709,
    "year": 2025,
    "title": "Security Analysis of Privately Verifiable Privacy Pass",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765172",
    "abstract": "Privacy Pass is an anonymous authentication protocol which was initially designed by Davidson et al. (PETS'18) to reduce the number of CAPTCHAs that TOR users must solve. It issues single-use authentication tokens with anonymous and unlinkable redemption guarantees. The issuer and verifier of the protocol share a symmetric key, and tokens are privately verifiable. The protocol has sparked interest from both academia and industry, which led to an Internet Engineering Task Force (IETF) standard. While Davidson et al. formally analyzed the original protocol, the IETF standard introduces several changes to their protocol. Thus, the standardized version's formal security remains unexamined. We fill this gap by analyzing the IETF standard's privately verifiable Privacy Pass protocol. In particular, there are two main discrepancies between the analyzed and standardized version: First, the IETF version introduces a redemption context, that can be used for blindly embedding a validity period into the Privacy Pass tokens. We show that this variant has significant differences to public metadata extension that has been proposed for the same purpose in the literature. Redemption context offers better privacy and security than public metadata. We capture both stronger guarantees through game-based security definitions and show that the currently considered one-more unforgeability notion for Privacy Pass is insufficient when a redemption context is used. Thus, we propose a new property, targeted context unforgeability, and prove its incomparability to one-more unforgeability. Second, Davidson et al. focused on a concrete Diffie-Hellman based construction, whereas the IETF version is built generically from a verifiable oblivious pseudorandom function (VOPRF). Further, the analyzed protocol omitted the full redemption phase needed to prevent double-spending. We prove that the generic IETF construction satisfies the desired security and privacy guarantees covering the full life-cycle of tokens. Our analysis relies on natural security properties of VOPRFs, providing compatibility with any secure VOPRF instantiation. This enables crypto agility, e.g., allowing to switch to efficient quantum-safe VOPRFs when they become available.",
    "status": "done"
  },
  {
    "id": 1710,
    "year": 2025,
    "title": "Threshold ECDSA in Two Rounds",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765176",
    "abstract": "We propose the first two-round multi-party signing protocol for the Elliptic Curve Digital Signature Algorithm (ECDSA) in the threshold-optimal setting, reducing the number of rounds by one compared to the state of the art (Doerner et al., S&amp;P '24). We also resolve the security issue of presigning pointed out by Groth and Shoup (Eurocrypt '22), evading a security loss that increases with the number of pre-released, unused presignatures, for the first time among threshold-optimal schemes. Our construction builds on Non-Interactive Multiplication (NIM), a notion proposed by Boyle et al. (PKC '25), which allows parties to evaluate multiplications on secret-shared values in one round. In particular, we use the construction of Abram et al. (Eurocrypt '24) instantiated with class groups. The setup is minimal and transparent, consisting of only two class-group generators. The signing protocol is efficient in bandwidth, with a message size of 1.9 KiB at 128-bit security, and has competitive computational performance.",
    "status": "done"
  },
  {
    "id": 1711,
    "year": 2025,
    "title": "Fast Homomorphic Evaluation of LWR-based PRFs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765177",
    "abstract": "Certain applications of fully homomorphic encryption (such as transciphering, universal thresholdizers, and PIR) require randomness while operating over encrypted data. This randomness has to be obliviously generated in the encrypted domain and remain encrypted throughout the computation. Moreover, it should be guaranteed that independent-looking random coins can be obliviously generated for different computations.In this work, we consider the homomorphic evaluation of pseudorandom functions (PRFs) with a focus on practical lattice-based candidates. In the homomorphic PRF evaluation setting, given a fully homomorphic encryption of the PRF secret key s, it should be possible to homomorphically compute encryptions of PRF evaluations {PRFs(xi}Moveri=1 for public inputs {xi.}Moveriover=1 We consider this problem for PRF families based on the hardness of the Learning-With-Rounding (LWR) problem introduced by Banerjee, Peikert, and Rosen (EUROCRYPT 2012). We build on a random oracle variant of a PRF construction suggested by Banerjee et al., and demonstrate that it can be evaluated using only two sequential programmable bootstraps in the TFHE homomorphic encryption scheme. We also describe several modifications of this PRF---which we prove as secure as the original function---that support homomorphic evaluations using only one programmable bootstrap per slot.Numerical experiments were conducted using practically relevant FHE parameter sets from the TFHE-rs library. Our benchmarks show that a throughput of about 1000 encrypted pseudorandom bits per second (resp. 900 encrypted pseudorandom bits per second) can be achieved on an AWS hpc7a.96xlarge machine (resp. on a standard laptop with an Apple M2 chip), on a single thread. The PRF evaluation keys in our experiments have sizes roughly 40\\% and 60\\% of a bootstrapping key.",
    "status": "done"
  },
  {
    "id": 1712,
    "year": 2025,
    "title": "Fast Amortized Bootstrapping with Small Keys and Polynomial Noise Overhead",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765181",
    "abstract": "Most homomorphic encryption (FHE) schemes exploit a technique called single-instruction multiple-data (SIMD) to process several messages in parallel. However, they base their security in somehow strong assumptions, such as the hardness of approximate lattice problems with superpolynomial approximation factor. On the other extreme of the spectrum, there are lightweight FHE schemes that have much faster bootstrapping but no SIMD capabilities. On the positive side, the security of these schemes is based on lattice problems with (low-degree) polynomial approximation factor only, which is a much weaker security assumption. Aiming the best of those two options, Micciancio and Sorrell (ICALP'18) proposed a new amortized bootstrapping that can process many messages at once, yielding sublinear time complexity per message, and allowing one to construct FHE based on lattice problems with polynomial approximation factor.Some subsequent works on this line achieve near-optimal asymptotic performance, nevertheless, concrete efficiency remains mostly an open problem. The only existing implementation to date (GPV23, Asiacrypt 2023) requires keys of up to a hundred gigabytes while only providing gains for relatively large messages. In this paper, we introduce a new method for amortized bootstrapping where the number of homomorphic operations required per message is O(h) and the noise overhead is O(√h λlogλ), where h is the Hamming weight of the LWE secret key and λ is the security parameter. This allows us to use much smaller parameters and to obtain faster running time. Our method is based on a new efficient homomorphic evaluation of sparse polynomial multiplication. We bootstrap 2 to 8-bit messages in 1.46 ms to 28.5 ms, respectively. Compared to TFHE-rs, this represents a performance improvement of 2.5 to 38.7 times while requiring bootstrapping keys up to 47.5 times smaller.",
    "status": "done"
  },
  {
    "id": 1713,
    "year": 2025,
    "title": "The Power to Never Be Wrong: Evasions and Anachronistic Attacks Against Web Archives",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765051",
    "abstract": "The Web is subject to link rot, where links break as webpages are updated or deleted. Web archiving services, such as the Wayback Machine, have emerged as a key solution to address link rot by archiving web content and preserving the look and feel of websites over time. These services offer critical functionality to users, serving as a historical baseline for an ever-changing Web. Implicit in everyone's use of these services is that they are capable of providing an accurate record of the past and can, therefore, provide reliable ground truth for comparing the past to the present. In this paper, we demonstrate that this implicit assumption does not necessarily hold. To this end, we propose two new threat models against web archiving services in which attackers can exert control over how their websites are archived. Evasive adversaries can distinguish crawlers operated by web archiving services from regular users, selectively denying or altering the content delivered to the former. Anachronistic adversaries can not only identify archive crawlers but also deliver content that enables them to retain control over archived snapshots. By abusing fundamental access-control mechanisms of the Web, these attackers can effectively alter the past as recorded by web archiving services. We found that all web archives we investigated suffer from one or more of these issues, challenging our current reliance on them.",
    "status": "done"
  },
  {
    "id": 1714,
    "year": 2025,
    "title": "Deep Dive into In-app Browsers: Uncovering Hidden Pitfalls in Certificate Validation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765215",
    "abstract": "While providing a seamless user experience by enabling web access within the app, in-app browsers raise security concerns, particularly in certificate validation, which can leave users vulnerable to Man-In-The-Middle (MITM) or phishing attacks unless appropriately implemented.In this paper, we systematically evaluated the certificate validation mechanisms of in-app browsers, also known as WebView, focusing on how effectively they comply with X.509 certificate standards and support advanced certificate extensions related to revocation and Certificate Transparency (CT). To ensure reproducibility and enable platform-specific trust anchor control which is particularly challenging on Android 14 and later, we developed a unified framework called FAITH using physical devices for iOS and Android emulators. Using FAITH and 115 crafted certificate chains—including 87 non-compliant chains and 28 designed to test advanced certificate extensions—we tested 20 popular Android and iOS apps, as well as desktop and mobile browsers. Android WebView apps accepted 77.0\\% of non-compliant chains and all non-compliant intermediate CA certificate tests, significantly higher than mainstream browsers and iOS apps. We identified the root cause in Android WebView's reliance on the system-level certificate validation handler, which performs minimal checks and lacks support for extensions such as OCSP Must-Staple and Precertificate. Additionally, we found that cached intermediate CA certificates are reused during validation in Android WebView, which exposes the process to unintended bypass of certificate checks. To demonstrate its real-world impact, we constructed a detailed CA caching attack scenario, and disclosed it to responsible vendors including Google. The reported bug was subsequently acknowledged as a valid security vulnerability. Finally, we conclude by providing recommendations to improve WebView's certificate validation behavior.",
    "status": "done"
  },
  {
    "id": 1715,
    "year": 2025,
    "title": "GAPDiS: Gradient-Assisted Perturbation Design via Sequence Editing for Website Fingerprinting Defense",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765084",
    "abstract": "As deep learning-based website fingerprinting (WF) attacks become increasingly accurate, user privacy faces mounting risks. Existing defenses struggle with the discrete nature of packet direction sequences, rendering gradient-based optimization infeasible and leading to inefficient, heuristic-based perturbation solutions. We propose a novel defense framework that bridges this gap by introducing gradient---aligned offset vectors and a cosine similarity---based reward to evaluate and select perturbation candidates aligned with the gradient direction. We further design a parallel reward computation algorithm to improve efficiency and integrate it into GAPDiS, a universal perturbation generation method that combines gradient guidance with improved tabu search for global optimization. For practical deployment, GAPDiS supports both PT bridge and P4 switch implementations. Experiments on the AWF dataset show that GAPDiS reduces the classification accuracy of WF models from over 98\\% to below 7\\% with only 2.56\\% bandwidth overhead---achieving a 68.1\\% improvement over state-of-the-art methods.",
    "status": "done"
  },
  {
    "id": 1716,
    "year": 2025,
    "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765094",
    "abstract": "The web continues to grow, but dependency-monitoring tools and standards for resource integrity lag behind. Currently, there exists no robust method to verify the integrity of web resources, much less in a generalizable yet performant manner, and supply chains remain one of the most targeted parts of the attack surface of web applications.In this paper, we present the design of LiMS, a transparent system to bootstrap link integrity guarantees in web browsing sessions with minimal overhead. At its core, LiMS uses a set of customizable integrity policies to declare the (un)expected properties of resources, verifies these policies, and enforces them for website visitors. We discuss how basic integrity policies can serve as building blocks for a comprehensive set of integrity policies, while providing guarantees that would be sufficient to defend against recent supply chain attacks detailed by security industry reports. Finally, we evaluate our open-sourced prototype by simulating deployments on a representative sample of 450 domains that are diverse in ranking and category. We find that our proposal offers the ability to bootstrap marked security improvements with an overall overhead of hundreds of milliseconds on initial page loads, and negligible overhead on reloads, regardless of network speeds. In addition, from examining archived data for the sample sites, we find that several of the proposed policy building blocks suit their dependency usage patterns, and would incur minimal administrative overhead.",
    "status": "done"
  },
  {
    "id": 1717,
    "year": 2025,
    "title": "In the DOM We Trust: Exploring the Hidden Dangers of Reading from the DOM on the Web",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765117",
    "abstract": "The DOM tree is a central part of modern web development, enabling JavaScript to interact with page content and structure. Only a few prior studies have studied its trustworthiness, despite its widespread use in guiding program logic and security decisions. Most notably, script gadgets have shown how this trust can be exploited by triggering the execution of benign JavaScript fragments with seemingly harmless markup injections. In this paper, we show that script gadgets are only the tip of the iceberg. Seemingly-benign markup injections can trigger the execution of fragments - that we call DOM gadgets - that, unlike script gadgets, do not necessarily result in a cross-site scripting vulnerability. Instead, they can result in a broader set of attacks, such as browser request hijacking attacks, cross-site request forgery attacks, and user interface manipulations.In this paper, we introduce an automated approach that combines static and dynamic analysis to detect DOM gadgets, tracing flows from the DOM to security-sensitive sinks, and assessing the presence of validation or sanitization checks. We conduct a large-scale web crawl across the top 15k domains and identify 2.6 million DOM-to-sink data flows that could lead to DOM gadget exploitation. We complement this by automatically detecting markup injection vulnerabilities, finding 657 DOM gadgets on 37 sites with the markup injection vulnerability required to exploit the DOM gadget. We further analyze these flows to assess the presence and effectiveness of security checks, revealing that 10\\% of DOM gadget flows receive no validation or sanitization checks. Our results indicate that DOM-based input trust is both widespread and underprotected. Our work highlights the scale and diversity of DOM gadget vulnerabilities in the wild, motivating a rethink of the DOM's role in web application trust boundaries and offering tools to aid in their identification and mitigation.",
    "status": "done"
  },
  {
    "id": 1718,
    "year": 2025,
    "title": "Head(er)s Up! Detecting Security Header Inconsistencies in Browsers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765119",
    "abstract": "In the modern Web, security headers are of the utmost importance for websites to provide protection against various attacks, such as Cross-Site Scripting, Clickjacking, and Cross-Site Leaks. As each security header uses a different syntax and has unique processing rules, correctly implementing them is a complex task for both browser and website developers. Inconsistency in browser behavior related to security headers harms websites as their security depends on their users' browsers. At the same time, compatibility issues may deter developers from deploying such headers in the first place. In this work, we performed a differential evaluation of the security header parsing and enforcement behavior in desktop and mobile browsers to uncover problematic browser differences. We systematically ran 177,146 tests covering 16 security-relevant headers multiple times in 16 browser configurations covering over 97\\% of the browser engine market share. We identified 5,606 (3.16\\%) tests that behave inconsistently across browsers. Our subsequent analysis revealed 42 root causes, highlighting the prevalence of implementation issues. 31 of these root causes were yet unknown and resulted in 36 bug reports against the affected browsers and specifications. Many of our reports have already resulted in fixes improving web consistency and users' security. To foster open science and enable browser vendors to continuously test their security header implementations, we open-source our test framework.",
    "status": "done"
  },
  {
    "id": 1719,
    "year": 2025,
    "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744856",
    "abstract": "The increasing demand for domain-specific and human-aligned Large Language Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning (SFT) techniques. SFT datasets often comprise valuable instruction-response pairs, making them highly valuable targets for potential extraction. This paper studies this critical research problem for the first time. We start by formally defining and formulating the problem, then explore various attack goals, types, and variants based on the unique properties of SFT data in real-world scenarios. Based on our analysis of extraction behaviors of direct extraction, we develop a novel extraction method specifically designed for SFT models, called Differentiated Data Extraction (DDE), which exploits the confidence levels of fine-tuned models and their behavioral differences from pre-trained base models. Through extensive experiments across multiple domains and scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our results show that DDE consistently outperforms existing extraction baselines in all attack settings. To counter this new attack, we propose a defense mechanism that mitigates DDE attacks with minimal impact on model performance. Overall, our research reveals hidden data leak risks in fine-tuned LLMs and provides insights for developing more secure models.",
    "status": "done"
  },
  {
    "id": 1720,
    "year": 2025,
    "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744859",
    "abstract": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone to adversarial attacks. Typically, such attacks either involve frequent queries to the target model or rely on surrogate models closely mirroring the target model --- often trained with subsets of the target model's training data --- to achieve high attack success rates through transferability. However, in realistic scenarios where training data is inaccessible and excessive queries can raise alarms, crafting adversarial examples becomes more challenging. In this paper, we present UnivIntruder, a novel attack framework that relies solely on a single, publicly available CLIP model and publicly available datasets. By using textual concepts, UnivIntruder generates universal, transferable, and targeted adversarial perturbations that mislead DNNs into misclassifying inputs into adversary-specified classes defined by textual concepts. Our extensive experiments show that our approach achieves an Attack Success Rate (ASR) of up to 85\\% on ImageNet and over 99\\% on CIFAR-10, significantly outperforming existing transfer-based methods. Additionally, we reveal real-world vulnerabilities, showing that even without querying target models, UnivIntruder compromises image search engines like Google and Baidu with ASR rates up to 84\\%, and vision language models like GPT-4 and Claude-3.5 with ASR rates up to 80\\%. These findings underscore the practicality of our attack in scenarios where traditional avenues are blocked, highlighting the need to reevaluate security paradigms in AI applications.",
    "status": "done"
  },
  {
    "id": 1721,
    "year": 2025,
    "title": "DivTrackee versus DynTracker: Promoting Diversity in Anti-Facial Recognition against Dynamic FR Strategy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744867",
    "abstract": "The widespread adoption of facial recognition (FR) models raises serious concerns about their potential misuse, motivating the development of anti-facial recognition (AFR) to protect user facial privacy. In this paper, we argue that the static FR strategy, predominantly adopted in prior literature for evaluating AFR efficacy, cannot faithfully characterize the actual capabilities of determined trackers who aim to track a specific target identity. In particular, we introduce DynTracker, a dynamic FR strategy where the model's gallery database is iteratively updated with newly recognized target identity images. Surprisingly, such a simple approach renders all the existing AFR protections ineffective. To mitigate the privacy threats posed by DynTracker, we advocate for explicitly promoting diversity in the AFR-protected images. We hypothesize that the lack of diversity is the primary cause of the failure of existing AFR methods. Specifically, we develop DivTrackee, a novel method for crafting diverse AFR protections that builds upon a text-guided image generation framework and diversity-promoting adversarial losses. Through comprehensive experiments on various image benchmarks and feature extractors, we demonstrate DynTracker's strength in breaking existing AFR methods and the superiority of DivTrackee in preventing user facial images from being identified by dynamic FR strategies. We believe our work can act as an important initial step towards developing more effective AFR methods for protecting user facial privacy against determined trackers.",
    "status": "done"
  },
  {
    "id": 1722,
    "year": 2025,
    "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744868",
    "abstract": "The growing adoption of artificial intelligence (AI) has amplified concerns about trustworthiness, including integrity, privacy, robustness, and bias. To assess and attribute these threats, we propose ConceptLens, a generic framework that leverages pre-trained multimodal models to identify the root causes of integrity threats by analyzing Concept Shift in probing samples. ConceptLens demonstrates strong detection performance for vanilla data poisoning attacks and uncovers vulnerabilities to bias injection, such as the generation of covert advertisements through malicious concept shifts. It identifies privacy risks in unaltered but high-risk samples, filters them before training, and provides insights into model weaknesses arising from incomplete or imbalanced training data. Additionally, at the model level, it attributes concepts that the target model is overly dependent on, identifies misleading concepts, and explains how disrupting key concepts negatively impacts the model. It uncovers sociological biases in generative content, revealing disparities across sociological contexts. ConceptLens reveals how otherwise safe training and inference data can be unintentionally and easily exploited to undermine safety alignment.",
    "status": "done"
  },
  {
    "id": 1723,
    "year": 2025,
    "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744882",
    "abstract": "We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50\\% or greater) attack success rate is ideal. However, for certain elections, even a 5\\% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.",
    "status": "done"
  },
  {
    "id": 1724,
    "year": 2025,
    "title": "FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744883",
    "abstract": "Due to the lack of data auditing techniques for untrusted clients, Federated Learning (FL) is vulnerable to backdoor attacks. Although various methods have been proposed to protect FL against backdoor attacks, they still exhibit poor defense performance in extreme data heterogeneity scenarios. Worse still, these methods strongly rely on additional datasets, violating the privacy protection requirements of FL. To overcome the above shortcomings, this paper proposes a novel data-free backdoor defense approach for FL, named FilterFL, which strives to prevent uploaded client models with backdoor knowledge from participating in the aggregation operation in each FL communication round. Based on our knowledge extraction and backdoor filtering schemes using two well-designed Conditional Generative Adversarial Networks (CGANs), FilterFL extracts incremental knowledge learned by a newly updated global model and filters its backdoor components, which can be used to generate one sample that reflects backdoor knowledge for each category. If an uploaded local model can confidently classify a generated sample into its target category, the knowledge contributed by the model will be excluded from the aggregation. In this way, FilterFL can effectively defend against backdoor attacks without using any additional auxiliary data. Comprehensive experiments on well-known datasets demonstrate that, compared with state-of-the-art methods, our approach achieves the best defense performance within various data heterogeneity scenarios.",
    "status": "done"
  },
  {
    "id": 1725,
    "year": 2025,
    "title": "Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765071",
    "abstract": "Ethical questions are discussed regularly in computer security. Still, researchers in computer security lack clear guidance on how to make, document, and assess ethical decisions in research when what is morally right or acceptable is not clear-cut. In this work, we give an overview of the discussion of ethical implications in current published work in computer security by reviewing all 1154 publications at top 4 security conferences published in 2024, finding inconsistent levels of ethics reporting with a strong focus of reporting institutional or ethics board approval, human subjects protection, and responsible disclosure, and a lack of discussion of balancing harms and benefits. We further report on the results of a semi-structured interview study with 24 computer security and privacy researchers (among whom were also: reviewers, ethics committee members, and/or program chairs) and their ethical decision-making both as authors and during peer review, finding a strong desire for ethical research, but a lack of consistency in considered values, ethical frameworks (if articulated), decision-making, and outcomes. We present an overview of the current state of the discussion of ethics and current de-facto standards in computer security research, contributing suggestions to improve the state of ethics in computer security research.",
    "status": "done"
  },
  {
    "id": 1726,
    "year": 2025,
    "title": "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765072",
    "abstract": "Privacy policies are often complex. An exception is the two-page standardized notice that U.S. financial institutions must provide under the Gramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile apps, and other services that involve complex data sharing practices that require additional privacy notices and do-not-sell opt-outs. We conducted a large-scale analysis of how U.S. banks implement privacy policies and controls in response to GLBA; other federal privacy policy requirements; and the California Consumer Privacy Act (CCPA), a key example for U.S. state privacy laws. We focused on the disclosure and control of a set of especially privacy-invasive practices: third-party data sharing for marketing-related purposes. We collected privacy policies for the 2,067 largest U.S. banks, 45.2\\% of which provided multiple policies. Across disclosures and controls for the same bank, we identified frequent, concerning inconsistencies---53.8\\% of banks with multiple privacy policies indicated in GLBA notices that they do not share with third parties but disclosed sharing in other policies. This multiplicity of policies, with the inconsistencies it causes, may create consumer confusion and undermine the transparency goals of the very laws that require them. Our findings call into question whether current policy requirements, such as the GLBA notice, are achieving their intended goals in today's online banking landscape. We discuss potential avenues for reforming and harmonizing privacy policies and control requirements across federal and state laws.",
    "status": "done"
  },
  {
    "id": 1727,
    "year": 2025,
    "title": "How Blind and Low-Vision Users Manage Their Passwords",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765081",
    "abstract": "Managing passwords securely and conveniently is still an open problem for many users. Existing research has examined users' password management strategies and identified pain points, such as security concerns, leading to insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle this problem and how password managers can assist them. This paper presents the results of a qualitative interview study with N = 33 BLV participants. We found that all participants utilize password managers to some extent, which they perceive as fairly accessible. However, the adoption is mainly driven by the convenience of storing and retrieving passwords. The security advantages -- generating strong, random passwords -- were avoided mainly due to the absence of practical accessibility. Password managers do not adhere to BLV users' underlying needs for agency, which stem from experiences with inaccessible software and vendors who deprioritize accessibility issues. Underutilization of password managers leads BLV users to adopt insecure practices, such as reusing predictable passwords or resorting to 'security through obscurity' by writing important credentials in braille. We conclude our analysis by discussing the need to implement practical accessibility and usability improvements for password managers as a way of establishing trust and secure practices while maintaining BLV users' agency.",
    "status": "done"
  },
  {
    "id": 1728,
    "year": 2025,
    "title": "A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765085",
    "abstract": "An advanced persistent threat (APT) refers to a covert and long-term cyberattack, typically conducted by state-sponsored actors, targeting critical sectors and often remaining undetected for long periods. In response, collective intelligence from around the globe collaborates to identify and trace surreptitious activities, generating substantial documentation on APT campaigns publicly available on the web. While a multitude of prior works predominantly focus on specific aspects of APT cases, such as detection, evaluation, cyber threat intelligence, and dataset creation, limited attention has been devoted to revisiting and investigating these scattered dossiers in a longitudinal manner.The objective of our study lies in filling the gap by offering a macro perspective, connecting key insights and global trends in the past APT attacks. We systematically analyze six reliable sources--- three focused on technical reports and another three on threat actors--- examining 1,509 APT dossiers (i.e., totaling 24,215 pages) spanning from 2014 to 2023 (a decade), and identifying 603 unique APT groups in the world. To efficiently unearth relevant information, we employ a hybrid methodology that combines rule-based information retrieval with large-language-model-based search techniques. Our longitudinal analysis reveals shifts in threat actor activities, global attack vectors, changes in targeted sectors, and the relationships between cyberattacks and significant events, such as elections or wars, which provides insights into historical patterns in APT evolution. Over the past decade, 154 countries have been affected, primarily using malicious documents and spear phishing as the dominant initial infiltration vectors, and a noticeable decline in zero-day exploitation since 2016. Furthermore, we present our findings through interactive visualization tools, such as an APT map or a flow diagram, to facilitate intuitive understanding of the global patterns and trends in APT activities.",
    "status": "done"
  },
  {
    "id": 1729,
    "year": 2025,
    "title": "UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765088",
    "abstract": "With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which improves the effectiveness and robustness of existing classifiers, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.",
    "status": "done"
  },
  {
    "id": 1730,
    "year": 2025,
    "title": "The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765091",
    "abstract": "Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility.In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30\\% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we improve on an existing approach for automatically selecting the optimal number of bins, and achieve high utility while reducing both privacy budget consumption and computational overhead.",
    "status": "done"
  },
  {
    "id": 1731,
    "year": 2025,
    "title": "Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765092",
    "abstract": "High-definition (HD) maps provide precise environmental information essential for prediction and planning in autonomous driving (AD) systems. Due to the high cost of labeling and maintenance, recent research has turned to online HD map construction using onboard sensor data, offering wider coverage and more timely updates for autonomous vehicles (AVs). However, the robustness of online map construction under adversarial conditions remains underexplored. In this paper, we present a systematic vulnerability analysis of online map construction models, which reveals that these models exhibit an inherent bias toward predicting symmetric road structures. In asymmetric scenes like forks or merges, this bias often causes the model to mistakenly predict a straight boundary that mirrors the opposite side. We demonstrate that this vulnerability persists in the real-world and can be reliably triggered by obstruction or targeted interference. Leveraging this vulnerability, we propose a novel two-stage attack framework capable of manipulating online constructed maps. First, our method identifies vulnerable asymmetric scenes along the victim AV's potential route. Then, we optimize the location and pattern of camera-blinding attacks and adversarial patch attacks. Evaluations on a public AD dataset demonstrate that our attacks can degrade mapping accuracy by up to 9.9\\% in average precision, render up to 44\\% of targeted routes unreachable, and increase unsafe planned trajectory rates—colliding with real-world road boundaries—by up to 27\\%. These attacks are also validated on a real-world testbed vehicle. We further analyze root causes of the symmetry bias, attributing them to training data imbalance, model architecture, and map element representation. Based on these findings, we propose asymmetric data fine-tuning as a targeted defense, which significantly improves model robustness. To the best of our knowledge, this study presents the first vulnerability assessment of online map construction models and introduces the first digital and physical attack against them.",
    "status": "done"
  },
  {
    "id": 1732,
    "year": 2025,
    "title": "Pixnapping: Bringing Pixel Stealing out of the Stone Age",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765093",
    "abstract": "Pixel stealing attacks enable malicious websites to leak sensitive content displayed in victim websites. The idea, introduced by Stone in 2013, is to embed victim websites in iframes and use SVG filters to compute on, and create side channels as a function of, those websites' pixels. Fortunately, despite the danger, pixel stealing attacks are all but mitigated today thanks to websites and web browsers heavily restricting iframes and cross-origin cookie sharing.This paper introduces a pixel stealing framework targeting Android devices that bypasses all browser mitigations and can even steal secrets from non-browser apps. Our key observation is that Android APIs enable an attacker to create an analog to Stone-style attacks outside of the browser. Specifically, a malicious app can force victim pixels into the rendering pipeline via Android intents and compute on those victim pixels using a stack of semi-transparent Android activities. Crucially, our framework enables stealing secrets only stored locally (e.g., 2FA codes and Google Maps Timeline), which have never before been in reach of pixel stealing attacks.We instantiate our pixel stealing framework on Google and Samsung phones---which differ in both hardware and graphical software. On the Google phones, we additionally provide evidence that the pixel color-dependent timing measured in our attack is due to GPU graphical data compression. We demonstrate end-to-end attacks that steal pixels from both browser and non-browser victims, including accounts.google.com, gmail.com, Google Maps, Google Messages, and Venmo. Finally, we demonstrate an end-to-end attack capable of stealthily stealing security-critical and ephemeral 2FA codes from Google Authenticator in under 30 seconds.",
    "status": "done"
  },
  {
    "id": 1733,
    "year": 2025,
    "title": "HW-Spy: Handwriting Inference by Tracing Pen-Tail Movements",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765109",
    "abstract": "While keyboard typing has been the most common way of inputting texts, handwriting still plays an important role in generating, inputting, or recording information like filling out essential/private forms. Considerable research has been done to identify and demonstrate the risk of keystroke-inference attacks. However, little has been done on handwriting inference despite its high risk of leaking sensitive information. To assess this under-explored risk of information leakage, we present a novel handwriting-inference attack, called HW-Spy, by tracing the victim's pen-tail movements when both the pen tip and the writing surface are outside the view of the attacker's camera, which usually happens when the victim is multitasking during an online meeting, when the victim's writing scene (in a public space) is recorded by a remote camera, or when the victim's writing behaviors are captured by the surveillance camera in a bank/dealership/realty office. In particular, we apply image segmentation to the recorded video frames of the victim's writing activities and extract the victim's pen-tail movements as a 2D coordinate sequence. We then identify the stroke-associated movements from the recorded pen's in-air video frames using a 1D U-Net model trained for stroke mask prediction and segment the characters based on the thus-derived motion features. The pen-tail's coordinate segments are then fed into a Long Short-Term Memory (LSTM) network to reconstruct the actual handwriting, which is processed further by a transformer-based model to infer the hand-written content. Our extensive experimentation shows HW-Spy to achieve an accuracy, up to 84.2\\%, of personalized handwriting inference and a comparable accuracy, up to 79.5\\%, of non-personalized handwriting inference.",
    "status": "done"
  },
  {
    "id": 1734,
    "year": 2025,
    "title": "Combating Falsification of Speech Videos with Live Optical Signatures",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765112",
    "abstract": "High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes VeriLight, a low-overhead and unobtrusive system for protecting speech videos from visual manipulations of speaker identity and lip and facial motion. Unlike the predominant purely digital falsification detection methods, VeriLight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of VeriLight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds &gt;200 bps into video while remaining imperceptible both in video and live. Experiments on extensive video datasets show VeriLight achieves AUCs ≥ 0.99 and a true positive rate of 100\\% in detecting falsified videos. Further, VeriLight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its feature extraction methods. A demonstration of VeriLight is available at https://mobilex.cs.columbia.edu/verilight.",
    "status": "done"
  },
  {
    "id": 1735,
    "year": 2025,
    "title": "ConTest: Taming the Cyber-physical Input Space in Fuzz Testing with Control Theory",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765129",
    "abstract": "With the proliferation of Cyber-Physical Systems (CPSs) in daily life, the security of these systems is becoming an pressing problem. Fuzz testing has recently gained attention as a promising approach for automatically detecting vulnerabilities, however, the prohibitively large search space of physical and cyber inputs remains an open research challenge. To address this gap, the paper draws on control theory, leveraging physics-informed control models to guide exploration of the input space. We design and develop ConTest, a fuzzing tool that leverages Lyapunov functions of the control model for both detection and mutation to efficiently search through the parameter space with a provable guarantee on the effectiveness of bug-finding effectiveness under bounded dynamic model errors. We implemented a prototype of ConTest and deployed it to detect spatial and temporal input validation bugs in two representative robotic vehicle (RV) platforms, ArduPilot and PX4. A total of 253 input validation bugs were found, 58 of them being zero-day bugs, and 54 of them were acknowledged by the vendors.",
    "status": "done"
  },
  {
    "id": 1736,
    "year": 2025,
    "title": "RISCover: Automatic Discovery of User-exploitable Architectural Security Vulnerabilities in Closed-Source RISC-V CPUs",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765141",
    "abstract": "The open and extensible RISC-V instruction set has enabled many new CPU vendors and implementations, but most commercial CPUs are closed-source, significantly hindering vulnerability analysis—especially for bugs exploitable from unprivileged user space.We present RISCover, a user-space framework for detecting architectural vulnerabilities in closed-source RISC-V CPUs. It compares instruction-sequence behavior across CPUs, identifying deviations without source code, hardware changes, or models, and achieving orders-of-magnitude speedups over RTL-based methods. Unlike prior work, RISCover runs user code on Linux directly on real hardware, exposing vulnerabilities exploitable by unprivileged attackers. Evaluated on 8 off-the-shelf CPUs from 3 different vendors, it uncovers 4 previously unknown vulnerabilities.Notably, GhostWrite lets unprivileged code write chosen bytes to physical memory, enabling arbitrary data leakage and full machine-mode execution, while 3 unprivileged ''halt-and-catch-fire'' bugs halt CPUs and misaligned zero-stores silently corrupt data. Our results highlight the pressing need for post-silicon fuzzing techniques. RISCover complements existing RTL-level fuzzers by enabling rapid and automated security analysis of closed-source CPUs.",
    "status": "done"
  },
  {
    "id": 1737,
    "year": 2025,
    "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765037",
    "abstract": "Machine learning model repositories, such as the Hugging Face Model Hub, facilitate model exchanges. However, bad actors can deliver malware through compromised models. Existing defenses, such as safer model formats, restrictive (but inflexible) loading policies, and model scanners, have shortcomings: 44.9\\% of popular models on Hugging Face still use the insecure pickle format, 15\\% of these cannot be loaded by restrictive loading policies, and model scanners have both false positives and false negatives. Pickle remains the de facto standard for model exchange, and the ML community lacks a tool that offers transparent safe loading.We present PickleBall to help machine learning engineers load pickle-based models safely. PickleBall statically analyzes the source code of machine learning libraries and computes custom policies that specify a safe load-time behavior for benign models. It then dynamically enforces these policies during load time as a drop-in replacement for the pickle module. PickleBall generates policies that correctly load 79.8\\% of benign pickle-based models in our dataset, while rejecting all (100\\%) malicious examples in the same dataset. In comparison, evaluated model scanners fail to identify known malicious models, and the state-of-the-art loader loads 22\\% fewer benign models than PickleBall. PickleBall removes the threat of arbitrary function invocation from malicious pickle-based models, raising the bar for attackers as they have to depend on code reuse techniques.",
    "status": "done"
  },
  {
    "id": 1738,
    "year": 2025,
    "title": "Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765049",
    "abstract": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to enhance Large Language Models (LLMs) in code synthesis, improving the functional correctness of the generated code. However, existing RACG systems largely overlook security, leading to substantial risks. Especially, the poisoning of malicious code into knowledge bases can mislead LLMs, resulting in the generation of insecure outputs, which poses a critical threat in modern software development. To address this, we propose a security-hardening framework for RACG systems, CodeGuarder, that shifts the paradigm from retrieving only functional code examples to incorporating both functional code and security knowledge. Our framework constructs a security knowledge base by analyzing real-world vulnerabilities from the ReposVul dataset. For each code generation query, a retriever decomposes the query into fine-grained sub-tasks and fetches relevant security knowledge. To prioritize critical security guidance, we introduce a re-ranking and filtering mechanism by leveraging the LLMs' susceptibility to different vulnerability types. This filtered security knowledge is seamlessly integrated into the generation prompt. Our evaluation shows CodeGuarder significantly improves code security rates across various LLMs, achieving average improvements of 20.12\\% in standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning scenarios without compromising functional correctness. Furthermore, CodeGuarder demonstrates strong generalization, enhancing security even when the targeted language's security knowledge is lacking. This work presents CodeGuarder as a pivotal advancement towards building secure and trustworthy RACG systems.",
    "status": "done"
  },
  {
    "id": 1739,
    "year": 2025,
    "title": "Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765059",
    "abstract": "Tag-based sanitizers attach a small ''key'' to each pointer and a matching ''lock'' tag to its target memory object, enabling runtime verification of pointer-object consistency and helping developers to detect potential memory violations. However, the limited tag encoding space challenges existing studies in assigning distinct tags to memory objects across temporal and spatial dimensions, leading to potential tag collisions. Such limitations reduce the probabilistic protection capabilities of sanitizers and make them vulnerable to sophisticated tag probing attacks. In this paper, we present ClusterTag, a novel cluster-based memory allocator aimed at simultaneously mitigating tag collisions in both temporal and spatial dimensions. The core design of ClusterTag effectively balances the significant mismatch between tag encoding space and memory objects: it divides memory objects into multiple independent clusters, thereby limiting tag collisions to finite chunks within each cluster. To mitigate tag collisions across clusters, we design a cluster-grained heap randomization scheme. This approach introduces random address intervals between clusters and further breaks the entropy limitation of the tag space. ClusterTag has been implemented as an independent memory allocator that seamlessly integrates with tag-based sanitizers such as HWASan, and maintains comparable performance overhead (within 1\\%) at various randomization densities. Security evaluations on the Juliet dataset indicate that ClusterTag exhibits deterministic results across 500 repeated tests (5,652 reported and 1,530 missed), while the existing three types of tag assignment strategies all exhibit probabilistic false negatives due to tag collisions. Quantitative analysis across three tag collision distance metrics-minimum, average, and unpredictability-demonstrates that ClusterTag achieves balanced improvements across all three, whereas prior tag assignment schemes (random, staggered, fixed) show significant trade-offs in at least one metric.",
    "status": "done"
  },
  {
    "id": 1740,
    "year": 2025,
    "title": "Recover Function Signature from Combined Constraints",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765089",
    "abstract": "Recovering function signatures is a cornerstone of binary program analysis, yet it remains a challenging task. Existing methods either rely on disassembly-based constraints, which struggle with cross-architecture compatibility and scalability, or adopt learning-based approaches that are resource-intensive and often inaccurate. In this paper, we present CDA, a novel decompilation-based method for recovering function signatures that combines the strengths of multiple decompilers while mitigating their limitations. The core idea behind CDA is leveraging probabilistic constraints to estimate the likelihood of each function signature recovery result produced by decompilers, guided by inference rules specifically designed to address the limitations of decompilers. Based on these probabilities, CDA selects the recovery results with the highest likelihood as the final outcomes. We extensively evaluate CDA across five tasks --- variadic function/position detection, parameter identification, return value detection, and parameter type recovery --- comparing it against state-of-the-art tools, including IDA, Ghidra, Binary Ninja, and TYGR. Experimental results show that CDA outperforms baseline tools across multiple architectures (x64, x86, AArch64, Arm, and Mips) and optimization levels (O0-O3), highlighting its robustness and reliability in diverse compilation environments.",
    "status": "done"
  },
  {
    "id": 1741,
    "year": 2025,
    "title": "Dangers Behind Access Control: Understanding and Exploiting Implicit Permissions in Kubernetes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765106",
    "abstract": "As the de-facto standard for container orchestration, Kubernetes is extensively adopted by numerous companies and cloud vendors, making its security critical. In this paper, we define a new attack surface called implicit permission: The execution of explicitly granted permissions in Kubernetes dynamically leads to implicit operations on other resources, enabling new permissions beyond the explicitly granted ones. Such implicit permissions create security vulnerabilities that attackers can exploit to compromise an entire cluster. Automatically identifying implicit permissions is challenging due to implicit relation reasoning and dynamic behaviors across diverse components of Kubernetes. To address that, we devise a systematic approach that combines static analysis techniques with the advanced capabilities of the large language model (LLM, e.g., GPT-4.5). Initially, we develop a static analysis to identify all Kubernetes resources. Building on this, we use static analysis to identify all explicit permissions for each resource. Finally, by combining the semantic reasoning capabilities of LLMs with the pattern-based precision of static analysis, we reason about what explicit permissions may dynamically lead to implicit permissions through complex interactions and uncover 593 implicit permissions derived from explicit permissions. We use the implicit permission references as insights to identify potential risks of CNCF projects and applications provided by the top four cloud vendors. With responsible disclosure, we obtain five new CVEs, six acknowledgments of cloud vendors, and a bounty awarded by Google. These acknowledgments underlie the practical impact of our attack.",
    "status": "done"
  },
  {
    "id": 1742,
    "year": 2025,
    "title": "Tide: An Efficient Kernel-level Isolation Execution Environment on AArch64 via Dynamically Adjusting Output Address Size",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765111",
    "abstract": "To enforce the privilege separation in the kernel, kernel-level isolated execution environment (IEE) has become a recent research trend because it can protect critical resources and monitors. Our research found that to isolate the IEE memory, all existing IEEs must act as a reference monitor to isolate page tables and validate their updates, bringing a significant performance overhead. Hence, we propose Tide, a new kernel-level IEE based on the output address size hardware feature on AArch64, which could offload such checks to the hardware. However, it still faces the flexibility and security challenges. To address them, Tide presents using the stage-2 translation to expand the physical address range to flexibly map the IEE memory and perform extra access controls on the physical memory; it designs a novel gate to enter (sneak) into the IEE securely by disabling translation temporarily, and ensures it can only be executed at the fixed locations. The experimental results show that Tide is performant than all existing IEEs on protecting critical kernel structures and security tools.",
    "status": "done"
  },
  {
    "id": 1743,
    "year": 2025,
    "title": "Accountable Liveness",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765032",
    "abstract": "Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the x-partially-synchronous network model in which, intuitively, at most an x fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown ''global stablization time''). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if x &lt; 1/2 and undefined &lt; n/2, where n denotes the number of nodes and undefined the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of x and undefined). Our results provide rigorous foundations for liveness-accountability heuristics such as the ''inactivity leaks'' employed in Ethereum.",
    "status": "done"
  },
  {
    "id": 1744,
    "year": 2025,
    "title": "How to Beat Nakamoto in the Race",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765058",
    "abstract": "This paper studies proof-of-work Nakamoto consensus protocols under bounded network delays, settling two long-standing questions in blockchain security: What is the most effective attack on block safety under a given block confirmation latency? And what is the resulting probability of safety violation? A Markov decision process (MDP) framework is introduced to precisely characterize the system state (including the blocktree and timings of all blocks mined), the adversary's potential actions, and the state transitions due to the adversarial action and the random block arrival processes. An optimal attack, called bait-and-switch, is proposed and proved to maximize the adversary's chance of violating block safety by ''beating Nakamoto in the race''. The exact probability of this violation is calculated for any given confirmation depth using Markov chain analysis, offering fresh insights into the interplay of network delay, confirmation rules, and blockchain security.",
    "status": "done"
  },
  {
    "id": 1745,
    "year": 2025,
    "title": "Committee Selection with Non-Proportional Weights",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765074",
    "abstract": "Committees are extensively used in the designs of various Proof-of-Stake (PoS) blockchains. A committee is simply a randomly selected subset of the parties/nodes in the system. Ideally, the committee should i) be as small as possible, and ii) properly represent the entire system, in terms of the corruption ratio. Existing committee selection schemes all follow the principle of proportionality, which says that a committee member should neither over-represent nor under-represent the stake it holds. In this work, highly surprisingly, we discover that proportionality actually leads to sub-optimal designs. Namely, better security and smaller committee size can be achieved when parties over-represent/under-represent their stakes. We then explore such non-proportional designs, and show that they can help to reduce error by many orders of magnitude, under realistic settings and real-world stake distributions of 6 major cryptocurrencies.",
    "status": "done"
  },
  {
    "id": 1746,
    "year": 2025,
    "title": "Elastic Restaking Networks: United we fall, (partially) divided we stand",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765077",
    "abstract": "Many blockchain-based decentralized services require their validators (operators) to deposit stake (collateral), which is forfeited (slashed) if they misbehave. Restaking networks let validators secure multiple services by reusing stake. These networks have quickly gained traction, leveraging over 20 billion in stake. However, restaking introduces a new attack vector where validators can coordinate to misbehave across multiple services simultaneously, extracting digital assets while forfeiting their stake only once. Previous work focused either on preventing coordinated misbehavior or on protecting services if all other services are Byzantine and might unjustly cause slashing due to bugs or malice. The first model overlooks how a single Byzantine service can collapse the network, while the second ignores shared-stake benefits. To bridge the gap, we analyze the system as a strategic game of coordinated misbehavior, when a given fraction of the services are Byzantine. We introduce elastic restaking networks, where validators can allocate portions of their stake that may cumulatively exceed their total stake, and when allocations are lost, the remaining stake stretches to cover remaining allocations. We show that elastic networks exhibit superior robustness compared to previous approaches, and demonstrate a synergistic effect where an elastic restaking network enhances its blockchain's security, contrary to community concerns of an opposite effect in existing networks. We then design incentives for tuning validators' allocations. Our elastic restaking system and incentive design have immediate practical implications for deployed restaking networks.",
    "status": "done"
  },
  {
    "id": 1747,
    "year": 2025,
    "title": "Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765082",
    "abstract": "Remote Procedure Call (RPC) services have become a primary gateway for users to access public blockchains. While they offer significant convenience, RPC services also introduce critical privacy challenges that remain insufficiently examined. Existing deanonymization attacks either do not apply to blockchain RPC users or incur costs like transaction fees assuming an active network eavesdropper. In this paper, we propose a novel deanonymization attack that can link an IP address of a RPC user to this user's blockchain pseudonym. Our analysis reveals a temporal correlation between the timestamps of transaction confirmations recorded on the public ledger and those of TCP packets sent by the victim when querying transaction status. We assume a strong passive adversary with access to network infrastructure, capable of monitoring traffic at network border routers or Internet exchange points. By monitoring network traffic and analyzing public ledgers, the attacker can link the IP address of the TCP packet to the pseudonym of the transaction initiator by exploiting the temporal correlation. This deanonymization attack incurs zero transaction fee. We mathematically model and analyze the attack method, perform large-scale measurements of blockchain ledgers, and conduct real-world attacks to validate the attack. Our attack achieves a high success rate of over 95\\% against normal RPC users on various blockchain networks, including Ethereum, Bitcoin and Solana.",
    "status": "done"
  },
  {
    "id": 1748,
    "year": 2025,
    "title": "Breaking Omert\\`{a}: On Threshold Cryptography, Smart Collusion, and Whistleblowing",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765087",
    "abstract": "Cryptographic protocols often make honesty assumptions---e.g., fewer than t out of n participants are adversarial. In practice, these assumptions can be hard to ensure, particularly given monetary incentives for participants to collude and deviate from the protocol.In this work, we explore combining techniques from cryptography and mechanism design to discourage collusion. We formalize protocols in which colluders submit a cryptographic proof to whistleblow against their co-conspirators, revealing the dishonest behavior publicly. We provide general results on the cryptographic feasibility, and show how whistleblowing fits a number of applications including secret sharing, randomness beacons, and anonymous credentials.We also introduce smart collusion --- a new model for players to collude. Analogous to blockchain smart contracts, smart collusion allows colluding parties to arbitrarily coordinate and impose penalties on defectors (e.g., those that blow the whistle). We show that unconditional security is impossible against smart colluders even when whistleblowing is anonymous and can identify all colluding players. On the positive side, we construct a whistleblowing protocol that requires only a small deposit and can protect against smart collusion even with roughly t times larger deposit.",
    "status": "done"
  },
  {
    "id": 1749,
    "year": 2025,
    "title": "Harnessing Vital Sign Vibration Harmonics for Effortless and Inbuilt XR User Authentication",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765060",
    "abstract": "Extended Reality (XR) headsets are increasingly serving as repositories for substantial volumes of sensitive data and gateways to web applications. This transition highlights the need for convenient and secure user authentication solutions. Traditional password/PIN-based schemes are ill-suited to the XR's gesture- and voice-based interfaces and are prone to shoulder-surfing attacks. Some recent XR systems incorporate two-factor authentication, but it requires additional operations on a second device (e.g., a smartphone or wearable). In this work, we introduce the first effortless and inbuilt XR user authentication system by leveraging the harmonics of vibrations excited by users' vital signs. The system is transparent to users (no efforts during enrollment and authentication) and requires no additional hardware. The key idea is that vital signs (i.e., breathing and heart beating) naturally generate low-frequency mechanical vibrations, causing human skull to vibrate and produces harmonic signals. When the harmonics pass the human head, they carry rich biometrics associated with the wearer's skull structure and soft tissues, which can be captured by the XR motion sensors. Instead of directly utilizing the vibrations, we extract more reliable biometrics from the ratios among different harmonic frequencies, which capture wearers' unique head and facial attenuation properties and are non-volatile when the periodicity and amplitude of vital signs fluctuate. We further design an adaptive filter to mitigate the body motion distortions in common XR interactions. By adopting advanced deep learning models with the attention mechanism, our system realizes effective and robust authentication across XR scenarios. Evaluations across 10 months, with 52 users and two popular XR headsets, show that our system can accurately authenticate users with over 95\\% true positive rates and rejects unauthorized users with over 98\\% true negative rates under various XR scenarios, with biometrics remaining consistent over long-term periods.",
    "status": "done"
  },
  {
    "id": 1750,
    "year": 2025,
    "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765064",
    "abstract": "Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87\\% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6\\%, significantly outperforming all baseline defenses.",
    "status": "done"
  },
  {
    "id": 1751,
    "year": 2025,
    "title": "Sentry: Authenticating Machine Learning Artifacts on the Fly",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765070",
    "abstract": "Machine learning systems increasingly rely on open-source artifacts such as datasets and models that are created or hosted by other parties. The reliance on external datasets and pre-trained models exposes the system to supply chain attacks where an artifact can be poisoned before it is delivered to the end-user. Such attacks are possible due to the lack of any authenticity verification in existing machine learning systems. Incorporating cryptographic solutions such as hashing and signing can mitigate the risk of supply chain attacks. However, existing frameworks for integrity verification based on cryptographic techniques can incur significant overhead when applied to state-of-the-art machine learning artifacts due to their scale, and are not compatible with GPU platforms. In this paper, we develop Sentry, a novel GPU-based framework that verifies the authenticity of machine learning artifacts by implementing cryptographic signing and verification for datasets and models. Sentry ties developer identities to signatures and performs authentication on the fly as artifacts are loaded on GPU memory, making it compatible with GPU data movement solutions such as NVIDIA GPUDirect that bypass the CPU. Sentry incorporates GPU acceleration of cryptographic hash constructions such as Merkle tree and lattice hashing, implementing memory optimizations and resource partitioning schemes for a high throughput performance. Our evaluations show that Sentry is a practical solution to bring authenticity to machine learning systems, achieving orders of magnitude speedup over a CPU-based baseline.",
    "status": "done"
  },
  {
    "id": 1752,
    "year": 2025,
    "title": "Training Robust Classifiers for Classifying Encrypted Traffic under Dynamic Network Conditions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765073",
    "abstract": "Most existing DL-based encrypted traffic classification methods suffer performance degradation in real-world deployments due to dynamic network conditions, e.g., network environment changes and traffic obfuscation. Dynamic network conditions cause encrypted traffic to exhibit distinct feature patterns during training and testing phases. To address this issue, we propose MetaTraffic, a novel and general DL training framework built upon meta-learning that enhances the performance of supervised DL models designed for encrypted traffic classification against dynamic network conditions. Our key observation is that the traffic of the same network behaviors share the same semantic features even under different network conditions, which can be considered as stable feature representations. Therefore, MetaTraffic helps DL models learn stable feature representations by minimizing the discrepancies in how the models represent traffic features under different network conditions, thereby achieving robust classification under dynamic network conditions. We implement MetaTraffic based on meta-learning with three innovative facilitate modules to enhance its performance. We evaluate MetaTraffic using three public datasets and three new large-scale encrypted traffic datasets that cover multiple types of network conditions. Experimental results show that, under dynamic multiple types of network conditions, our framework improves the accuracy of DL models by 8.94\\% and the F1-Macro score by 12.55\\%, while existing robust training methods decrease the accuracy by 28.85\\% and the F1-Macro score by 33.52\\%.",
    "status": "done"
  },
  {
    "id": 1753,
    "year": 2025,
    "title": "Adversarial Observations in Weather Forecasting",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765076",
    "abstract": "AI-based systems, such as Google's GenCast, have recently redefined the state of the art in weather forecasting, offering more accurate and timely predictions of both everyday weather and extreme events. While these systems are on the verge of replacing traditional meteorological methods, they also introduce new vulnerabilities into the forecasting process. In this paper, we investigate this threat and present a novel attack on autoregressive diffusion models, such as those used in GenCast, capable of manipulating weather forecasts and fabricating extreme events, including hurricanes, heat waves, and intense rainfall. The attack introduces subtle perturbations into weather observations that are statistically indistinguishable from natural noise and change less than 0.1\\% of the measurements—comparable to tampering with data from a single meteorological satellite. As modern forecasting integrates data from nearly one hundred satellites and many other sources operated by different countries, our findings highlight a critical security risk with the potential to cause large-scale disruptions and undermine public trust in weather forecasting.",
    "status": "done"
  },
  {
    "id": 1754,
    "year": 2025,
    "title": "Co-Prime: A Co-design Framework for Privacy Preserving Machine Learning on FPGA",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765090",
    "abstract": "In enormous privacy-sensitive machine learning application domains with collaborative data acquisition from multiple participants, secure multi-party computation (MPC) becomes a promising solution for privacy-preserving machine learning (PPML). Secret sharing protocols is a prevalent MPC strategy, where frequent data distribution and recombination are applied to uphold the confidentiality of participants' data. A key challenge for practical deployment of secret sharing protocols in PPML is the massive and unbalanced computation and communication workloads occurred in various linear and non-linear stages of machine learning. The imbalance could be further amplified when powerful hardware accelerators are designed to reduce the computation latency. In this work, we propose Co-Prime, an FPGA-based 3PC framework for efficient PPML without assistance from a secure third party. Co-Prime integrates protocol and hardware co-optimizations to mitigate the communication bottlenecks in secret sharing schemes. Particularly, Co-Prime proposes a novel protocol conversion technique that seamlessly converts data formats to adaptively adopt preferred protocols in various stages of PPML. Accelerator-friendly MPC primitives and system-level design space exploration schemes are designed to achieve latency hiding through overlapping computation and network communication. Finally, it enables direct interaction with data streams via network communication modules on FPGAs to further reduce the network communication overhead. Experimental results demonstrate significant performance improvements over existing privacy-preserving machine learning frameworks, with 2-18x speedup in inference latency across various LAN/WAN environments and neural network models.",
    "status": "done"
  },
  {
    "id": 1755,
    "year": 2025,
    "title": "Fuzzy Extractors are Practical: Cryptographic Strength Key Derivation from the Iris",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765098",
    "abstract": "Despite decades of effort, a persistent chasm has existed between the theory and practice of device-level biometric authentication. Theoretical constructions can, in principle, provide biometric authentication with cryptographically secure public enrollment data. However, concrete implementations of these techniques have failed to provide security with real-world parameters. The result is that deployed authentication algorithms rely on data that overtly leaks private information about the biometric; thus systems rely on externalized security measures such as trusted execution environments.We close this chasm. We introduce a key derivation system with 105 bits of entropy and a 92\\% true accept rate (TAR) for the iris. Our system advances 1) the feature extraction from the iris and 2) the fuzzy extractor used to derive keys. The fuzzy extractor builds on sample-then-lock (Canetti et al., Journal of Cryptology 2021). We (1) Introduce a new sampling method with a better trade-off between TAR and entropy when features have different quality,(2) Correct Canetti et al.'s main security proof, showing the minimum of min-entropy over subsets is the relevant security measure, and(3) Tighten Canetti et al.'s concrete analysis, nearly doubling security under reasonable assumptions. Our final feature extractor incorporates ideas from the new sampling method to produce features optimized for the sample-then-lock construction.The only statistical assumption needed to show security of our system is necessary: the accuracy of min-entropy estimation.At 105 bits, our quantitative level of security is well above prior work. Simhadri et al. (ISC, 2019) report 32 bits on the iris, but they have a bug in their analysis that reduces their strength. Zhang et al.'s (ePrint 2021/1559) system achieves 45 bits on the face but assumes independence between biometrics and the used error-correcting code, an assumption that cannot be easily verified.",
    "status": "done"
  },
  {
    "id": 1756,
    "year": 2025,
    "title": "May the Force Not Be With You: Brute-Force Resistant Biometric Authentication and Key Reconstruction",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744886",
    "abstract": "The use of biometric-based security protocols is on the steep rise. As biometrics become more popular, we witness more attacks. For example, recent BrutePrint/InfinityGauntlet attacks showed how to brute-force fingerprints stored on an Android phone in about 40 minutes. The attacks are possible because biometrics, like passwords, do not have high entropy. But unlike passwords, brute-force attacks are much more damaging for biometrics, because one cannot easily change biometrics in case of compromise. In this work, we propose a novel provably secure Brute-Force Resistant Biometrics (BFRB) protocol for biometric-based authentication and key reconstruction that protects against brute-force attacks even when the server storing biometric-related data is compromised. Our protocol utilizes a verifiable partially oblivious pseudorandom function, an authenticated encryption scheme, a pseudorandom function, and a hash. We formally define security for a BFRB protocol and reduce the security of our protocol to the security of the building blocks. We implement the protocol and study its performance for the ND-0405 iris dataset.",
    "status": "done"
  },
  {
    "id": 1757,
    "year": 2025,
    "title": "Committed Vector Oblivious Linear Evaluation and Its Applications",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744887",
    "abstract": "We introduce the notion of committed vector oblivious linear evaluation (C-VOLE), which allows a party holding a pre-committed vector to generate VOLE correlations with multiple parties on the committed value. It is a unifying tool that can be found useful in zero-knowledge proofs (ZKPs) of committed values, actively secure multi-party computation, private set intersection (PSI), etc. To achieve the best efficiency, we design a tailored commitment scheme and matching C-VOLE protocols, both based on the learning parity with noise assumption. In particular, exploiting the structures of the carefully designed LPN-based commitment minimizes the cost of ensuring consistency between the committed vector and VOLE correlation. As a result, we achieve an 28\\texttimes{} improvement over the protocol proposed in prior work (Usenix 2021) that uses ZKP to prove the correct opening of the commitment. We also apply C-VOLE to design a PSI protocol that allows one server to run PSI repeatedly with multiple clients while ensuring that the same set is used across all executions. Compared with the state-of-the-art PSI (CCS 2024) with similar security requirements, our protocol reduces the communication overhead by a factor of 35\\texttimes{}.",
    "status": "done"
  },
  {
    "id": 1758,
    "year": 2025,
    "title": "Lodia: Towards Optimal Sparse Matrix-Vector Multiplication for Batched Fully Homomorphic Encryption",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765025",
    "abstract": "Encrypted matrix-vector multiplication is a fundamental component of a variety of applications that involve data privacy concerns. Current algorithms utilizing fully homomorphic encryption (FHE) generally use batching to enhance computational efficiency while neglecting the sparsity of the matrices, a characteristic that exists naturally in many practical situations. Alternatively, porting plaintext algorithms that skip zero elements to address sparsity may fail to utilize batching and introduce additional privacy concerns.We propose Lodia, an efficient outsourced sparse matrix-vector multiplication (SpMV) algorithm for batched FHE schemes without sacrificing privacy. It only requires Θ((n+m)log(n+m)/s) FHE operations, where n is the number of rows/columns, m is the number of non-zero elements of the matrix, and s is the batch size of the FHE scheme. This is optimal for m=Ω(n) and m=O(nρ) for some ρ&lt; 2 (i.e., an≤m≤bnρ asymptotically), covering most practical cases. To our knowledge, no method has been published with better than Θ(n2/s) FHE operations, suitable for any sparse matrix, and without privacy concerns.Lodia utilizes a novel low-diagonal decomposition, which decomposes a sparse matrix into a series of special matrices named low-diagonal matrices. Based on a conventional method encoding the matrix in diagonal order, each low-diagonal matrix can be efficiently multiplied by a vector. This results in an efficient SpMV method suitable for any sparse matrix. Experiments show that Lodia practically achieves a speedup of up to 96\\texttimes{} compared to baselines that ignore matrix sparsity, and up to 3.6\\texttimes{} compared to implementations even with fewer security guarantees. This is the first SpMV solution on encrypted data that can process a substantial matrix with over 8 million rows/columns and 125 million non-zero elements.",
    "status": "done"
  },
  {
    "id": 1759,
    "year": 2025,
    "title": "Separating Broadcast from Cheater Identification",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765041",
    "abstract": "Secure Multiparty Computation (MPC) protocols that achieve Identifiable Abort (IA) guarantee honest parties that if they are denied output, they will be notified of the identity of at least one corrupt party. Cheater identification provides recourse in the event of a protocol failure, and in some settings---such as key management---can even be desired over Guaranteed Output Delivery. However, unlike the weaker security with abort setting, IA protocols make integral use of a broadcast channel. In this work, we call attention to the fact that instantiating the broadcast channel itself---commonly overlooked in prior works on IA---may be the most complex and expensive component in deployments. For instance in ECDSA key management, broadcast would clearly dominate the cost of the secure computation (i.e. threshold signing). We therefore initiate a deeper investigation into the relationship between cheater identification and broadcast. As prior work has shown that the traditional notion of IA implies broadcast, we show that this connection can be circumvented: we allow honest parties to differ in which cheaters they identify, however with the ability to prove claims of cheating to any external auditor. We construct an honest majority threshold ECDSA signing protocol that offers our new notion of Provable Identifiable Selective Abort (PISA) without a traditional broadcast channel. This enables an efficient and easily deployable cheater identification mechanism for distributed key management. Our benchmarks show that with a signing threshold t=10, the computational burden of the worst case execution path is under 500ms on standard hardware. Furthermore, we generalize our methodology: we show that any MPC protocol that achieves IA with r broadcasts can be compiled to one that achieves PISA with 2(r+1) point to point rounds.",
    "status": "done"
  },
  {
    "id": 1760,
    "year": 2025,
    "title": "Correlation-Aware Secure Sorting and Permutation for Iterative Two-Party Graph Analysis",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765050",
    "abstract": "Secure multi-party computation techniques enable in-depth analysis on joint graphs that inherently encompass comprehensive topology information and extended attributes, while preserving data privacy. In the two-party setting, existing approaches suffer from inefficiencies due to redundant secure sorting or costly secure shuffling operations required for secure message passing. Some works improve efficiency by relaxing security assumptions, either through differential privacy or by introducing helper parties.We present PETAL, a high-performance parallel framework for private iterative two-party graph analysis based on garbled circuits, ensuring simulation-based security under the semi-honest adversary assumption. Exploiting the iterative nature of graph analysis, we propose a novel two-party secret permutation protocol to replace secret sorting for data reordering after the first iteration, significantly improving performance. The protocol leverages correlations between permutations across iterations to reuse intermediate results, achieving linear communication after an initial setup. For the initial reordering, we design a two-party secret sorting protocol with low asymptotic complexity and small constant factors. On 2M-sized graphs, evaluations show PETAL achieves on average a 4.6\\texttimes{} speedup and 55\\% less communication across four real-world applications, over the advanced version of the GraphSC framework by Araki et al.",
    "status": "done"
  },
  {
    "id": 1761,
    "year": 2025,
    "title": "Timing Attacks on Differential Privacy are Practical",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765146",
    "abstract": "Differential privacy (DP) has become a standard approach for computing privacy-preserving statistics. However, in interactive settings, the observable runtime of DP queries can inadvertently leak sensitive information, violating privacy guarantees. Prior work has shown that timing side channels can undermine DP in specific settings. In this work, we show that popular libraries for implementing differential privacy, including diffprivlib, OpenDP, and PyDP, frequently introduce such timing side channels, leading to measurable privacy degradation. Our analysis reveals timing vulnerabilities not only within commonly used DP mechanisms (e.g., private sums, counts, means, and selection) but also in commonly used pre-processing steps such as filtering and sorting. We show that these seemingly innocuous operations frequently exhibit runtimes that are sensitive not only to the presence of an individual's data in the input but also to the ordering of the input data.Several of the discovered timing side channels arise from programs whose runtimes depend on the size of the input dataset. The distinction between whether the dataset size is considered private or public information corresponds to bounded versus unbounded DP. We show that mechanisms satisfying unbounded DP with respect to their output distributions often trivially reveal their input size through their runtime distributions. We give several examples of practical attacks that can be used to re-identify individuals in a dataset given such a timing side channel. Finally, we propose an empirical auditing technique for detecting timing side-channel vulnerabilities in DP implementations. Our auditing algorithm provides a lower bound on privacy loss when both the program's output and runtime are observable to an adversary. Using our auditing framework, we are able to quantify conservative bounds on the privacy leakage of these mechanisms when runtimes are observable to an adversary.",
    "status": "done"
  },
  {
    "id": 1762,
    "year": 2025,
    "title": "SlicedPIR: Offloading Heavyweight Work with NTT",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765154",
    "abstract": "We present SlicedPIR, a distributed Private Information Retrieval (PIR) protocol. SlicedPIR efficiently alleviates the server's compute bottleneck by offloading its load across multiple untrusted client machines. In contrast to prior work, SlicedPIR induces only a modest network overhead when the server offloads its work. It achieves those communication savings by exploiting the polynomial encoding of homomorphic encryption schemes typically used in PIR protocols. This encoding lets the server make novel use of the Number Theoretic Transform (NTT) to distribute points on the polynomials as ''slices'' of its data rather than the polynomials themselves. Using NTT allows the clients to process recursive PIR queries on their slices and return a succinct result to the server. The server efficiently verifies the clients' results by leveraging the Schwartz-Zippel lemma, which we adapt to the PIR use case. We show how to integrate SlicedPIR into a private messaging system, where clients write messages to the server's database and then use PIR to secretly query for messages from their friends. We implement a prototype of SlicedPIR and run experiments to show that it scales well with the number of clients and database size. Concretely, SlicedPIR achieves better performance and cuts network usage by over 95\\% compared to the state-of-the-art.",
    "status": "done"
  },
  {
    "id": 1763,
    "year": 2025,
    "title": "Byte by Byte: Unmasking Browser Fingerprinting at the Function Level using V8 Bytecode Transformers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765158",
    "abstract": "Browser fingerprinting enables persistent cross-site user tracking via subtle techniques that often evade conventional defenses or cause website breakage when script-level blocking countermeasures are applied. Addressing these challenges requires detection methods offering both function-level precision to minimize breakage and inherent robustness against code obfuscation and URL manipulation.We introduce ByteDefender, the first system leveraging V8 engine bytecode to detect fingerprinting operations specifically at the JavaScript function level. A Transformer-based classifier, trained offline on bytecode sequences, accurately identifies functions exhibiting fingerprinting behavior. We develop and evaluate lightweight signatures derived from this model to enable low-overhead, on-device matching against function bytecode during compilation but prior to execution, which only adds a 4\\% (average) latency to the page load time. This mechanism facilitates targeted, real-time prevention of fingerprinting function execution, thereby preserving legitimate script functionality. Operating directly on bytecode ensures inherent resilience against common code obfuscation and URL-based evasion. Our evaluation on the top 100k websites demonstrates high detection accuracy at both function- and script-level, with substantial improvements over state-of-the-art AST-based methods, particularly in robustness against obfuscation. ByteDefender offers a practical framework for effective, precise, and robust fingerprinting mitigation.",
    "status": "done"
  },
  {
    "id": 1764,
    "year": 2025,
    "title": "Optimal Mechanisms for Quantum Local Differential Privacy",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765178",
    "abstract": "Centralized differential privacy has been successfully applied to quantum computing and information processing to protect privacy and avoid leaks in the connections between neighboring quantum states. Consequently, quantum local differential privacy (QLDP) has been newly proposed to preserve quantum data privacy akin to the classical scenario where all states are viewed as neighboring states. However, the exploration of the QLDP framework is still in its early stages, primarily conceptual, which poses challenges for its practical implementation in safeguarding quantum state privacy. This paper delves into optimal QLDP mechanisms to balance privacy and utility to enhance the practical use of the QLDP framework. QLDP utilizes a parameter ε to manage privacy leaks and ensure the privacy of individual quantum states. The optimization of the QLDP value ε, denoted as ε^*, for any quantum mechanism is addressed as an optimization problem. The introduction of quantum noise is shown to provide privacy protections similar to classical scenarios, with quantum depolarizing noise identified as the optimal unital privatization mechanism within the QLDP framework. Unital mechanisms represent a diverse set of quantum mechanisms that encompass frequently employed quantum noise types. Quantum depolarizing noise optimizes both fidelity and trace distance utilities, which are crucial metrics in the field of quantum computation and information, and can be viewed as a quantum counterpart to classical randomized response methods. The study further explores the trade-off between utility and privacy across different quantum noise mechanisms, including unital and non-unital quantum noise mechanisms, through both analytical and numerically experimental approaches. This highlights the optimization of quantum depolarizing noise in the QLDP framework.",
    "status": "done"
  },
  {
    "id": 1765,
    "year": 2025,
    "title": "Competing for Attention: An Interview Study with Participants of Cryptography Competitions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765201",
    "abstract": "Cryptography competitions often contribute to the development and standardization of new cryptography. They help select primitives and algorithms that solve specific cryptographic problems securely and efficiently from a list of candidate submissions. Over the last decades, several competitions held by NIST and other research and regulatory organizations resulted in standards for, e.g., symmetric and asymmetric encryption, hashing, digital signatures, and, most recently, quantum secure cryptography. However, while these competitions fostered much technical research on the submitted schemes, little is currently known about the human aspects of cryptography competition processes, how they shape the competition results, and their perceived impact on cryptography security. To investigate human aspects of cryptography competitions, we interviewed 20 experienced cryptography competition participants on their experiences, their assessment of the competitions' impact and what it depended on, and suggestions to improve future competitions. We find that competitions bring attention to a cryptography area, providing research focus and motivation and establishing trust in schemes through community scrutiny and collaboration. Our participants highlighted the criticality of transparency, fairness, and trustworthiness of the competition organizer, emphasizing a need for clear and open communication. Based on these findings, we suggest strategies for future competitions to maximize engagement and provide transparent, trustworthy processes and results. We recommend stronger moderation of social conduct on official channels to ensure fairness and prevent putting off potential contributors. We also find substantial involvement and feedback collection from industry is critical. Transparent organization and evaluation elevate the competition and foster secure and well-adopted standards.",
    "status": "done"
  },
  {
    "id": 1766,
    "year": 2025,
    "title": "THOR: Secure Transformer Inference with Homomorphic Encryption",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765150",
    "abstract": "As large language models are increasingly deployed in cloud environments, privacy concerns have become a significant issue. To address this challenge, we present THOR, a non-interactive framework for secure transformer inference using homomorphic encryption. We first propose efficient matrix multiplication algorithms based on diagonal-major encoding and compact ciphertext packing. We extend these basic algorithms to support plaintext-ciphertext matrix multiplication (PC-MM) using parallel submatrix computation and ciphertext-ciphertext multiplication (CC-MM) with a baby-step giant-step strategy. We also design efficient evaluation strategies for non-linear functions such as softmax, LayerNorm, GELU, and Tanh, by integrating advanced approximation techniques with adaptive iterative methods. Our matrix multiplication algorithms outperform state-of-the-art methods, achieving up to 5.3X speedup in PC-MM for ℝ 768 X 768 X ℝ768X128 over BOLT (Pang et al., IEEE S&amp;P 2024) and 9.7X in CC-MM for 12X (ℝ64X128 X ℝ128X128) over Powerformer (Park et al., Preprint). THOR enables secure inference on the BERT-base model with 128 tokens in 10 minutes on a single GPU, while maintaining comparable accuracy on GLUE tasks.",
    "status": "done"
  },
  {
    "id": 1767,
    "year": 2025,
    "title": "Optimistic, Signature-Free Reliable Broadcast and Its Applications",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765220",
    "abstract": "Reliable broadcast (RBC) is a key primitive in fault-tolerant distributed systems, and improving its efficiency can benefit a wide range of applications. This work focuses on signature-free RBC protocols, which are particularly attractive due to their computational efficiency. Existing protocols in this setting incur an optimal 3 steps to reach a decision while tolerating up to undefined &lt; n/3 Byzantine faults, where n is the number of parties. In this work, we propose an optimistic RBC protocol that maintains the undefined &lt; n/3 fault tolerance but achieves termination in just 2 steps under certain optimistic conditions—when at least ⌉n+2 undefined-2 over -2 ⌈ non-broadcaster parties behave honestly. We also prove a matching lower bound on the number of honest parties required for 2-step termination.We show that our latency-reduction technique generalizes beyond RBC and applies to other primitives such as asynchronous verifiable secret sharing (AVSS) and asynchronous verifiable information dispersal (AVID), enabling them to complete in 2 steps under similar optimistic conditions.To highlight the practical impact of our RBC protocol, we integrate it into a new signature-free, post-quantum secure DAG-based Byzantine fault-tolerant (BFT) consensus protocol. Under optimistic conditions, this protocol achieves a commit latency of 3 steps—matching the performance of the best signature-based protocols. Our experimental evaluation shows that our protocol significantly outperforms existing post-quantum secure and signature-based protocols, even on machines with limited CPU resources. In contrast, signature-based protocols require high CPU capacity to achieve comparable performance.",
    "status": "done"
  },
  {
    "id": 1768,
    "year": 2025,
    "title": "GhostCache: Timer- and Counter-Free Cache Attacks Exploiting Weak Coherence on RISC-V and ARM Chips",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744833",
    "abstract": "Microarchitectural side-channel attacks, which have become increasingly prevalent, often rely on high-resolution timers. Emerging processor architectures have sought to mitigate these vulnerabilities by restricting access to fine-grained timers. In this work, we verify the widespread existence of weak coherence in L1 cache on multiple RISC chips, exploit it to bypass this type of mitigation and propose GhostCache, which constructs timer-free and counter-free instruction cache attacks. It introduces two novel and widely applied attack primitives, Modify+Recall and Call+ModifyCall, which are applicable to both RISC-V and ARM architectures and affect 6 commercial and 3 open-source large RISC processors. To the best of our knowledge, we present the first demonstration of timer-free and counter-free cache attacks on RISC-V processors. We also identify undisclosed features, such as the next-three-line prefetching mechanism and direct forwarding of evicted instructions from data cache to instruction cache. Furthermore, we develop four types of covert channels, achieving up to 1.68 MB/s with a 0.01\\% error rate. For side-channel attacks, GhostCache enables three types of timer-free real-world attacks. The first is an end-to-end website fingerprinting attack, achieving 92.02\\% accuracy across 100 website classes. The second is a set of kernel leakage attacks, including the discovery of a new Spectre disclosure gadget via a function pointer to leak arbitrary kernel data at 92.91\\% accuracy. We also launched an attack to reconstruct cryptographic keys. Lastly, we propose potential countermeasures to address these vulnerabilities in both RISC-V and ARM architectures.",
    "status": "done"
  },
  {
    "id": 1769,
    "year": 2025,
    "title": "Heracles: Chosen Plaintext Attack on AMD SEV-SNP",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765209",
    "abstract": "Confidential computing needs hardware support that stops privileged software from learning secrets of a guest virtual machine. AMD offers such hardware support in the form of SEV-SNP to create confidential virtual machines, such that hardware encrypts all the VM memory. Specifically, SEV-SNP uses the XEX encryption mode with address-dependent tweak values such that the same plaintext at different memory addresses yields different ciphertexts.Heracles makes three observations: the hypervisor can move encrypted guest pages in DRAM using three APIs; when it moves the guest pages to a new DRAM address, pages are re-encrypted; re-encryption is deterministic. By re-encrypting guest data at precisely chosen DRAM locations, we can create a chosen plaintext oracle allowing us to leak guest memory at block granularity. We build four primitives that leverage the victim's access patterns to amplify Heracles's impact to not only leak data at block but at byte granularity. In our case studies, we leak kernel memory, crypto keys, and user passwords, as well as demonstrate web session hijacking.",
    "status": "done"
  },
  {
    "id": 1770,
    "year": 2025,
    "title": "Towards Real-Time Defense against Object-Based LiDAR Attacks in Autonomous Driving",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765227",
    "abstract": "LiDAR (Light Detection and Ranging)-based object detection is a cornerstone of autonomous vehicle perception systems. Modern LiDAR perception relies heavily on deep neural networks (DNNs), which enable accurate object detection by learning geometric features from 3D point clouds. However, recent studies have shown that these systems are vulnerable to object-based adversarial attacks, where physical adversarial objects are strategically placed in the environment to manipulate LiDAR point clouds and mislead detection models. These attacks are practical, stealthy, and require no specialized hardware, posing a serious threat to the safety and reliability of AVs. Despite these risks, existing defense methods suffer from significant limitations, including high computational overhead, limited generalizability and effectiveness, and the inability to operate in real time. In this paper, we propose the first real-time defense mechanism against object-based LiDAR attacks in autonomous driving. Our solution is both detection model-agnostic and attack-agnostic, requiring no prior knowledge of the number, shape, size, or placement of adversarial objects. Positioned between the sensing and perception modules of the AV pipeline, the defense processes LiDAR point clouds in real time and employs a novel generative model that enables efficient and effective identification and removal of adversarial points from suspicious regions. Extensive experiments in both simulated and real-world environments demonstrate that our approach achieves high attack detection rates with minimal latency. This work offers a practical and robust defense solution to a growing security threat in autonomous driving.",
    "status": "done"
  },
  {
    "id": 1771,
    "year": 2025,
    "title": "RMPocalypse: How a Catch-22 Breaks AMD SEV-SNP",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765233",
    "abstract": "AMD SEV-SNP offers confidential computing in form of confidential VMs, such that the untrusted hypervisor cannot tamper with its confidentiality and integrity. SEV-SNP, the latest addition, ensures integrity via the Reverse Map Table (RMP) that stops the hypervisor from tampering guest page mappings. AMD uses RMP entries to protect the rest of the RMP, thus causing a Catch-22 during the RMP setup phase. To address this, SEV-SNP relies on AMD's Platform Security Processor (PSP), that resides next to the x86 cores executing SEV-SNP VMs, to perform the RMP initialization. During initialization, only PSP should be able to alter the RMP memory. All other memory accesses must be fenced, especially from the x86 cores. We present RMPocalypse, a novel attack that shows a critical gap in the security of RMP initialization, wherein the x86 cores maliciously control parts of the initial RMP state. Our analysis shows that the vulnerability arises due to the complex, but insufficient, interplay of multiple hardware components and distributed access controls. To show the impact of our finding, we exploit this gap to break confidentiality and integrity guarantees of SEV-SNP. We demonstrate RMPocalypse by enabling debug on production-mode CVMs, faking attestation, VMSA state replay, and code injection.",
    "status": "done"
  },
  {
    "id": 1772,
    "year": 2025,
    "title": "Dynamic Vulnerability Patching for Heterogeneous Embedded Systems Using Stack Frame Reconstruction",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765200",
    "abstract": "Existing dynamic vulnerability patching techniques are not well-suited for embedded devices, especially mission-critical ones such as medical equipment, as they have limited computational power and memory but uninterrupted service requirements. Those devices often lack sufficient idle memory for dynamic patching, and the diverse architectures of embedded systems further complicate the creation of patch triggers that are compatible across various system kernels and hardware platforms. To address these challenges, we propose a hot patching framework called StackPatch that facilitates patch development based on stack frame reconstruction. StackPatch introduces different triggering strategies to update programs stored in memory units. We leverage the exception-handling mechanisms commonly available in embedded processors to enhance StackPatch's adaptability across different processor architectures for control flow redirection. We evaluated StackPatch on embedded devices featuring three major microcontroller (MCU) architectures: ARM, RISC-V, and Xtensa. In the experiments, we used StackPatch to successfully fix 102 publicly disclosed vulnerabilities in real-time operating systems (RTOSes). We applied patching to medical devices, soft programmable logic controllers (PLCs), and network services, with StackPatch consistently completing each vulnerability remediation in less than 260 MCU clock cycles.",
    "status": "done"
  },
  {
    "id": 1773,
    "year": 2025,
    "title": "Chekhov's Gun: Uncovering Hidden Risks in macOS Application-Sandboxed PID-Domain Services",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765205",
    "abstract": "macOS delegates many high-privilege operations to dedicated PID-domain services, which applications can register and communicate with through inter-process communication (IPC). This architecture improves userland stability and security but also introduces attractive attack surfaces for adversaries. In this paper, we systematically analyze PID-domain services and uncover an overlooked attack vector: PID-domain services that are restricted to an Application Sandbox identical to the calling application can still be exploited due to subtle entitlement differences. To fully understand this attack surface, we reverse-engineered the implementation of the entitlement mechanism for the Application Sandbox in macOS. Based on these insights, we designed an entitlement-guided taint analysis framework to automatically detect vulnerable PID-domain services within this attack surface. Using our detection tool, we identified 19 confirmed zero-day vulnerabilities in the latest macOS 15.2, with 7 assigned CVE identifiers.",
    "status": "done"
  },
  {
    "id": 1774,
    "year": 2025,
    "title": "A System Framework to Symbolically Explore Intel TDX Module Execution",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765212",
    "abstract": "We present TDXplorer, the first dynamic symbolic analysis system for Intel's TDX Module, the software trusted computing base of TDX. Without using TDX hardware, an analyzer function on top of TDXplorer can not only apply dynamic analysis to control and instrument the TDX Module's execution, but also carry out symbolic execution for path exploration as well as security and functionality reasoning. The two types of analysis are seamlessly integrated in a way that symbolic execution is conducted directly upon the TDX Module's binary code and runtime states, which are shaped by using dynamic analysis techniques. We implement TDXplorer on Linux and measure its performance and correctness against executions on a TDX platform. Our case studies on symbolic modeling of secure EPT creation and KeyHole region management demonstrate that TDXplorer is a versatile and capable tool supporting various analysis tasks.",
    "status": "done"
  },
  {
    "id": 1775,
    "year": 2025,
    "title": "Windows plays Jenga: Uncovering Design Weaknesses in Windows File System Security",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765217",
    "abstract": "File systems are essential components of modern operating systems, with Windows being one of the most dominant platforms. Recently, a series of attacks have exploited the Windows file system to trigger serious security threats such as privilege escalation. Over the past several years, dozens of such attacks have been reported and even exploited in the wild. However, Microsoft has consistently addressed these issues with targeted patches rather than fundamental redesigns — resembling a precarious game of Jenga where security measures are stacked upon an unstable foundation. In this paper, we present a five-step comprehensive analysis of the Windows file system's design weaknesses. First, we analyze how Windows differs from another operating system, Linux. Second, we investigated how these discrepancies lead to security vulnerabilities in real-world applications and identified 13 high-impact vulnerabilities, including 11 previously unknown ones. Third, we show that current compatibility layers in modern programming languages fail to handle these discrepancies properly. Specifically, we examined compatibility layers in six programming languages and found 27 non-compliant and 9 inconsistencies, rendering these layers unreliable. Fourth, through a user study involving 21 experienced developers, we found that most were unfamiliar with OS-level file system discrepancies and rarely implemented appropriate mitigations. Finally, we analyze existing countermeasures and discuss their limitations. Our findings reveal critical yet largely obscured security risks resulting from design flaws in the Windows file system. Furthermore, we suggest that Microsoft rethink its strategy and address these fundamental weaknesses.",
    "status": "done"
  },
  {
    "id": 1776,
    "year": 2025,
    "title": "Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765096",
    "abstract": "The DNS infrastructure is infamous for facilitating reflective amplification attacks. Various countermeasures such as server shielding, access control, rate limiting, and protocol restrictions have been implemented. Still, the threat remains throughout the deployment of DNS servers. In this paper, we report on and evaluate the often unnoticed threat that derives from transparent DNS forwarders, a widely deployed, incompletely functional set of DNS components. Transparent DNS forwarders transfer DNS requests without rebuilding packets with correct source addresses. As such, transparent forwarders feed DNS requests into (mainly powerful and anycasted) open recursive resolvers, which thereby can be misused to participate unwillingly in distributed reflective amplification attacks. We show how transparent forwarders raise severe threats to the Internet infrastructure. They easily circumvent rate limiting and achieve an additional, scalable impact via the DNS anycast infrastructure. We empirically verify this scaling behavior up to a factor of 14. Transparent forwarders can also assist in bypassing firewall rules that protect recursive resolvers, making these shielded infrastructure entities part of the global DNS attack surface.",
    "status": "done"
  },
  {
    "id": 1777,
    "year": 2025,
    "title": "Training with Only 1.0 ‰ Samples: Malicious Traffic Detection via Cross-Modality Feature Fusion",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765143",
    "abstract": "Machine Learning (ML) based malicious traffic detection systems can accurately recognize unseen network attacks by learning from large-scale traffic datasets. However, deploying such systems across multiple networks involves substantial efforts to construct large training datasets for each network. This paper addresses the issue of training with minimal datasets, that is, achieving accurate malicious traffic detection by learning a small portion of traffic in entirely new network environments, thereby eliminating prohibitive labor costs associated with traffic dataset construction. We develop tFusion to effectively extract information from limited datasets by treating network traffic data as multimodal data, comprising features from multiple sensory modalities of packets, flows, and hosts. In particular, we design a dedicated crossmodal attention model that fuses fine-grained per-packet sequential features with coarse-grained per-flow and per-host statistical features, to synthesize correlations among the different granularities of traffic features. Moreover, we design a topology-driven contrastive learning approach that pre- trains the models while reducing topology-related biases, which allows tFusion to achieve generic detection across various networks. We deploy tFusion in an institutional network and measure its performance over five days. tFusion requires human experts to label only 1.0 ‰ traffic, yet it achieves 99.82\\% accuracy when detecting various attacks. Meanwhile, it outperforms 14 existing methods by improving over 12.76\\% accuracy on 11 existing datasets.",
    "status": "done"
  },
  {
    "id": 1778,
    "year": 2025,
    "title": "Fingerprinting Deep Packet Inspection Devices by their Ambiguities",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765145",
    "abstract": "Users around the world face escalating network interference such as censorship, throttling, and interception, largely driven by the commoditization and growing availability of Deep Packet Inspection (DPI) devices. Once reserved for a few well-resourced nation-state actors, the ability to interfere with traffic at scale is now within reach of nearly any network operator. Despite this proliferation, our understanding of DPIs and their deployments on the Internet remains limited---being network intermediary leaves DPI unresponsive to conventional host-based scanning tools, and DPI vendors actively obscuring their products further complicates measurement efforts.In this work, we present a remote measurement framework, dMAP (DPI Mapper), that derives behavioral fingerprints for DPIs to differentiate and cluster these otherwise indistinguishable middleboxes at scale, as a first step toward active reconnaissance of DPIs on the Internet. Our key insight is that parsing and interpreting traffic as network intermediaries inherently involves ambiguities---from under-specified protocol behaviors to differing RFC interpretations---forcing DPI vendors into independent implementation choices that create measurable variance among DPIs. Based on differential fuzzing, dMAP systematically discovers, selects, and deploys specialized probes that translate DPI's internal parsing behaviors into externally observable fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its practical feasibility, showing that even a modest set of 20-40 discriminative probes reliably differentiates a wide range of DPI implementations, including major nation-state censorship infrastructures and commercial DPI products. We discuss how our fingerprinting methodology generalizes beyond censorship to other forms of targeted interference, and we hope our work inspires further measurement efforts toward greater visibility and transparency into DPI devices deployed across the global Internet.",
    "status": "done"
  },
  {
    "id": 1779,
    "year": 2025,
    "title": "Don't Look Up: There Are Sensitive Internal Links in the Clear on GEO Satellites",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765198",
    "abstract": "Geosynchronous (GEO) satellite links provide IP backhaul to remote critical infrastructure for utilities, telecom, government, military, and commercial users. To date, academic studies of GEO infrastructure have focused on a handful of satellites and specific use cases. We perform the first broad scan of IP traffic on 39 GEO satellites across 25 distinct longitudes with 411 transponders using consumer-grade equipment. We overcome the poor signal quality plaguing prior work and build the first general parser that can handle the diverse protocols in use by heterogeneous endpoints. We found 50\\% of GEO links contained cleartext IP traffic; while link-layer encryption has been standard practice in satellite TV for decades, IP links typically lacked encryption at both the link and network layers. This gives us a unique view into the internal network security practices of these organizations. We observed unencrypted cellular backhaul traffic from several providers including cleartext call and text contents, job scheduling and industrial control systems for utility infrastructure, military asset tracking, inventory management for global retail stores, and in-flight wifi.",
    "status": "done"
  },
  {
    "id": 1780,
    "year": 2025,
    "title": "Here Comes the AI Worm: Preventing the Propagation of Adversarial Self-Replicating Prompts Within GenAI Ecosystems",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765196",
    "abstract": "In this paper, we show that when the communication between GenAI-powered applications relies on RAG-based inference, an attacker can initiate a computer worm-like chain reaction that we call RAGworm. This is done by crafting an adversarial self-replicating prompt that triggers a cascade of indirect prompt injections within the ecosystem and forces each affected application to perform malicious actions and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of malicious activities intended to promote content, distribute propaganda, and extract confidential user data within a GenAI ecosystem of GenAI-powered email assistants. We demonstrate that RAGworm can trigger the aforementioned malicious activities with a super-linear propagation rate, where each client compromises 20 new clients within the first 1-3 days (depending on the number of emails sent per day). In addition, we analyze how the performance of RAGworm is affected by various factors. Finally, we introduce the DonkeyRail, a guardrail intended to detect and prevent the propagation of RAGworm with minimal latency, high accuracy, and a low false-positive rate. We evaluate the guardrail's performance and show that it yields a true-positive rate of 1.0 with a false-positive rate of 0.017 while adding a negligible latency of 7.6-38.3 ms (depending on the number of documents retrieved). We also show that the guardrail is robust against out-of-distribution worms, consisting of unseen jailbreaking prompts and various worm use cases.",
    "status": "done"
  },
  {
    "id": 1781,
    "year": 2025,
    "title": "Deep Learning from Imperfectly Labeled Malware Data",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765197",
    "abstract": "Deep learning approaches have achieved remarkable performance in malware classification and detection. However, their success relies on the availability of large, accurately labeled datasets: a critical yet challenging requirement in the malware domain. In practice, most malware datasets are automatically labeled using outputs from antivirus engines, a process that often introduces significant label noise. Such imperfections can severely degrade the performance and generalizability of deep learning models. To address this challenge, we introduce SLB, a framework designed to robustly train deep learning–based malware systems while simultaneously refining dataset labels. SLB begins by partitioning the dataset into two subsets: a clean set containing samples with reliable labels, and a noisy set with samples that may be mislabeled, to which pseudo labels are assigned. As training progresses, SLB continuously monitors the model's predictions to dynamically update both sets. Specifically, samples in the noisy set that consistently receive predictions aligning with their (observed or pseudo) labels are promoted to the clean set, whereas samples in the clean set that exhibit unstable predictions are reclassified as noisy. This iterative process not only enhances model performance but also progressively corrects labeling errors. We evaluated SLB on multiple security datasets with both synthetic and real-world label noise across various deep learning architectures and ML algorithms. Experimental results show that SLB significantly improves malware detection performance and reduces overall noise. For example, on the Android binary dataset with 25\\% injected label noise, SLB reduced the noise to below 1.5\\% while increasing the macro F1 score from 74.51\\% to 96.03\\% and the accuracy score from 87.66\\% to 98.68\\%.",
    "status": "done"
  },
  {
    "id": 1782,
    "year": 2025,
    "title": "PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and Verification",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765223",
    "abstract": "With the urgent need to enhance the safety of LLM applications, there has been a growing focus on alignment training algorithms designed to keep large language models (LLMs) behaving in alignment with human values. Alignment training algorithms rely heavily on preference datasets, which are essential for finetuning LLMs to follow human preferences. However, generating and annotating these datasets is often costly and labor-intensive, making it critical to protect their copyright against unauthorized use. In this paper, we propose PreferCare, the first framework tailor-made for preference dataset copyright protection via watermark injection and verification. PreferCare comprises two consecutive stages: injection and verification. In the injection stage, a style transfer-based watermark signal and a bi-level watermark optimization process are designed to embed the watermark into the preference dataset. In the verification stage, we employ statistical tests to determine whether a suspect LLM has used the watermarked preference dataset without authorization. Extensive experiments on multiple popular LLMs have demonstrated that PreferCare achieves effectiveness, harmlessness, transferability, and robustness across diverse settings, and can successfully verify the watermark within 20 queries.",
    "status": "done"
  },
  {
    "id": 1783,
    "year": 2025,
    "title": "SCOPE: Expanding Client-Side Post-Processing for Efficient Privacy-Preserving Model Inference",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765228",
    "abstract": "Privacy-Preserving Inference (PPI) enables users to leverage powerful machine learning models without revealing sensitive input data. However, existing state-of-the-art solutions remain impractical due to significant computation and communication overheads.In this paper, we propose SCOPE (Secure Client-side Operation Expansion), a novel framework that significantly reduces these overheads by expanding the scope of client-side post-processing. We observe that modern HE-based inference schemes inevitably transfer masked intermediate ciphertexts to the client solely for two simple operations—element selection and rearrangement—incurring disproportionately high communication costs. To address this inefficiency, SCOPE introduces a semantic-preserving masking strategy that carefully calibrates the noise magnitude in masked ciphertexts, retaining minimal yet sufficient semantic information. By leveraging the inherent information asymmetry between server and client, this strategy enables clients to perform additional lightweight linear (e.g., normalization) and nonlinear (e.g., ReLU) operations locally, thus substantially reducing both computational and communication overhead, while still effectively protecting sensitive model parameters.Extensive experiments demonstrate that SCOPE achieves 3X speedup and 2.8X communication reduction over state-of-the-art approaches, with minimal accuracy degradation (&lt;2.9\\%). Further evaluations under adaptive attack settings confirm that the controlled semantic exposure introduced by SCOPE does not increase the risk of model extraction attacks. The source code is available at https://anonymous.4open.science/r/scope-ccs25.",
    "status": "done"
  },
  {
    "id": 1784,
    "year": 2025,
    "title": "Phalanx: An FHE-Friendly SNARK for Verifiable Computation on Encrypted Data",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765226",
    "abstract": "Verifiable Computation over encrypted data (VCoed) has two popular paradigms: SNARK-FHE (applying SNARKs to prove FHE operations) and FHE-SNARK (homomorphically evaluating SNARK proofs). For the existing works, FHE-SNARK has a much better efficiency compared to SNARK-FHE.In this work, we follow the line of FHE-SNARK and further improve its efficiency by designing Phalanx—an FHE-friendly SNARK that is: a) 3\\texttimes{} lower multiplicative depth than FRI-based SNARKs; and b) Compatible with FHE SIMD operations.Based on Phalanx, we construct an FHE-SNARK scheme that has: a) 7.3\\texttimes{} ~ 24.4\\texttimes{} speedup: 2.27-hour proof generation for 220- gate circuits on a single core CPU and 0.68-hour when the input ciphertexts are in iNTT form (vs. 16.57 hours in the state-of-the-art); and b) Practical verification: 61.4 MB proofs with 2.8 seconds verification (single core).",
    "status": "done"
  },
  {
    "id": 1785,
    "year": 2025,
    "title": "Practical Zero-Knowledge PIOP for Maliciously Secure Multiparty Homomorphic Encryption",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765229",
    "abstract": "Homomorphic encryption (HE) is a foundational technology in privacy-enhancing cryptography, enabling computation over encrypted data. Recently, generalized HE primitives designed for multi-party applications, such as multi-party HE (MPHE), have garnered significant research interest. While constructing secure multi-party protocols from MPHE in the semi-honest model is straightforward, achieving malicious security remains challenging as it requires zero-knowledge arguments of knowledge (ZKAoKs) for MPHE ciphertexts and public keys.In this work, we design practical ZKAoKs for MPHE that validate the well-formedness of public keys and ciphertexts. Specifically, we develop our ZKAoKs within the polynomial interactive oracle proof (PIOP) framework. To achieve this, we introduce novel optimization techniques that seamlessly integrate constraints for MPHE into the PIOP framework, enabling the design of PIOPs for validating all types of MPHE public keys, including relinearization and automorphism keys. To the best of our knowledge, our construction is the first ZKAoK for MPHE that validates automorphism keys.We instantiate our PIOP using a lattice-based polynomial commitment scheme (PCS). When compared with the previous state-of-the-art construction, PELTA (ACM CCS 2023), our implementation achieves a 5.4x reduction in proof size, a 111x speed-up in proof generation, and a 768x improvement in verification time for validating the encryption key. In addition to the encryption key, we provide benchmark results for all types of ZKAoKs required for MPHE, presenting the first concrete performance results in compiling passively secure MPHE-based protocols into maliciously secure ones.",
    "status": "done"
  },
  {
    "id": 1786,
    "year": 2025,
    "title": "Practical TFHE Ciphertext Sanitization for Oblivious Circuit Evaluation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765231",
    "abstract": "Homomorphic encryption (FHE) enables the computation of arbitrary circuits over encrypted data. A widespread application of HE is oblivious circuit evaluation, where a sender evaluates its private circuit over a receiver's encrypted data, covering scenarios such as oblivious inference and oblivious PRF protocols. However, while the security of HE guarantees the receiver's privacy against the sender, the privacy of the sender's circuit is not solely derived from the security of HE.One effective solution to this problem is ciphertext sanitization, an algorithm that removes any information contained in a ciphertext except for the plaintext. Since its introduction by Ducas and Stehl\\'{e} (Eurocrypt 2016), several approaches have been proposed for constructing sanitization algorithms for TFHE, but they remain highly impractical.In this work, we present a novel sanitization algorithm for TFHE ciphertexts that is practically deployable. Unlike prior methods that introduce randomization throughout the entire bootstrapping procedure or require repeated bootstrappings, our approach involves only two lightweight randomization steps at the input and output of the original TFHE bootstrapping, without modifying its core operations. As a result, our algorithm achieves sanitization with a single bootstrapping and minimal randomization, fully leveraging the fast performance of TFHE bootstrapping.In addition, we design a zero-knowledge argument of knowledge (ZKAoK) for TFHE ciphertexts and bootstrapping keys to address malicious receivers. To the best of our knowledge, this work is the first to construct a concrete ZKAoK that covers all receiver-sent materials, enabling a secure TFHE-based protocol against a malicious receiver.We provide a proof-of-concept implementation to demonstrate the practicality of our solution. Our experiments show that it takes approximately 42.17 ms to sanitize a single TFHE ciphertext, achieving up to a 31\\texttimes{} speedup compared to the state-of-the-art method by Kluczniak (CiC 2025).",
    "status": "done"
  },
  {
    "id": 1787,
    "year": 2025,
    "title": "Multi-Party Private Set Operations from Predicative Zero-Sharing",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765234",
    "abstract": "Typical protocols in the multi-party private set operations (MPSO) setting enable m &gt; 2 parties to perform certain secure computation on the intersection or union of their private sets, realizing a very limited range of MPSO functionalities. Most works in this field focus on just one or two specific functionalities, resulting in a large variety of isolated schemes and a lack of a unified framework in MPSO research. In this work, we present an MPSO framework, which allows m parties, each holding a set, to securely compute any set formulas (arbitrary compositions of a finite number of binary set operations, including intersection, union and difference) on their private sets. Our framework is highly versatile and can be instantiated to accommodate a broad spectrum of MPSO functionalities. To the best of our knowledge, this is the first framework to achieve such a level of flexibility and generality in MPSO, without relying on generic secure multi-party computation (MPC) techniques. Our framework exhibits favorable theoretical and practical performance. With computation and communication complexity scaling linearly with the set size n, it achieves optimal complexity that is on par with the naive solution for widely used functionalities, such as multi-party private set intersection (MPSI), MPSI with cardinality output (MPSI-card), and MPSI with cardinality and sum (MPSI-card-sum), for the first time in the standard semi-honest model. Furthermore, the instantiations of our framework, which primarily rely on symmetric-key techniques, provide efficient protocols for MPSI, MPSI-card, MPSI-card-sum, and multi-party private set union (MPSU), with online performance that either surpasses or matches the state of the art in standard semi-honest model. At the technical core of our framework is a newly introduced primitive called predicative zero-sharing. This primitive captures the universality of a number of MPC protocols and is composable. We believe it may be of independent interest.",
    "status": "done"
  },
  {
    "id": 1788,
    "year": 2025,
    "title": "WPC: Weight Plaintext Compression for CNN Inference based on RNS-CKKS",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765022",
    "abstract": "Convolutional neural network (CNN) inference based on RNS-CKKS enables secure processing on encrypted data but introduces significant weight size overhead. Weight plaintext, weight in RNS-CKKS format, can reach tens to hundreds of gigabytes. Existing compression methods either add high computational cost or yield low compression rates. In this work, we propose WPC, Weight Plaintext Compression, to compress weight plaintext for RNS-CKKS-based CNN inference. We observe that the transformation from the weight in CNN models to the weight plaintext in RNS-CKKS format involves an operation akin to the Discrete Fourier Transform, which shifts data between the time and frequency domains while retaining redundant information from periodic and discrete data. Based on this observation, we first introduce the Periodic Transmit Theorem, which states that periodic patterns can be preserved during the transformation process, thereby enabling compression. We then propose Channel Innermost Packing Scheme and Rotation Padding to rearrange the weight data into periodic patterns for compression. Results show that WPC achieves 1.25 to 2.18 times speedup on an A100 GPU and 46.08 to 139.11 times compression rate.",
    "status": "done"
  },
  {
    "id": 1789,
    "year": 2025,
    "title": "FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to Retrieval-Augmented Generation Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765023",
    "abstract": "Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving external knowledge, reducing hallucinations and satisfying real-time information needs. While existing research mainly targets RAG's performance and efficiency, emerging studies highlight critical security concerns. Yet, current adversarial approaches remain limited, mostly addressing white-box scenarios or heuristic black-box attacks without fully investigating vulnerabilities in the retrieval phase. Additionally, prior works mainly focus on factoid Q&amp;A tasks, their attacks lack complexity and can be easily corrected by advanced LLMs. In this paper, we investigate a more realistic and critical threat scenario: adversarial attacks intended for opinion manipulation against black-box RAG models, particularly on controversial topics. Specifically, we propose FlippedRAG, a transfer-based adversarial attack against black-box RAG-like systems. We first demonstrate that the underlying retriever of a black-box RAG can be reverse-engineered and approximated by enumerating critical queries, candidates, and answers, enabling us to train a surrogate retriever. Leveraging the surrogate retriever, we further craft target poisoning triggers, altering vary few documents to effectively manipulate both retrieval and subsequent generation, transferring the attack to the original black-box RAG model. Extensive empirical results show that FlippedRAG substantially outperforms baseline methods, improving the average attack success rate by 16.7\\%. Across four diverse domains, FlippedRAG achieves on average a 50\\% directional shift in the opinion polarity of RAG-generated responses, ultimately causing a notable 20\\% shift in user cognition. Furthermore, we actively evaluate the performance of several potential defensive measures, concluding that existing mitigation strategies remain insufficient against such sophisticated manipulation attacks. These results highlight an urgent need for developing innovative defensive solutions to ensure the security and trustworthiness of RAG systems.",
    "status": "done"
  },
  {
    "id": 1790,
    "year": 2025,
    "title": "Mosformer: Maliciously Secure Three-Party Inference Framework for Large Transformers",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765028",
    "abstract": "Transformer-based models like BERT and GPT have achieved state-of-the-art performance across a wide range of AI tasks but raise serious privacy concerns when deployed as cloud inference services. To address this, secure multi-party computation (MPC) is commonly employed, encrypting both user inputs and model parameters to enable inference without revealing any private information. However, existing MPC-based secure transformer inference protocols are predominantly designed under the semi-honest security model. Extending these protocols to support malicious security remains a significant challenge, primarily due to the substantial overhead introduced by securely evaluating complex non-linear functions required for adversarial resilience. We introduce Mosformer, the first maliciously secure three-party (3PC) inference framework that efficiently supports large transformers such as BERT and GPT. We first design constant-round comparison and lookup table protocols with malicious security, leveraging verifiable distributed point functions (VDPFs). Building on these, we develop a suite of 3PC protocols for efficient and secure evaluation of complex non-linear functions in transformers. Together with optimized modulus conversion, our approach substantially reduces the overhead of secure transformer inference while preserving model accuracy. Experimental results on the vanilla transformer block show that Mosformer achieves up to a 5.3\\texttimes{} speedup and a 4.3\\texttimes{} reduction in communication over prior maliciously secure protocols. Despite offering stronger security guarantees, Mosformer achieves comparable or even superior online performance to state-of-the-art semi-honest 2PC and 3PC frameworks, including BOLT (Oakland 2024), BumbleBee (NDSS 2025), SHAFT (NDSS 2025), and Ditto (ICML 2024), on full-scale models such as BERT and GPT-2.",
    "status": "done"
  },
  {
    "id": 1791,
    "year": 2025,
    "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765045",
    "abstract": "Differentially private (DP) image synthesis aims to generate artificial images that retain the properties of a sensitive image dataset while protecting the privacy of individual images within the dataset. Despite recent advancements, we find that inconsistent--and sometimes flawed--evaluation protocols have been applied across studies. This not only impedes the understanding of current methods but also hinders future advancements in the field. To address the issue, this paper introduces DPImageBench, with thoughtful design across several dimensions: (1) Methods. We study twelve prominent methods and systematically characterize each based on model architecture, pretraining strategy, and privacy mechanism. (2) Evaluation. We include nine datasets and seven metrics to thoroughly assess these methods. Notably, we find that the common practice of selecting downstream classifiers based on the highest accuracy on sensitive test sets not only violates DP but also overestimates the utility. DPImageBench corrects for it. (3) Platform. Despite the wide variety of methods and evaluation protocols, DPImageBench provides a standardized interface that accommodates current and future implementations within a unified framework. With DPImageBench, we have several noteworthy findings. For example, contrary to the common wisdom that pretraining on public image datasets is usually beneficial, we find that the distributional similarity between pretraining and sensitive images significantly impacts the performance of the synthetic images and does not always yield improvements. The source code is available.",
    "status": "done"
  },
  {
    "id": 1792,
    "year": 2025,
    "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765052",
    "abstract": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling the creation of highly realistic and customized images from text prompts. With the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users can now customize powerful pre-trained models using minimal computational resources. However, the widespread sharing of fine-tuned DMs on open platforms raises growing ethical and legal concerns, as these models may inadvertently or deliberately generate sensitive or unauthorized content, such as copyrighted material, private individuals, or harmful content. Despite increasing regulatory attention on generative AI, there are currently no practical tools for systematically auditing these models before deployment. In this paper, we address the problem of concept auditing: determining whether a fine-tuned DM has learned to generate a specific target concept. Existing approaches typically rely on prompt-based input crafting and output-based image classification but they suffer from critical limitations, including prompt uncertainty, concept drift, and poor scalability. To overcome these challenges, we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric concept auditing framework. By treating the DM as the object of inspection, PAIA enables direct analysis of internal model behavior, bypassing the need for optimized prompts or generated images. It integrates two key components: a prompt-agnostic strategy that mitigates prompt sensitivity by analyzing model behavior during late-stage denoising, and an image-free detection method based on conditional calibrated error, which compares the internal dynamics of a fine-tuned model against its base version. Our auditing setting assumes internal access to DMs, but does not require access to proprietary fine-tuning data or user prompts, an assumption aligned with how hosted platforms audit uploaded models. We evaluate PAIA on 320 controlled models trained with curated concept datasets and 771 real-world community models sourced from a public DM sharing platform, covering a wide range of concepts including celebrities, cartoon characters, videogame entities, and movie references. Evaluation results show that PAIA achieves over 90\\% detection accuracy while reducing auditing time by 18 - 40x compared to existing baselines, and remains robust under adaptive attacks. To our knowledge, PAIA is the first scalable and practical solution for pre-deployment concept auditing of diffusion models, providing a practical foundation for safer and more transparent diffusion model sharing.",
    "status": "done"
  },
  {
    "id": 1793,
    "year": 2025,
    "title": "Provable Repair of Deep Neural Network Defects by Preimage Synthesis and Property Refinement",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765057",
    "abstract": "It is known that deep neural networks may exhibit dangerous behaviors under various security threats (e.g., backdoor attacks, adversarial attacks and safety property violation) and there exists an ongoing arms race between attackers and defenders. In this work, we propose a complementary perspective to utilize recent progress on ''neural network repair'' to mitigate these security threats and repair various kinds of neural network defects (arising from different security threats) within a unified framework, offering a potential silver bullet solution to real-world scenarios. To substantially push the boundary of existing repair techniques (suffering from limitations such as lack of guarantees, limited scalability, considerable overhead, etc) in addressing more practical contexts, we propose ProRepair, a novel provable neural network repair framework driven by formal preimage synthesis and property refinement. The key intuitions are: (i) synthesizing a precise proxy box to characterize the feature space preimage, which can derive a bounded distance term sufficient to guide the subsequent repair step towards the correct outputs, and (ii) performing property refinement to enable surgical corrections and scale to more complex tasks. We evaluate ProRepair across four security threats repair tasks on six benchmarks and the results demonstrate it outperforms existing methods in effectiveness, efficiency and scalability. For point-wise repair, ProRepair corrects models while preserving performance and achieving significantly improved generalization, with a speed-up of 5\\texttimes{} to 2000\\texttimes{} over existing provable approaches. In region-wise repair, ProRepair successfully repairs all 36 safety property violation instances (compared to 8 by the best existing method), and can handle 18\\texttimes{} higher dimensional spaces.",
    "status": "done"
  },
  {
    "id": 1794,
    "year": 2025,
    "title": "DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765103",
    "abstract": "While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines—for example, achieving 97.42\\% AUC and 94.35\\% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40\\%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10\\%-20\\% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems.",
    "status": "done"
  },
  {
    "id": 1795,
    "year": 2025,
    "title": "Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765105",
    "abstract": "Recent advancements have shown that Large Language Models (LLMs) possess significant versatility, making them suitable for applications in many areas. Several studies have shown how general-purpose LLMs can be adapted to domain-specific tasks. However, these domain-adapted LLMs can be exposed to greater privacy risks, which are especially exacerbated in the medical field. In this paper, we present the study investigating the susceptibility of LLMs to leaking sensitive health information. We conduct prompt-based attacks on LLMs trained with medical datasets, showing that medical LLMs can inadvertently disclose confidential patient data. To contribute towards mitigating privacy risks in the medical domain, we implement red teaming defense strategies to make LLMs robust against malicious attacks. For this medical red teaming approach, we develop and publicly release MediRed, a dataset of 1,000 red team attacks. By leveraging this dataset to enhance our defense mechanisms, we achieve up to 56\\% improvement in privacy protection compared to base models. Our code and dataset are available at https://github.com/yujinKang32/Private_Med_LLM.git",
    "status": "done"
  },
  {
    "id": 1796,
    "year": 2025,
    "title": "One-Sided Bounded Noise: Theory, Optimization Algorithms and Applications",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765110",
    "abstract": "We investigate the optimal trade-off between utility and privacy using one-sided perturbation. Unlike conventional privacy-preserving statistical releases, randomization for obfuscating side-channel information is often constrained by infrastructure limitations. In practical scenarios, these constraints may only allow positive and bounded perturbations. For example, extending processing time or sending and storing dummy messages/data is typically feasible. However, implementing modifications in the opposite direction is challenging due to restrictions imposed by hardware capacity, communication protocols, and data management systems. In this paper, we establish the foundation of the positive noise mechanism within three semantic privacy frameworks: Differential Privacy (DP), Maximal Leakage (MaxL), and Probably Approximately Correct (PAC) Privacy. We then present a series of results that characterize or approximate the optimal one-sided noise distribution, subject to a second-moment budget and a bounded maximal magnitude. Building on this theoretical foundation, we develop efficient tools to solve the underlying optimization problems. Through experiments conducted in various scenarios, we demonstrate that existing techniques, such as Truncated Biased Laplace noise, are often suboptimal and result in excessive performance degradation. For instance, in an anonymous communication system with a 250K message budget, our optimized DP noise mechanism achieves a 21\\texttimes{} reduction in dummy messages and an 18\\texttimes{} reduction in dummy message latency overhead compared to traditional methods.",
    "status": "done"
  },
  {
    "id": 1797,
    "year": 2025,
    "title": "PIIxel Leaks: Passive Identification of Personally Identifiable Information Leakage through Meta Pixel",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765113",
    "abstract": "Web pixels are one of the predominant techniques for tracking conversions and user behavior on the Web. The integration of Meta Pixel (the most widely used tracking pixel) into a website enables Meta to collect sensitive information about the website's visitors and match it with their Facebook or Instagram profiles. In addition to detailed navigation history, Meta Pixel also collects personally identifiable information (PII) entered by visitors in online forms present on the website, such as emails and phone numbers.In this paper, we present a scalable and comprehensive approach for measuring PII leakage through Meta Pixel by passively inspecting its core components, without the need to interact with the dynamic elements of a website. This is possible by statically identifying and analyzing the configuration profile of a Meta Pixel instance and extracting the information it is set up to collect. By developing a hybrid crawling approach (static and headless), we analyzed the top-1M most popular websites and found that 12.2\\% of them leak at least one instance of PII to Meta. We also found that in addition to email addresses and phone numbers, Meta Pixel also tracks PII such as age, gender, and geographical information, which can be used to not only reveal the identity of a user, but also their demographic characteristics. Finally, we assess the ability of Meta Pixel to track the browsing journey of a user by recording the sequence of full URLs visited across sub-pages.",
    "status": "done"
  },
  {
    "id": 1798,
    "year": 2025,
    "title": "Amigo: Secure Group Mesh Messaging in Realistic Protest Settings",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765133",
    "abstract": "During large-scale protests, a repressive government will often disable the Internet to thwart communication between protesters. Smartphone mesh networks, which route messages over short-range, possibly ephemeral, radio connections between nearby phones, allow protesters to communicate without relying on centralized Internet infrastructure. Unfortunately, prior work on providing secure communication in Internet shutdown settings fails to adequately consider protester needs. Previous attempts fail to support efficient private group communication (a crucial requirement for protests), and evaluate their solutions in network environments which fail to accurately capture link churn, physical spectrum contention, and the mobility models found in realistic protest settings. In this paper, we introduce Amigo, a novel mesh messaging system which supports group communication through a decentralized approach to continuous key agreement, and forwards messages using a novel routing protocol. Amigo is uniquely designed to handle the challenges of ad-hoc routing scenarios, where dynamic network topologies and node mobility make achieving key agreement nontrivial. Our extensive simulations reveal the poor scalability of prior approaches, the benefits of Amigo's protest-specific optimizations, and the challenges that still must be solved to scale secure mesh networks to protests with thousands of participants.",
    "status": "done"
  },
  {
    "id": 1799,
    "year": 2025,
    "title": "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765139",
    "abstract": "Despite longstanding criticism from the privacy community, k-anonymity remains a widely used standard for data anonymization, mainly due to its simplicity, regulatory alignment, and preservation of data utility. However, non-experts often defend k-anonymity on the grounds that, in the absence of auxiliary information, no known attacks can compromise its protections. In this work, we refute this claim by introducing Combinatorial Refinement Attacks (CRA), a new class of privacy attacks targeting k-anonymized datasets produced using local recoding. This is the first method that does not rely on external auxiliary information or assumptions about the underlying data distribution. CRA leverages the utility-optimizing behavior of local recoding anonymization of ARX, which is a widely used open-source software for anonymizing data in clinical settings, to formulate a linear program that significantly reduces the space of plausible sensitive values. To validate our findings, we partnered with a network of free community health clinics, an environment where (1) auxiliary information is indeed hard to find due to the population they serve and (2) open-source k-anonymity solutions are attractive due to regulatory obligations and limited resources. Our results on real-world clinical microdata reveal that even in the absence of external information, established anonymization frameworks do not deliver the promised level of privacy, raising critical privacy concerns.",
    "status": "done"
  },
  {
    "id": 1800,
    "year": 2025,
    "title": "Digital Safety for Children with Intellectual Disabilities When Using Mobile Devices from Parents' and Teachers' Perspectives",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765101",
    "abstract": "As mobile devices become increasingly integrated into children's daily lives, digital safety has emerged as a pressing concern, particularly for children with intellectual disabilities (ID), who are more vulnerable due to their cognitive and behavioral challenges. Despite their heightened risk, little research has addressed the unique digital safety issues these children face. To bridge this gap, we conducted semi-structured interviews with parents and special education teachers who are key figures for overseeing the digital access and safety of children with ID. Our findings highlight four primary concerns: imitation of harmful behaviors, accidental misoperation of devices, risks from fraud, and exposure to cyberbullying. To address these, parents and teachers largely rely on proactive educational strategies supported by technical controls and device restrictions. We conclude by emphasizing the need to adapt special education practices to the evolving digital landscape and propose inclusive safety strategies applicable to other at-risk user groups.",
    "status": "done"
  },
  {
    "id": 1801,
    "year": 2025,
    "title": "Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765102",
    "abstract": "Virtual Reality (VR) technology is rapidly growing in recent years. VR devices such as Meta Quest 3 utilize numerous sensors to collect users' data to provide an immersive experience. Due to the extensive data collection and the immersive nature, the security of VR devices is paramount. Leading VR devices often adopt and customize Android systems, which makes them susceptible to both Android-based vulnerabilities and new issues introduced by VR-specific customizations (e.g., system services to support continuous head and hand tracking). While prior work has extensively examined the security properties of the Android software stack, how these security properties hold for VR systems remains unexplored. In this paper, we present the first comprehensive security analysis of VR firmware. We collect over 300 versions of VR firmware from two major vendors, Quest and Pico, and perform a longitudinal analysis across the kernel layer, the system binary and library layer, and the application layer. We have identified several security issues in these VR firmware, including missing kernel-level security features, insufficient binary hardening, inconsistent permission enforcement, and inadequate SELinux policy enforcement. Based on our findings, we synthesize recommendations for VR vendors to improve security and trust for VR devices. This paper will act as an important security resource for VR developers, users, and vendors, and will also direct future advancements in secure VR ecosystem",
    "status": "done"
  },
  {
    "id": 1802,
    "year": 2025,
    "title": "Hidden in Plain Bytes: Investigating Interpersonal Account Compromise with Data Exports",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765147",
    "abstract": "When survivors of technology-facilitated abuse (TFA) suspect someone has accessed their online accounts, they often rely on built-in account security interfaces (ASIs), such as trusted device lists within settings, to assess account compromise. However, these interfaces typically offer limited or ambiguous details about past account accesses and security-critical events. Under right of access provisions in data protection laws, users can request structured exports of their personal data from online services. In this study, we explore whether and how data exports can supplement ASIs to support compromise investigations, particularly in interpersonal threat contexts. We simulated four types of account compromise attacks across six popular platforms, analyzing the resulting data exports and ASIs. Our findings show that data exports consistently contain more granular login histories and richer device/network identifiers than interfaces. Some even link security-related actions (e.g., password changes) and other post-authentication activity to specific devices, offering forensic value for identifying compromise. We discuss usability and other practical challenges of using data exports during TFA interventions.",
    "status": "done"
  },
  {
    "id": 1803,
    "year": 2025,
    "title": "How to Design Secure Honey Vault Schemes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765162",
    "abstract": "Password vaults enable a user to store multiple passwords with a single master password. Honey encryption (HE) protected password vaults (called honey vaults), are promising in resisting offline master password guessing attacks. Trial-decrypted with incorrect master passwords, honey vaults are designed to yield plausible-looking decoy vaults to confuse attackers, forcing them to perform online verifications to know whether a decrypted vault is the real one. In this paper, we demonstrate how to design secure honey vault schemes in a principled approach. We first identify three major types of vulnerabilities, and propose three critical design criteria based on rigorous theories, with each aiming to address one type of vulnerability. These criteria are: (1) Employing an accurate password probability model (PPM) in the natural language encoder (NLE, a key component of a honey vault) to resist distribution-aware distinguishing attacks; (2) Employing sequence-based PPMs for unique passwords, and sufficiently concise reuse models to resist encoding attacks (USENIX SEC'19); (3) Hiding a user's real-vault-related (i.e., adaptive) PPM to resist extraction attacks (USENIX SEC'21). To meet these key criteria, we propose VaultGuard with an innovative NLE and HE-Adaptive to honey-encrypt a user's real vault and the adaptive PPM, respectively. Our NLE eliminates the first and second vulnerabilities, while HE-Adaptive addresses the third. Security evaluations on real-world data reveal that our VaultGuard can significantly enhance honey vault security, forcing attackers to perform 1.10∼3.98 times online verifications. We also provide an efficient proof-of-concept VaultGuard implementation on the client side. We believe this work provides general principles and actionable guidelines for designing secure honey vault schemes.",
    "status": "done"
  },
  {
    "id": 1804,
    "year": 2025,
    "title": "Phishing Susceptibility and the (In-)Effectiveness of Common Anti-Phishing Interventions in a Large University Hospital",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765164",
    "abstract": "Phishing attacks via email remain a major entry point for security and privacy breaches in hospitals. In the European Union, faced with both regulatory pressure to act and limited resources for cybersecurity, hospitals may resort to minimal-effort, off-the-shelf anti-phishing interventions such as warning banners in enterprise email systems. However, their effectiveness remains uncertain, particularly given the highly diverse workforce comprising medical, nursing, functional, administrative, IT, and other staff groups. We conducted a large-scale phishing simulation at a German university hospital, targeting 7,044 email accounts, to analyze how phishing susceptibility varies across staff groups, how email characteristics---such as timing, tone, context, and persuasive framing---influence susceptibility, and how 11 common in-situ anti-phishing interventions affect risky staff behavior. We found that susceptibility but also intervention effectiveness differed markedly across staff groups. Even a small number of phishing emails posed a substantial risk that persisted for about three days. The most effective interventions involved robust technical detection, including spam filtering and in-email phishing warnings. Friction-based measures, such as disabling links and active warning pages, showed mixed but promising effects. In contrast, display name suppression and the widely used method of generic [EXTERNAL] email tagging had no or inconsistent effects. Surveys revealed that some staff reacted with fear, shame, guilt, and hostility, highlighting the ethical challenges of such simulations. Our findings provide actionable guidance for phishing resilience in healthcare and similarly complex organizations.",
    "status": "done"
  },
  {
    "id": 1805,
    "year": 2025,
    "title": "YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765168",
    "abstract": "Large Language Models (LLMs) are increasingly used by teenagers and young adults in everyday life, ranging from emotional support and creative expression to educational assistance. However, their unique vulnerabilities and risk profiles remain under-examined in current safety benchmarks and moderation systems, leaving this population disproportionately exposed to harm. In this work, we present Youth AI Risk (YAIR), the first benchmark dataset designed to evaluate and improve the safety of youth–LLM interactions. YAIR consists of 12,449 annotated conversation snippets spanning 78 fine-grained risk types, grounded in a taxonomy of youth-specific harms such as grooming, boundary violation, identity confusion, and emotional overreliance. We systematically evaluate widely adopted moderation models on YAIR and find that existing approaches substantially underperform in detecting youth-centered risks, often missing contextually subtle yet developmentally harmful interactions. To address these gaps, we introduce YouthSafe, a real-time risk detection model optimized for youth–GenAI contexts. YouthSafe significantly outperforms prior systems across multiple metrics on risk detection and classification, offering a concrete step toward safer and more developmentally appropriate AI interactions for young users.",
    "status": "done"
  },
  {
    "id": 1806,
    "year": 2025,
    "title": "GASLITEing the Retrieval: Exploring Vulnerabilities in Dense Embedding-based Search",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765095",
    "abstract": "Dense embedding-based text retrieval—retrieval of relevant passages from corpora via deep learning encodings—has emerged as a powerful method attaining state-of-the-art search results and popularizing Retrieval Augmented Generation (RAG). Still, like other search methods, embedding-based retrieval may be susceptible to search-engine optimization (SEO) attacks, where adversaries promote malicious content by introducing adversarial passages to corpora. Prior work has shown such SEO is feasible, mostly demonstrating attacks against retrieval-integrated systems (e.g., RAG). Yet, these consider relaxed SEO threat models (e.g., targeting single queries), use baseline attack methods, and provide small-scale retrieval evaluation, thus obscuring our comprehensive understanding of retrievers' worst-case behavior.This work aims to faithfully and thoroughly assess retrievers' robustness, paving a path to uncover factors related to their susceptibility to SEO. To this end, we, first, propose the GASLITE attack for generating adversarial passages, that—without relying on the corpus content or modifying the model—carry adversary-chosen information while achieving high retrieval ranking, consistently outperforming prior approaches. Second, using GASLITE, we extensively evaluate retrievers' robustness, testing nine advanced models under varied threat models, while focusing on pertinent adversaries targeting queries on a specific concept (e.g., a public figure). Amongst our findings: retrievers are highly vulnerable to SEO against concept-specific queries, even under negligible poisoning rates (e.g., ≤0.0001\\% of the corpus), while generalizing across different corpora and query distributions; single-query SEO is completely solved by GASLITE; adaptive attacks demonstrate bypassing common defenses; robustness to SEO attacks varies substantially between retrievers. Third, exploring the latter finding, we identify key factors that may contribute to models' susceptibility to SEO, including specific properties in the embedding space's geometry, echoing the essentiality of worst-case evaluations, and laying the basis for future defenses.",
    "status": "done"
  },
  {
    "id": 1807,
    "year": 2025,
    "title": "The Phantom Menace in Crypto-Based PET-Hardened Deep Learning Models: Invisible Configuration-Induced Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765107",
    "abstract": "The increasing use of deep learning (DL) models has given rise to significant privacy concerns regarding training and inference data. To address these concerns, the community has increasingly adopted crypto-based privacy-enhancing technologies (CPET) like homomorphic encryption (HE), secure multi-party computation (MPC), and zero-knowledge proofs (ZKP). The integration of CPET with DL, often referred to as CPET-DL, is commonly facilitated by specialized frameworks like CrypTen, TenSEAL, and EZKL. These frameworks offer configurable parameters to balance model accuracy and computational efficiency during privacy-preserving operations. However, these configurations, while seemingly harmless, can introduce subtle vulnerabilities. The stealthy attacks induced by misconfigurations are hard to detect because 1) the plaintext models remain vulnerability-free, and 2) existing auditing tools are hardly applicable to CPET-hardened models. This creates a paradox: tools intended to protect privacy can be undermined through configuration manipulation. We present ConPETro, the first attack on CPET-hardened models by manipulating the CPET-DL framework configurations. We show that well-crafted configurations allow attackers to create CPET-hardened models that function similarly to benign plaintext models under normal inputs, but exhibit significantly reduced robustness for malicious inputs embedded with triggers. ConPETro strategically selects triggers to maximize behavioral deviations with benign models and uses gradient consistency to guide configuration exploration, effectively finding malicious configurations that bypass standard plaintext model auditing. Evaluations across three mainstream CPET-DL frameworks (HE, MPC, and ZKP) demonstrate ConPETro's effectiveness in both semantic and non-semantic triggers. ConPETro achieves an average maximum attack success rate (ASR) of 72.27\\% in CPET-hardened models with non-semantic triggers; the accuracy only drops by 4\\%, thus maintaining stealthiness. It also achieves a maximum ASR of 94.74\\% with semantic triggers across three datasets. We also demonstrate that our stealthy attacks can bypass advanced defense and detection tools.",
    "status": "done"
  },
  {
    "id": 1808,
    "year": 2025,
    "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765118",
    "abstract": "As deep learning models become widely deployed as components within larger production systems, their individual shortcomings can create system-level vulnerabilities with real-world impact. This paper studies how adversarial attacks targeting an ML component can degrade or bypass an entire production-grade malware detection system, performing a case study analysis of Gmail's pipeline where file-type identification relies on a ML model. The malware detection pipeline in use by Gmail contains a machine learning model that routes each potential malware sample to a specialized malware classifier to improve accuracy and performance. This model, called Magika, has been open sourced. By designing adversarial examples that fool Magika, we can cause the production malware service to incorrectly route malware to an unsuitable malware detector thereby increasing our chance of evading detection. Specifically, by changing just 13 bytes of a malware sample, we can successfully evade Magika in 90\\% of cases and thereby allow us to send malware files over Gmail. We then turn our attention to defenses, and develop an approach to mitigate the severity of these types of attacks. For our defended production model, a highly resourced adversary requires 50 bytes to achieve just a 20\\% attack success rate. We implement this defense, and, thanks to a collaboration with Google engineers, it has already been deployed in production for the Gmail classifier.",
    "status": "done"
  },
  {
    "id": 1809,
    "year": 2025,
    "title": "Cascading Adversarial Bias from Injection to Distillation in Language Models",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765122",
    "abstract": "Model distillation has become essential for creating deployable language models, but their widespread deployment raises concerns about about their resilience to adversarial manipulation. This paper investigates how adversaries can inject subtle biases into teacher models through minimal data poisoning during training, which propagates to a smaller distilled student model and becomes significantly amplified. We identify two propagation modes: Untargeted (affecting multiple tasks) and Targeted (focusing on specific task while maintaining normal behavior elsewhere). With only 25 poisoned samples (0.25\\% poisoning rate), student models generate biased responses 76.9\\% of the time in targeted scenarios versus 69.4\\% in teachers, while untargeted propagation shows 5.7X-29.2X higher adversarial bias rate in students on unseen tasks. We validate across six bias types (targeted advertisement, phishing link, narrative manipulations, insecure coding practices), various distillation methods, and text/code generation modalities. Current defense mechanisms—including perplexity filtering, bias detection systems, and LLM-based autoraters—prove inadequate against these attacks. We propose practical design principles for building effective adversarial bias mitigation strategies to address this threat vector.",
    "status": "done"
  },
  {
    "id": 1810,
    "year": 2025,
    "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765124",
    "abstract": "Large language models (LLMs) have been widely adopted across various applications, leveraging customized system prompts for diverse tasks. Facing potential system prompt leakage risks, model developers have implemented strategies to prevent leakage, primarily by disabling LLMs from repeating their context when encountering known attack patterns. However, it remains vulnerable to new and unforeseen prompt-leaking techniques. In this paper, we first introduce a simple yet effective prompt leaking attack to reveal such risks. Our attack is capable of extracting system prompts from various LLM-based application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our findings further inspire us to search for a fundamental solution to the problems by having no system prompt in the context. To this end, we propose SysVec, a novel method that encodes system prompts as internal representation vectors rather than raw text. By doing so, SysVec minimizes the risk of unauthorized disclosure while preserving the LLM's core language capabilities. Remarkably, this approach not only enhances security but also improves the model's general instruction-following abilities. Experimental results demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.",
    "status": "done"
  },
  {
    "id": 1811,
    "year": 2025,
    "title": "Exact Robustness Certification of k-Nearest Neighbors",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765140",
    "abstract": "Robustness guarantees are essential for deploying machine learning models in security-critical environments where adversarial attacks pose a serious threat. While extensive progress has been made in certifying (deep) neural networks, nonparametric models such as k-Nearest Neighbors (k-NN) have been less investigated, despite their interpretability and usage in high-assurance settings. Prior certification methods for k-NN provide sound but incomplete guarantees, leaving many genuinely robust inputs uncertified. This work introduces a sound and complete certification framework for k-NN classifiers, offering exact robustness guarantees against adversarial perturbations. Our approach combines hypercube space decomposition with a novel graph-theoretic analysis based on an adversarial proximity precedence graph, enabling full coverage of adversarial regions. Extensive evaluation on widely used datasets demonstrates that our exact methodology significantly improves certification rates over existing techniques while maintaining scalability. By closing the gap between soundness and completeness, our framework advances the security guarantees of k-NN models and contributes to the broader goal of provably robust machine learning in adversarial settings.",
    "status": "done"
  },
  {
    "id": 1812,
    "year": 2025,
    "title": "IOValve: Leakage-Free I/O Sandbox for Large-Scale Untrusted Data Processing",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765121",
    "abstract": "The widespread adoption of Large Language Models (LLMs) is driving the rapidly growing demand for large-scale computations like training and fine-tuning models. In many areas, the confidentiality of the underlying data is of critical importance to their corporate or government owners. However, securing data in large-scale computations is challenging. First, its demand for enormous hardware resources typically requires outsourcing (e.g., to the public cloud). Second, the large and rapidly evolving software stack used in LLM training in conjunction with a growing incidence of supply chain attacks and software vulnerabilities makes it all but impossible for data owners to establish trust in the code that processes their highly sensitive data. Confidential computing and sandboxing are promising techniques for solving these problems. However, existing sandboxes do not address covert channels which limits their ability to protect confidential data. This paper proposes IOValve, a novel I/O sandbox for large-scale computations on confidential data. IOValve places sandbox enforcement on a programmable network device that is physically isolated from the processor hardware running the untrusted software stack. This construction allows IOValve to sidestep the multitude of side channels due to visible or hidden resource sharing. IOValve interposes on all network I/O of the sandbox and only transmits encrypted and regularized network traffic in order to prevent information leakage over the network. Our evaluation shows that IOValve has marginal performance overhead and supports real-world applications like LLM fine-tuning and batch inference, and molecular simulation.",
    "status": "done"
  },
  {
    "id": 1813,
    "year": 2025,
    "title": "BadAML: Exploiting Legacy Firmware Interfaces to Compromise Confidential Virtual Machines",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765123",
    "abstract": "Confidential virtual machines (CVMs) are an emerging form of trusted execution environment that enable existing operating systems (OSs) to run securely without trusting cloud providers. To this end, CVMs employ hardware-based memory encryption for runtime confidentiality and cryptographic attestation to verify memory integrity at startup. However, we reveal a previously overlooked attack vector that allows malicious cloud providers to bypass CVM attestation and execute arbitrary code within users' CVMs regardless of specific CVM configurations. Our attack, BadAML, exploits the Advanced Configuration and Power Interface (ACPI), a legacy yet widely adopted firmware interface for machine configuration. Specifically, BadAML leverages ACPI Machine Language (AML) to inject arbitrary binary code into the guest OS kernel without affecting CVM attestation. Because ACPI remains an essential component even in virtualized environments, BadAML constitutes a powerful and portable attack vector independent of guest OS type and CVM technology. We demonstrate proof-of-concept exploits of BadAML in both Linux and Windows CVM environments. We then analyze possible mitigation measures, discussing their effectiveness and limitations. Finally, we introduce AML sandboxing, a practical defense that restricts memory access to safe regions under the CVM threat model; we present its design, implementation, and evaluation, demonstrating its effectiveness across 18 real-world cloud CVM instances.",
    "status": "done"
  },
  {
    "id": 1814,
    "year": 2025,
    "title": "Protocol-Aware Firmware Rehosting for Effective Fuzzing of Embedded Network Stacks",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765125",
    "abstract": "One of the biggest attack surfaces of embedded systems is their network interfaces, which enable communication with other devices. Unlike their general-purpose counterparts, embedded systems are designed for specialized use cases, resulting in unique and diverse communication stacks. Unfortunately, current approaches for evaluating the security of these embedded network stacks require manual effort or access to hardware, and they generally focus only on small parts of the embedded system. A promising alternative is firmware rehosting, which enables fuzz testing of the entire firmware by generically emulating the physical hardware. However, existing rehosting methods often struggle to meaningfully explore network stacks due to their complex, multi-layered input formats. This limits their ability to uncover deeply nested software faults.To address this problem, we introduce a novel method to automatically detect and handle the use of network protocols in firmware called Pemu. By automatically deducing the available network protocols, Pemu can transparently generate valid network packets that encapsulate fuzzing data, allowing the fuzzing input to flow directly into deeper layers of the firmware logic. Our approach thus enables a deeper, more targeted, and layer-by-layer analysis of firmware components that were previously difficult or impossible to test. Our evaluation demonstrates that Pemu consistently improves the code coverage of three existing rehosting tools for embedded network stacks. Furthermore, our fuzzer rediscovered several known vulnerabilities and identified five previously unknown software faults, highlighting its effectiveness in uncovering deeply nested bugs in network-exposed code.",
    "status": "done"
  },
  {
    "id": 1815,
    "year": 2025,
    "title": "Dynamic Detection of Vulnerable DMA Race Conditions",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765126",
    "abstract": "The drivers of modern operating systems use Direct Memory Access (DMA) to efficiently communicate with peripheral devices. Since the memory accessed by DMA is a shared resource between driver and device, it is a possible source of race conditions. Peripheral devices are also often untrusted, so these race conditions open up a new potential attack vector against a trusted OS kernel. In this paper, we present DMARacer, a dynamic detector called for these DMA-based race conditions in kernel code. DMARacer tracks memory accesses to DMA memory throughout the kernel's lifetime and analyses them for various indicators of race conditions. Additionally, upon detecting a race condition, DMARacer uses taint tracking to trace its impact and identify any potential vulnerabilities it may trigger, such as memory corruption or denial-of-service. We used DMARacer to search the drivers of the Linux kernel for DMA-based errors and find that DMA-based race conditions are a systemic issue in driver code. In total, DMARacer was able to detect 817 problematic memory accesses and 344 vulnerable operations in the scanned Linux kernel drivers.",
    "status": "done"
  },
  {
    "id": 1816,
    "year": 2025,
    "title": "Attestable Builds: Compiling Verifiable Binaries on Untrusted Systems using Trusted Execution Environments",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765128",
    "abstract": "In this paper we present attestable builds, a new paradigm to provide strong source-to-binary correspondence in software artifacts. We tackle the challenge of opaque build pipelines that disconnect the trust between source code, which can be understood and audited, and the final binary artifact which is difficult to inspect. Our system uses modern trusted execution environments (TEEs) and sandboxed build containers to provide strong guarantees that a given artifact was correctly built from a specific source code snapshot. As such it complements existing approaches like reproducible builds which typically require time-intensive modifications to existing build configurations and dependencies, and require independent parties to continuously build and verify artifacts. In comparison, an attestable build requires only minimal changes to an existing project, and offers nearly instantaneous verification of the correspondence between a given binary and the source code and build pipeline used to construct it. We evaluate it by building open-source software libraries—focusing on projects which are important to the trust chain and have proven difficult to be built deterministically. The overhead (42 seconds start-up latency and 14\\% increase in build duration) is small in comparison to the overall build time. Importantly, our prototype can build complex projects such as LLVM Clang without requiring any modifications to their source code and build scripts. Finally, we formally model and verify the attestable build design to demonstrate its security against well-resourced adversaries.",
    "status": "done"
  },
  {
    "id": 1817,
    "year": 2025,
    "title": "Augmenting Search-based Program Synthesis with Local Inference Rules to Improve Black-box Deobfuscation",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765134",
    "abstract": "Code obfuscation aims to protect programs from reverse engineering, with applications ranging from intellectual property protection to malware hardening. Recent works on black-box analyses propose to leverage program synthesis in order to infer the semantics of highly obfuscated code blocks. Being fully black-box, these approaches are immune to syntactic complexity and can thus bypass standard obfuscation mechanisms. Yet, they are restricted by their synthesis capabilities and can only be applied to semantically simple code blocks. It explains why they have mainly been used on virtual machine handlers, where behaviors are usually simple enough. Applying black-box deobfuscation at scale beyond virtualization is still an open problem, notably because black-box methods cannot synthesize complex behaviors involving, for example, arbitrary constant values or affine or polynomial relations over mixed-boolean-arithmetic expressions. In this article, we show how to combine search-based program synthesis with local inference rules, resulting in a new method named Search Modulo Inference Rules (Smir that boosts search-based program synthesis while keeping its generality and flexibility. We instantiate Smir with inference rules for hard synthesis problems like arbitrary constant values and affine or polynomial relations over mixed boolean expressions, yielding the new black-box deobfuscation tool: XSmir. Experiments on obfuscated codes, real-world binaries, and synthetic benchmarks demonstrate that XSmir significantly outperforms prior black-box deobfuscators, synthesizing overall 76\\% and 84\\% of the expressions from our real-world obfuscated and non-obfuscated benchmarks where prior works recover 63\\% and 55\\%, together with 2 to 3 times less false positive and slightly improved compression rate.",
    "status": "done"
  },
  {
    "id": 1818,
    "year": 2025,
    "title": "Right the Ship: Assessing the Legitimacy of Invalid Routes in RPKI",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744853",
    "abstract": "Resource Public Key Infrastructure (RPKI) aims to prevent prefix hijacking by providing secure mappings between IP prefixes and their authorized origin Autonomous Systems (ASes). In recent years, there has been notable growth in the deployment of RPKI and Route Origin Validation (ROV). Nonetheless, over 40\\% of the routes in the global routing table still lack the protection of RPKI. One of the critical reasons some networks are reluctant to deploy RPKI is the concern that some ROV-invalid routes may be legitimate, and filtering these routes will harm network service quality, especially affecting network connectivity.In this work, we perform a comprehensive measurement study to assess the legitimacy of ROV-invalid routes in RPKI. We evaluate the impact of filtering all ROV-invalid routes in the global routing table, presenting a view that some ROV-invalid routes are not illegitimate, defined as harmlessly ROV-invalid (h-invalid). We propose five characteristics and design a characteristics-based methodology for identifying h-invalid routes. Based on the methodology, we analyze the magnitude of h-invalid routes present on the Internet each day, revealing that over 91\\% of ROV-invalid results are h-invalid. Furthermore, we conclude three main reasons for h-invalid routes. Finally, with all our findings, we provide practical recommendations for network operators to help promote RPKI deployment.",
    "status": "done"
  },
  {
    "id": 1819,
    "year": 2025,
    "title": "Exploring and Analyzing Cross Layer DoS Attack Against UDP-based Services on Linux",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744878",
    "abstract": "The layered architecture of the TCP/IP protocol stack enables protocol layers to be implemented independently and flexibly. However, this layered design introduces potential security risks when shared resources are not properly managed between different layers. This paper investigates a neglected cross-layer shared resource risk, termed SocketFilled, which exploits the insecure usage of the UDP send buffer at the transport layer by the link layer, resulting in the interruption of response packets from the upper application layer. To explore the root causes of cross-layer DoS vulnerabilities resulting from the implementation of the TCP/IP protocol stack, we systematically analyzed the protocol standards of address resolution and reviewed the implementation in mainstream open-source operating systems. Moreover, we conducted a comprehensive experimental evaluation of mainstream operating systems (e.g., Linux and FreeBSD) and UDP services (e.g., DNS and QUIC). The experimental results show that the latest version of Linux and UDP service software (e.g., BIND9, PowerDNS, and Nginx) are affected, causing significant packet loss and even complete service interruption. Then, we estimated the impact range of SocketFilled in the wild and demonstrated that 17.3\\% of open resolvers,54.3\\% of authoritative servers of the Tranco Top 100K domains, and 3.8\\% of these well-known domains' HTTP/3 servers are potentially affected, including Bing, Amazon, and Shopee, after excluding the influence of cloud servers. We have conducted responsible disclosure by reporting the vulnerability to the Linux community. Our research highlights the effectiveness of cross-layer mechanisms in DoS attacks and calls for heightened attention to the layered complexity of protocol stack implementations within the security community.",
    "status": "done"
  },
  {
    "id": 1820,
    "year": 2025,
    "title": "Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP Address Sharing Scenarios",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744888",
    "abstract": "Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of modern Internet infrastructure. In this paper, we investigate the security vulnerabilities associated with PMTUD within the context of prevalent IP address sharing practices. We reveal that PMTUD is inadequately designed to handle IP address sharing, creating vulnerabilities that attackers can exploit to perform off-path TCP hijacking attacks. We demonstrate that by observing the path MTU value determined by a server for a public IP address (shared among multiple devices), an off-path attacker on the Internet, in collaboration with a malicious device, can infer the sequence numbers of TCP connections established by other legitimate devices sharing the same IP address. This vulnerability enables the attacker to perform off-path TCP hijacking attacks, significantly compromising the security of the affected TCP connections. Our attack involves first identifying a target TCP connection originating from the shared IP address, followed by inferring the sequence numbers of the identified connection. We thoroughly assess the impacts of our attack under various network configurations. Experimental results reveal that the attack can be executed within an average time of 220 seconds, achieving a success rate of 70\\%. Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection, highlight the threat it poses to various applications. Additionally, we evaluate our attack across 50 real-world networks with IP address sharing---including public Wi-Fi, VPNs, and 5G---and find 38 vulnerable. Finally, we responsibly disclose the vulnerabilities, receive recognition from organizations such as IETF, Linux, and Cisco, and propose our countermeasures.",
    "status": "done"
  },
  {
    "id": 1821,
    "year": 2025,
    "title": "SISTAR: An Efficient DDoS Detection and Mitigation Framework Utilizing Programmable Data Planes",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765046",
    "abstract": "DDoS attacks have become one of the most severe cybersecurity threats, especially in application-layer attacks. With the emergence of Programmable Data Planes (PDPs), it has become possible to maintain line-rate throughput while achieving high detection rates, making them crucial in addressing DDoS challenges. However, due to the complexity of DDoS attacks, detection remains resource-intensive and overall network defense effectiveness is limited. This limitation becomes particularly pronounced in clustered environments, where coordinated defense is essential. This paper presents SISTAR, an innovative framework for efficient DDoS detection and mitigation using PDP. SISTAR integrates an improved Decision Tree - Constrained Threshold Segmentation (DT-CTS) model to achieve high detection accuracy while minimizing hardware resource usage. Through distributed deployment across multiple switches, SISTAR enhances network resilience by enabling rapid detection and coordinated response to DDoS attacks. We implement a prototype of SISTAR and evaluate its performance in a realistic testbed, the experimental results show that SISTAR surpasses existing models in terms of detection accuracy and resource efficiency. When combined with its alert pushback mechanism, SISTAR can effectively reduce network resource consumption caused by DDoS attacks.",
    "status": "done"
  },
  {
    "id": 1822,
    "year": 2025,
    "title": "ScannerGrouper: A Generalizable and Effective Scanning Organization Identification System Toward the Open World",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765053",
    "abstract": "In recent years, many scanning organizations deploy large numbers of scanners to actively probe the Internet. Identifying the organizations behind these scanners is of significant value. The problem of analyzing the sources of scanners has been investigated in various studies. However, as far as we know, the problem of effectively and generally identifying scanner organizations in real-world scenarios remains unsolved.In this paper, we present ScannerGrouper, a darknet-independent system specifically designed to identify the organizations behind Internet scanners in real-world scenarios. ScannerGrouper leverages monitoring systems capable of capturing service probes, e.g., honeypots, to collect traffic for subsequent analysis. To address the robustness challenge, ScannerGrouper selects features from the payloads of the first service probes sent by scanners through statistical analysis, and aggregates the identification results from multiple service-specific classifiers. To tackle the open-world issue, ScannerGrouper customizes a state-of-the-art open-set model to our specific task, and updates the system incrementally. We conduct extensive experiments to validate ScannerGrouper effectiveness. ScannerGrouper outperforms baseline solutions in identification performance, achieving a weighted average F1-score that is 1.63 to 4.05 times higher. We also experimentally analyze the identification results of unattributed scanners, training time, the performance of possible alternative models of the core module, and the impact of hyperparameters, etc.",
    "status": "done"
  },
  {
    "id": 1823,
    "year": 2025,
    "title": "On the Security of SSH Client Signatures",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765079",
    "abstract": "Administrators and developers use SSH client keys and signatures for authentication, for example, to access internet backbone servers or to commit new code on platforms like GitHub. However, unlike servers, SSH clients cannot be measured through internet scans. We close this gap in two steps. First, we collect SSH client public keys. Such keys are regularly published by their owners on open development platforms like GitHub and GitLab. We systematize previous non-academic work by subjecting these keys to various security tests in a longitudinal study. Second, in a series of black-box lab experiments, we analyze the implementations of algorithms for SSH client signatures in 24 popular SSH clients for Linux, Windows, and macOS. We extracted 31,622,338 keys from three public sources in two scans. Compared to previous work, we see a clear tendency to abandon RSA signatures in favor of EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139 keys generated from weak randomness, and 149 keys with common or small factors—the large majority of the retrieved keys exposed no weakness. Weak randomness can not only compromise a secret key through its public key, but also through signatures. It is well-known that a bias in random nonces in ECDSA can reveal the secret key through public signatures. For the first time, we show that the use of deterministic nonces in ECDSA can also be dangerous: The private signing key of a PuTTY client can be recovered from just 58 valid signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our finding in CVE-2024-31497, and they subsequently replaced the nonce generation algorithm.",
    "status": "done"
  },
  {
    "id": 1824,
    "year": 2025,
    "title": "Toss: Garbled PIR from Table-Only Stacking",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744831",
    "abstract": "Garbled Circuits (GC) is a foundational primitive for secure two-party computation (2PC). Garbled Private Information Retrieval (GPIR) is a GC technique for looking up a public array or database (DB) on a private index unknown to either player. GPIR immediately implies GC evaluation of functions implemented as a publicly known look-up table (LUT).GPIR is costly: it can be obtained by a linear scan, adapting Garbled RAM, stacking GC branches implementing access to table elements, and, most recently, from the GC look-up table ''logrow'' (Heath et al., Eurocrypt 2024). For a database of N rows with m-bit entries, logrow's computation is approximately O(NmΚ), and its communication is O(m·(log N·Κ + N)). Logrow thus can be effectively used on tables of size up to about 215.We propose Toss, a new efficient GPIR with dramatically reduced bandwidth consumption (a scarce resource in MPC!), both asymptotically and concretely. Our communication cost is O(m(N·m·Κ)), with a small constant, sublinear in both N and the security parameter Κ. Our computation cost is O(N·m·Κ + (√(N/Κ)·m + N)·cΚ)), where cΚ is the cost of hash evaluation. This matches or slightly improves on logrow's computational cost.In concrete terms, for a 220-row LUT of 8-bit items, we improve over logrow by a factor of &gt;31\\texttimes{} in communication. On a laptop over a 100 Mbps channel, throughput rises from ≈10.6 lookups/s to ≈81 lookups/s (&gt;7.5\\texttimes{} improvement); on a 10 Mbps channel, Toss is &gt;28\\texttimes{} faster. Communication improvement grows with N—for N = 225, m = 32, the gain exceeds 512\\texttimes{}.Toss builds on stacked garbling (SGC) and logrow with multiple low-level optimizations, requiring reworking of their internals and interfaces. We emphasize that constructing GPIR directly from SGC incurs logarithmic computational overhead, which actually reduces throughput in typical ''laptop + LAN'' testbeds.We implement our construction and report its performance.",
    "status": "done"
  },
  {
    "id": 1825,
    "year": 2025,
    "title": "Secure Noise Sampling for Differentially Private Collaborative Learning",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744834",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) trains machine learning (ML) models with formal privacy guarantees for the training set by adding random noise to gradient updates. In collaborative learning (CL), where multiple parties jointly train a model, noise addition occurs either (i) before or (ii) during secure gradient aggregation. The first option is deployed in distributed DP methods, which require greater amounts of total noise to achieve security, resulting in degraded model utility. The second approach preserves model utility but requires a secure multiparty computation (MPC) protocol. Existing methods for MPC noise generation require tens to hundreds of seconds of runtime per noise sample because of the number of parties involved. This makes them impractical for collaborative learning, which often requires thousands or more samples of noise in each training step.We present a novel protocol for MPC noise sampling tailored to the collaborative learning setting. It works by constructing an approximation of the distribution of interest which can be efficiently sampled by a series of table lookups. Our method achieves significant runtime improvements and requires much less communication compared to previous work, especially at higher numbers of parties. It is also highly flexible -- while previous MPC sampling methods tend to be optimized for specific distributions, we prove that our method can generically sample noise from statistically close approximations of arbitrary discrete distributions. This makes it compatible with a wide variety of DP mechanisms. Our experiments demonstrate the efficiency and utility of our method applied to a discrete Gaussian mechanism for differentially private collaborative learning. For 16 parties, we achieve a runtime of 0.06 seconds and 11.59 MB total communication per sample, a 230\\texttimes{} runtime improvement and 3\\texttimes{} less communication compared to the prior state-of-the-art for sampling from discrete Gaussian distribution in MPC.",
    "status": "done"
  },
  {
    "id": 1826,
    "year": 2025,
    "title": "Post-Quantum Threshold Ring Signature Applications from VOLE-in-the-Head",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744854",
    "abstract": "We propose efficient, post-quantum threshold ring signatures constructed from one-wayness of AES encryption and the VOLE-in-the-Head zero-knowledge proof system. Our scheme scales efficiently to large rings and extends the linkable ring signatures paradigm. We define and construct key-binding deterministic tags to achieve linkability. We then extend our threshold ring signatures to realize post-quantum anonymous ledger transactions in the spirit of Monero. Finally, our deterministic tags also enable succinct aggregation using approximate lower bound arguments of knowledge; this allows us to achieve succinct (approximate) multi-signatures without SNARKs. Our constructions assume symmetric key primitives only.Whilst it is common to build post-quantum signatures from the one-wayness property of AES and a post-quantum NIZK scheme, we extend this paradigm to define and construct novel security properties from AES that are useful for advanced signature applications. We introduce key-binding and pseudorandomness of functions from AES to establish linkability and anonymity of our threshold ring signatures from deterministic tags, and similarly establish binding and hiding properties of block ciphers modeled as ideal permutations to build commitments from AES, a crucial building block for our proposed post-quantum anonymous ledger scheme.",
    "status": "done"
  },
  {
    "id": 1827,
    "year": 2025,
    "title": "Distance-Aware OT with Application to Fuzzy PSI",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744857",
    "abstract": "A two-party fuzzy private set intersection (PSI) protocol between Alice and Bob with input sets A and B allows Alice to learn nothing more than the points of Bob that are ''δ-close'' to its points in some metric space dist . More formally, Alice learns only the set {b | dist (a,b) ≤ δ , a ∈ A, b ∈ B} for a predefined threshold δ and distance metric dist , while Bob learns nothing about Alice's set. Fuzzy PSI is a valuable privacy tool in scenarios where private set intersection needs to be computed over imprecise or measurement-based data, such as GPS coordinates or healthcare data. Previous approaches to fuzzy PSI rely on asymmetric cryptographic primitives, generic two-party computation (2PC) techniques like garbled circuits, or function secret sharing methods, all of which are computationally intensive and lead to poor concrete efficiency. This work introduces a new modular framework for fuzzy PSI, primarily built on efficient symmetric key primitives. Our framework reduces the design of efficient fuzzy PSI to a novel variant of oblivious transfer (OT), which we term distance-aware random OT (da-ROT). This variant enables the sender to obtain two random strings (r0, r1), while the receiver obtains one of these values rb, depending on whether the receiver's input keyword a and the sender's input keyword b are close in some metric space i.e., dist (a,b) ≤ δ. The da-ROT can be viewed as a natural extension of traditional OT, where the condition (choice bit) is known to the receiver. We propose efficient constructions for da-ROT based on standard OT techniques tailored for small domains, supporting distance metrics such as the Chebyshev norm, the Euclidean norm, and the Manhattan norm. By integrating these da-ROT constructions, our fuzzy PSI framework achieves up to a 14\\texttimes{} reduction in communication cost and up to a 54\\texttimes{} reduction in computation cost compared to previous state-of-the-art protocols, across input set sizes ranging from 28 to 216. Additionally, we extend our framework to compute fuzzy PSI cardinality and fuzzy join from traditional PSI-related functionalities. All proposed protocols are secure in the semi-honest model.",
    "status": "done"
  },
  {
    "id": 1828,
    "year": 2025,
    "title": "MegaBlocks: Breaking the Logarithmic I/O-Overhead Barrier for Oblivious RAM",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3765159",
    "abstract": "Oblivious RAM (ORAM) is a central cryptographic primitive that enables secure memory access while hiding access patterns. Among existing ORAM paradigms, hierarchical ORAMs were long considered impractical despite their asymptotic optimality. However, recent advancements (FutORAMa, CCS'23) demonstrate that hierarchical ORAM-based schemes can be made efficient given sufficient client-side memory. In this work, we present a new hierarchical ORAM construction that achieves practical performance without requiring large local memory.From a theoretical standpoint, we identify that there is a gap in the literature concerning the asymmetric setting, where the logical word size is asymptotically smaller than the physical memory block size. In this scenario, the best-known construction (OptORAMa, J. ACM '23,) turns every logical query into O(log N) physical memory accesses (quantity known as ''I/O overhead''), whereas the lower bound of Komargodski and Lin (CRYPTO'21) implies that Ω(log N /log log N) accesses are needed.We close this gap by constructing an optimal ORAM for the asymmetric setting, achieving an I/O overhead of O(log N / log log N). Our construction features exceptionally small constants (between 1 and 4, depending on the block size) and operates without requiring large local memory. We implement our scheme and compare it to PathORAM (CCS'13) and FutORAMa, demonstrating significant improvement. For 1TB logical memory, our construction obtains X10-X30 reduction in I/O overhead and bandwidth compared to PathORAM, and X7--X26 improvement over FutORAMa. This improvement applies when those schemes weren't designed to operate on large blocks, as in our settings, and the exact improvement depends on the physical block size and the exact local memory available.",
    "status": "done"
  },
  {
    "id": 1829,
    "year": 2025,
    "title": "CuKEM: A Concise and Unified Hybrid Key Encapsulation Mechanism",
    "publication": "ACM CCS",
    "paper": "https://doi.org/10.1145/3719027.3744863",
    "abstract": "In the post-quantum migration of the traditional key establishment protocol, hybrid key encapsulation mechanisms (KEMs) are recommended by standards bodies, including NIST, ETSI, and national security agencies like NCSC-UK, BSI-Germany etc. Recently, several hybrid KEMs with CCA security such as XOR-then-MAC, Dual-PRF and X-Wing (being standardized by IETF) are proposed based on CCA KEMs obtained by applying the complicated Fujisaki-Okamoto transform to public-key encryption (PKE) schemes. In some cryptographic protocols such as PQ-Noise and Signal, 1CCA security (similar to the CCA security except that the adversary is restricted to one single decapsulation query) is required. However, no specific scheme has been designed to specifically achieve 1CCA security (excluding the schemes that aim to achieve CCA security, as they inherently encompass 1CCA security).In this paper, we propose CuKEM, a concise and unified hybrid KEM framework directly built on PKEs. We prove that CuKEM, equipped with different modules, achieves security notions in both the random oracle model and the quantum random oracle model, including IND-CPA, IND-1CCA, and IND-CCA. Compared to the existing KEM-based designs, CuKEM is more concise because it simplifies or even eliminates certain hash operations without compromising security. The evaluation shows that CuKEM can significantly improve efficiency over current hybrid KEMs, e.g., the encapsulation(decapsulation) of CCA-secure CuKEM efficiency gains of up to 22.28\\% (16.22\\%) compared to X-Wing, while the 1CCA-secure CuKEM gains up to 13.97\\% (104.31\\%).",
    "status": "done"
  }
]